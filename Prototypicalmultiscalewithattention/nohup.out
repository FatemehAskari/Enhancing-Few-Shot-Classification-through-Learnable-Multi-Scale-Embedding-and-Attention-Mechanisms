/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
{'gpu': '0',
 'max_epoch': 200,
 'query': 15,
 'save_epoch': 20,
 'save_path': 'learngamma_MiniImageNet/save/1testshot',
 'shot': 1,
 'test_way': 5,
 'train_way': 30}
using gpu: 0
learngamma_MiniImageNet/save/1testshot exists, remove? ([y]/n)Traceback (most recent call last):
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/train_attentioon_mini.py", line 34, in <module>
    ensure_path(args.save_path)
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/utils_attention.py", line 17, in ensure_path
    if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 9] Bad file descriptor
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
{'gpu': '0',
 'max_epoch': 200,
 'query': 15,
 'save_epoch': 20,
 'save_path': 'learngamma_MiniImageNet/save/1testshot',
 'shot': 1,
 'test_way': 5,
 'train_way': 30}
using gpu: 0
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/train_attentioon_mini.py", line 193, in <module>
    loss_value = fit(support_images, support_labels, query_images, query_labels,model_true)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/train_attentioon_mini.py", line 159, in fit
    classification_scores = model(
                            ^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/prototypicalNetwork_attention.py", line 141, in forward
    d4=self.calculate(support_images,support_labels,query_images,51,512)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/prototypicalNetwork_attention.py", line 117, in calculate
    out3 = f2(query_images)
           ^^^^^^^^^^^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/fx/graph_module.py", line 662, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/fx/graph_module.py", line 281, in __call__
    raise e
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/fx/graph_module.py", line 271, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<eval_with_key>.19", line 5, in forward
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 23.64 GiB total capacity; 3.54 GiB already allocated; 100.50 MiB free; 3.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
{'gpu': '0',
 'max_epoch': 200,
 'query': 15,
 'save_epoch': 20,
 'save_path': 'learngamma_MiniImageNet/save/1testshot',
 'shot': 1,
 'test_way': 5,
 'train_way': 30}
using gpu: 0
learngamma_MiniImageNet/save/1testshot exists, remove? ([y]/n)Traceback (most recent call last):
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/train_attentioon_mini.py", line 34, in <module>
    ensure_path(args.save_path)
  File "/home/amirreza/Desktop/few shot/Final/Prototypicalmultiscalewithattention/utils_attention.py", line 17, in ensure_path
    if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 9] Bad file descriptor
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
{'gpu': '0',
 'max_epoch': 200,
 'query': 15,
 'save_epoch': 20,
 'save_path': 'learngamma_MiniImageNet/save/1testshot',
 'shot': 1,
 'test_way': 5,
 'train_way': 30}
using gpu: 0
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:02<?, ?it/s, loss=7.41]  0%|          | 1/360 [00:02<12:36,  2.11s/it, loss=7.41]  1%|          | 2/360 [00:02<06:04,  1.02s/it, loss=7.41]  1%|          | 3/360 [00:02<03:58,  1.50it/s, loss=7.41]  1%|          | 4/360 [00:02<02:59,  1.99it/s, loss=7.41]  1%|▏         | 5/360 [00:03<02:24,  2.45it/s, loss=7.41]  2%|▏         | 6/360 [00:03<02:18,  2.57it/s, loss=7.41]  2%|▏         | 7/360 [00:03<01:59,  2.94it/s, loss=7.41]  2%|▏         | 8/360 [00:03<01:48,  3.25it/s, loss=7.41]  2%|▎         | 9/360 [00:04<01:40,  3.49it/s, loss=7.41]  3%|▎         | 10/360 [00:04<01:35,  3.67it/s, loss=7.41]  3%|▎         | 10/360 [00:04<01:35,  3.67it/s, loss=4.95]  3%|▎         | 11/360 [00:04<01:31,  3.80it/s, loss=4.95]  3%|▎         | 12/360 [00:04<01:29,  3.90it/s, loss=4.95]  4%|▎         | 13/360 [00:05<01:26,  4.01it/s, loss=4.95]  4%|▍         | 14/360 [00:05<01:24,  4.09it/s, loss=4.95]  4%|▍         | 15/360 [00:05<01:23,  4.15it/s, loss=4.95]  4%|▍         | 16/360 [00:05<01:21,  4.20it/s, loss=4.95]  5%|▍         | 17/360 [00:06<01:20,  4.24it/s, loss=4.95]  5%|▌         | 18/360 [00:06<01:20,  4.27it/s, loss=4.95]  5%|▌         | 19/360 [00:06<01:19,  4.28it/s, loss=4.95]  6%|▌         | 20/360 [00:06<01:19,  4.30it/s, loss=4.95]  6%|▌         | 20/360 [00:06<01:19,  4.30it/s, loss=4.01]  6%|▌         | 21/360 [00:06<01:18,  4.30it/s, loss=4.01]  6%|▌         | 22/360 [00:07<01:18,  4.31it/s, loss=4.01]  6%|▋         | 23/360 [00:07<01:18,  4.31it/s, loss=4.01]  7%|▋         | 24/360 [00:07<01:18,  4.29it/s, loss=4.01]  7%|▋         | 25/360 [00:07<01:17,  4.30it/s, loss=4.01]  7%|▋         | 26/360 [00:08<01:17,  4.30it/s, loss=4.01]  8%|▊         | 27/360 [00:08<01:17,  4.31it/s, loss=4.01]  8%|▊         | 28/360 [00:08<01:16,  4.31it/s, loss=4.01]  8%|▊         | 29/360 [00:08<01:16,  4.32it/s, loss=4.01]  8%|▊         | 30/360 [00:09<01:16,  4.33it/s, loss=4.01]  8%|▊         | 30/360 [00:09<01:16,  4.33it/s, loss=3.42]  9%|▊         | 31/360 [00:09<01:16,  4.33it/s, loss=3.42]  9%|▉         | 32/360 [00:09<01:15,  4.32it/s, loss=3.42]  9%|▉         | 33/360 [00:09<01:15,  4.31it/s, loss=3.42]  9%|▉         | 34/360 [00:10<01:15,  4.31it/s, loss=3.42] 10%|▉         | 35/360 [00:10<01:15,  4.31it/s, loss=3.42] 10%|█         | 36/360 [00:10<01:15,  4.31it/s, loss=3.42] 10%|█         | 37/360 [00:10<01:14,  4.31it/s, loss=3.42] 11%|█         | 38/360 [00:10<01:14,  4.32it/s, loss=3.42] 11%|█         | 39/360 [00:11<01:14,  4.32it/s, loss=3.42] 11%|█         | 40/360 [00:11<01:14,  4.32it/s, loss=3.42] 11%|█         | 40/360 [00:11<01:14,  4.32it/s, loss=3.09] 11%|█▏        | 41/360 [00:11<01:13,  4.32it/s, loss=3.09] 12%|█▏        | 42/360 [00:11<01:13,  4.32it/s, loss=3.09] 12%|█▏        | 43/360 [00:12<01:13,  4.32it/s, loss=3.09] 12%|█▏        | 44/360 [00:12<01:13,  4.32it/s, loss=3.09] 12%|█▎        | 45/360 [00:12<01:20,  3.93it/s, loss=3.09] 13%|█▎        | 46/360 [00:12<01:17,  4.06it/s, loss=3.09] 13%|█▎        | 47/360 [00:13<01:15,  4.13it/s, loss=3.09] 13%|█▎        | 48/360 [00:13<01:14,  4.18it/s, loss=3.09] 14%|█▎        | 49/360 [00:13<01:14,  4.20it/s, loss=3.09] 14%|█▍        | 50/360 [00:13<01:13,  4.23it/s, loss=3.09] 14%|█▍        | 50/360 [00:14<01:13,  4.23it/s, loss=2.71] 14%|█▍        | 51/360 [00:14<01:12,  4.24it/s, loss=2.71] 14%|█▍        | 52/360 [00:14<01:12,  4.26it/s, loss=2.71] 15%|█▍        | 53/360 [00:14<01:11,  4.27it/s, loss=2.71] 15%|█▌        | 54/360 [00:14<01:11,  4.28it/s, loss=2.71] 15%|█▌        | 55/360 [00:14<01:11,  4.28it/s, loss=2.71] 16%|█▌        | 56/360 [00:15<01:11,  4.28it/s, loss=2.71] 16%|█▌        | 57/360 [00:15<01:10,  4.28it/s, loss=2.71] 16%|█▌        | 58/360 [00:15<01:10,  4.28it/s, loss=2.71] 16%|█▋        | 59/360 [00:15<01:10,  4.29it/s, loss=2.71] 17%|█▋        | 60/360 [00:16<01:09,  4.29it/s, loss=2.71] 17%|█▋        | 60/360 [00:16<01:09,  4.29it/s, loss=2.65] 17%|█▋        | 61/360 [00:16<01:09,  4.29it/s, loss=2.65] 17%|█▋        | 62/360 [00:16<01:09,  4.29it/s, loss=2.65] 18%|█▊        | 63/360 [00:16<01:09,  4.28it/s, loss=2.65] 18%|█▊        | 64/360 [00:17<01:09,  4.27it/s, loss=2.65] 18%|█▊        | 65/360 [00:17<01:08,  4.29it/s, loss=2.65] 18%|█▊        | 66/360 [00:17<01:08,  4.29it/s, loss=2.65] 19%|█▊        | 67/360 [00:17<01:10,  4.16it/s, loss=2.65] 19%|█▉        | 68/360 [00:18<01:11,  4.11it/s, loss=2.65] 19%|█▉        | 69/360 [00:18<01:09,  4.16it/s, loss=2.65] 19%|█▉        | 70/360 [00:18<01:09,  4.19it/s, loss=2.65] 19%|█▉        | 70/360 [00:18<01:09,  4.19it/s, loss=2.42] 20%|█▉        | 71/360 [00:18<01:08,  4.23it/s, loss=2.42] 20%|██        | 72/360 [00:18<01:07,  4.26it/s, loss=2.42] 20%|██        | 73/360 [00:19<01:07,  4.27it/s, loss=2.42] 21%|██        | 74/360 [00:19<01:06,  4.29it/s, loss=2.42] 21%|██        | 75/360 [00:19<01:06,  4.30it/s, loss=2.42] 21%|██        | 76/360 [00:19<01:06,  4.30it/s, loss=2.42] 21%|██▏       | 77/360 [00:20<01:06,  4.25it/s, loss=2.42] 22%|██▏       | 78/360 [00:20<01:06,  4.26it/s, loss=2.42] 22%|██▏       | 79/360 [00:20<01:06,  4.22it/s, loss=2.42] 22%|██▏       | 80/360 [00:20<01:05,  4.25it/s, loss=2.42] 22%|██▏       | 80/360 [00:21<01:05,  4.25it/s, loss=2.26] 22%|██▎       | 81/360 [00:21<01:05,  4.27it/s, loss=2.26] 23%|██▎       | 82/360 [00:21<01:04,  4.28it/s, loss=2.26] 23%|██▎       | 83/360 [00:21<01:04,  4.30it/s, loss=2.26] 23%|██▎       | 84/360 [00:21<01:04,  4.30it/s, loss=2.26] 24%|██▎       | 85/360 [00:22<01:10,  3.90it/s, loss=2.26] 24%|██▍       | 86/360 [00:22<01:07,  4.03it/s, loss=2.26] 24%|██▍       | 87/360 [00:22<01:06,  4.11it/s, loss=2.26] 24%|██▍       | 88/360 [00:22<01:05,  4.17it/s, loss=2.26] 25%|██▍       | 89/360 [00:22<01:04,  4.21it/s, loss=2.26] 25%|██▌       | 90/360 [00:23<01:03,  4.23it/s, loss=2.26] 25%|██▌       | 90/360 [00:23<01:03,  4.23it/s, loss=2.2]  25%|██▌       | 91/360 [00:23<01:03,  4.25it/s, loss=2.2] 26%|██▌       | 92/360 [00:23<01:02,  4.26it/s, loss=2.2] 26%|██▌       | 93/360 [00:23<01:02,  4.26it/s, loss=2.2] 26%|██▌       | 94/360 [00:24<01:02,  4.27it/s, loss=2.2] 26%|██▋       | 95/360 [00:24<01:02,  4.26it/s, loss=2.2] 27%|██▋       | 96/360 [00:24<01:01,  4.26it/s, loss=2.2] 27%|██▋       | 97/360 [00:24<01:01,  4.27it/s, loss=2.2] 27%|██▋       | 98/360 [00:25<01:01,  4.27it/s, loss=2.2] 28%|██▊       | 99/360 [00:25<01:01,  4.27it/s, loss=2.2] 28%|██▊       | 100/360 [00:25<01:00,  4.28it/s, loss=2.2] 28%|██▊       | 100/360 [00:25<01:00,  4.28it/s, loss=2.13] 28%|██▊       | 101/360 [00:25<01:01,  4.24it/s, loss=2.13] 28%|██▊       | 102/360 [00:26<01:00,  4.25it/s, loss=2.13] 29%|██▊       | 103/360 [00:26<01:00,  4.26it/s, loss=2.13] 29%|██▉       | 104/360 [00:26<01:00,  4.26it/s, loss=2.13] 29%|██▉       | 105/360 [00:26<00:59,  4.27it/s, loss=2.13] 29%|██▉       | 106/360 [00:26<00:59,  4.27it/s, loss=2.13] 30%|██▉       | 107/360 [00:27<00:59,  4.27it/s, loss=2.13] 30%|███       | 108/360 [00:27<00:58,  4.27it/s, loss=2.13] 30%|███       | 109/360 [00:27<00:58,  4.26it/s, loss=2.13] 31%|███       | 110/360 [00:27<00:58,  4.26it/s, loss=2.13] 31%|███       | 110/360 [00:28<00:58,  4.26it/s, loss=1.93] 31%|███       | 111/360 [00:28<01:04,  3.86it/s, loss=1.93] 31%|███       | 112/360 [00:28<01:02,  4.00it/s, loss=1.93] 31%|███▏      | 113/360 [00:28<01:00,  4.09it/s, loss=1.93] 32%|███▏      | 114/360 [00:28<00:59,  4.16it/s, loss=1.93] 32%|███▏      | 115/360 [00:29<00:58,  4.19it/s, loss=1.93] 32%|███▏      | 116/360 [00:29<00:57,  4.22it/s, loss=1.93] 32%|███▎      | 117/360 [00:29<00:57,  4.25it/s, loss=1.93] 33%|███▎      | 118/360 [00:29<00:57,  4.24it/s, loss=1.93] 33%|███▎      | 119/360 [00:30<00:57,  4.23it/s, loss=1.93] 33%|███▎      | 120/360 [00:30<00:56,  4.24it/s, loss=1.93] 33%|███▎      | 120/360 [00:30<00:56,  4.24it/s, loss=1.94] 34%|███▎      | 121/360 [00:30<00:56,  4.24it/s, loss=1.94] 34%|███▍      | 122/360 [00:30<00:56,  4.25it/s, loss=1.94] 34%|███▍      | 123/360 [00:31<00:55,  4.25it/s, loss=1.94] 34%|███▍      | 124/360 [00:31<00:55,  4.24it/s, loss=1.94] 35%|███▍      | 125/360 [00:31<00:55,  4.22it/s, loss=1.94] 35%|███▌      | 126/360 [00:31<00:55,  4.21it/s, loss=1.94] 35%|███▌      | 127/360 [00:32<00:56,  4.14it/s, loss=1.94] 36%|███▌      | 128/360 [00:32<00:55,  4.15it/s, loss=1.94] 36%|███▌      | 129/360 [00:32<00:55,  4.17it/s, loss=1.94] 36%|███▌      | 130/360 [00:32<00:54,  4.20it/s, loss=1.94] 36%|███▌      | 130/360 [00:32<00:54,  4.20it/s, loss=1.77] 36%|███▋      | 131/360 [00:32<00:54,  4.22it/s, loss=1.77] 37%|███▋      | 132/360 [00:33<00:53,  4.24it/s, loss=1.77] 37%|███▋      | 133/360 [00:33<00:53,  4.25it/s, loss=1.77] 37%|███▋      | 134/360 [00:33<00:53,  4.24it/s, loss=1.77] 38%|███▊      | 135/360 [00:33<00:53,  4.23it/s, loss=1.77] 38%|███▊      | 136/360 [00:34<00:52,  4.25it/s, loss=1.77] 38%|███▊      | 137/360 [00:34<00:57,  3.86it/s, loss=1.77] 38%|███▊      | 138/360 [00:34<00:55,  3.99it/s, loss=1.77] 39%|███▊      | 139/360 [00:34<00:54,  4.08it/s, loss=1.77] 39%|███▉      | 140/360 [00:35<00:52,  4.15it/s, loss=1.77] 39%|███▉      | 140/360 [00:35<00:52,  4.15it/s, loss=1.86] 39%|███▉      | 141/360 [00:35<00:52,  4.19it/s, loss=1.86] 39%|███▉      | 142/360 [00:35<00:51,  4.19it/s, loss=1.86] 40%|███▉      | 143/360 [00:35<00:51,  4.21it/s, loss=1.86] 40%|████      | 144/360 [00:36<00:51,  4.23it/s, loss=1.86] 40%|████      | 145/360 [00:36<00:50,  4.23it/s, loss=1.86] 41%|████      | 146/360 [00:36<00:50,  4.24it/s, loss=1.86] 41%|████      | 147/360 [00:36<00:50,  4.24it/s, loss=1.86] 41%|████      | 148/360 [00:37<00:49,  4.25it/s, loss=1.86] 41%|████▏     | 149/360 [00:37<00:49,  4.24it/s, loss=1.86] 42%|████▏     | 150/360 [00:37<00:49,  4.25it/s, loss=1.86] 42%|████▏     | 150/360 [00:37<00:49,  4.25it/s, loss=1.84] 42%|████▏     | 151/360 [00:37<00:49,  4.22it/s, loss=1.84] 42%|████▏     | 152/360 [00:37<00:49,  4.23it/s, loss=1.84] 42%|████▎     | 153/360 [00:38<00:48,  4.23it/s, loss=1.84] 43%|████▎     | 154/360 [00:38<00:48,  4.23it/s, loss=1.84] 43%|████▎     | 155/360 [00:38<00:48,  4.19it/s, loss=1.84] 43%|████▎     | 156/360 [00:38<00:48,  4.22it/s, loss=1.84] 44%|████▎     | 157/360 [00:39<00:47,  4.23it/s, loss=1.84] 44%|████▍     | 158/360 [00:39<00:47,  4.24it/s, loss=1.84] 44%|████▍     | 159/360 [00:39<00:47,  4.26it/s, loss=1.84] 44%|████▍     | 160/360 [00:39<00:47,  4.26it/s, loss=1.84] 44%|████▍     | 160/360 [00:40<00:47,  4.26it/s, loss=1.77] 45%|████▍     | 161/360 [00:40<00:46,  4.26it/s, loss=1.77] 45%|████▌     | 162/360 [00:40<00:46,  4.26it/s, loss=1.77] 45%|████▌     | 163/360 [00:40<00:51,  3.86it/s, loss=1.77] 46%|████▌     | 164/360 [00:40<00:49,  4.00it/s, loss=1.77] 46%|████▌     | 165/360 [00:41<00:47,  4.07it/s, loss=1.77] 46%|████▌     | 166/360 [00:41<00:46,  4.13it/s, loss=1.77] 46%|████▋     | 167/360 [00:41<00:46,  4.18it/s, loss=1.77] 47%|████▋     | 168/360 [00:41<00:45,  4.21it/s, loss=1.77] 47%|████▋     | 169/360 [00:42<00:45,  4.24it/s, loss=1.77] 47%|████▋     | 170/360 [00:42<00:44,  4.25it/s, loss=1.77] 47%|████▋     | 170/360 [00:42<00:44,  4.25it/s, loss=1.63] 48%|████▊     | 171/360 [00:42<00:44,  4.25it/s, loss=1.63] 48%|████▊     | 172/360 [00:42<00:44,  4.25it/s, loss=1.63] 48%|████▊     | 173/360 [00:42<00:43,  4.26it/s, loss=1.63] 48%|████▊     | 174/360 [00:43<00:43,  4.27it/s, loss=1.63] 49%|████▊     | 175/360 [00:43<00:43,  4.27it/s, loss=1.63] 49%|████▉     | 176/360 [00:43<00:43,  4.27it/s, loss=1.63] 49%|████▉     | 177/360 [00:43<00:42,  4.28it/s, loss=1.63] 49%|████▉     | 178/360 [00:44<00:42,  4.27it/s, loss=1.63] 50%|████▉     | 179/360 [00:44<00:42,  4.26it/s, loss=1.63] 50%|█████     | 180/360 [00:44<00:42,  4.24it/s, loss=1.63] 50%|█████     | 180/360 [00:44<00:42,  4.24it/s, loss=1.56] 50%|█████     | 181/360 [00:44<00:42,  4.25it/s, loss=1.56] 51%|█████     | 182/360 [00:45<00:41,  4.26it/s, loss=1.56] 51%|█████     | 183/360 [00:45<00:41,  4.26it/s, loss=1.56] 51%|█████     | 184/360 [00:45<00:41,  4.26it/s, loss=1.56] 51%|█████▏    | 185/360 [00:45<00:41,  4.26it/s, loss=1.56] 52%|█████▏    | 186/360 [00:46<00:40,  4.25it/s, loss=1.56] 52%|█████▏    | 187/360 [00:46<00:40,  4.24it/s, loss=1.56] 52%|█████▏    | 188/360 [00:46<00:40,  4.24it/s, loss=1.56] 52%|█████▎    | 189/360 [00:46<00:44,  3.83it/s, loss=1.56] 53%|█████▎    | 190/360 [00:47<00:42,  3.96it/s, loss=1.56] 53%|█████▎    | 190/360 [00:47<00:42,  3.96it/s, loss=1.61] 53%|█████▎    | 191/360 [00:47<00:41,  4.04it/s, loss=1.61] 53%|█████▎    | 192/360 [00:47<00:40,  4.10it/s, loss=1.61] 54%|█████▎    | 193/360 [00:47<00:40,  4.15it/s, loss=1.61] 54%|█████▍    | 194/360 [00:47<00:39,  4.17it/s, loss=1.61] 54%|█████▍    | 195/360 [00:48<00:39,  4.20it/s, loss=1.61] 54%|█████▍    | 196/360 [00:48<00:38,  4.22it/s, loss=1.61] 55%|█████▍    | 197/360 [00:48<00:38,  4.24it/s, loss=1.61] 55%|█████▌    | 198/360 [00:48<00:38,  4.24it/s, loss=1.61] 55%|█████▌    | 199/360 [00:49<00:37,  4.25it/s, loss=1.61] 56%|█████▌    | 200/360 [00:49<00:37,  4.26it/s, loss=1.61] 56%|█████▌    | 200/360 [00:49<00:37,  4.26it/s, loss=1.59] 56%|█████▌    | 201/360 [00:49<00:37,  4.26it/s, loss=1.59] 56%|█████▌    | 202/360 [00:49<00:37,  4.25it/s, loss=1.59] 56%|█████▋    | 203/360 [00:50<00:36,  4.25it/s, loss=1.59] 57%|█████▋    | 204/360 [00:50<00:36,  4.25it/s, loss=1.59] 57%|█████▋    | 205/360 [00:50<00:36,  4.24it/s, loss=1.59] 57%|█████▋    | 206/360 [00:50<00:36,  4.25it/s, loss=1.59] 57%|█████▊    | 207/360 [00:51<00:36,  4.25it/s, loss=1.59] 58%|█████▊    | 208/360 [00:51<00:35,  4.24it/s, loss=1.59] 58%|█████▊    | 209/360 [00:51<00:35,  4.21it/s, loss=1.59] 58%|█████▊    | 210/360 [00:51<00:35,  4.23it/s, loss=1.59] 58%|█████▊    | 210/360 [00:51<00:35,  4.23it/s, loss=1.59] 59%|█████▊    | 211/360 [00:51<00:35,  4.23it/s, loss=1.59] 59%|█████▉    | 212/360 [00:52<00:34,  4.24it/s, loss=1.59] 59%|█████▉    | 213/360 [00:52<00:34,  4.22it/s, loss=1.59] 59%|█████▉    | 214/360 [00:52<00:34,  4.22it/s, loss=1.59] 60%|█████▉    | 215/360 [00:53<00:37,  3.82it/s, loss=1.59] 60%|██████    | 216/360 [00:53<00:36,  3.96it/s, loss=1.59] 60%|██████    | 217/360 [00:53<00:35,  4.06it/s, loss=1.59] 61%|██████    | 218/360 [00:53<00:35,  4.04it/s, loss=1.59] 61%|██████    | 219/360 [00:53<00:34,  4.10it/s, loss=1.59] 61%|██████    | 220/360 [00:54<00:33,  4.15it/s, loss=1.59] 61%|██████    | 220/360 [00:54<00:33,  4.15it/s, loss=1.5]  61%|██████▏   | 221/360 [00:54<00:33,  4.17it/s, loss=1.5] 62%|██████▏   | 222/360 [00:54<00:32,  4.20it/s, loss=1.5] 62%|██████▏   | 223/360 [00:54<00:32,  4.20it/s, loss=1.5] 62%|██████▏   | 224/360 [00:55<00:32,  4.23it/s, loss=1.5] 62%|██████▎   | 225/360 [00:55<00:32,  4.17it/s, loss=1.5] 63%|██████▎   | 226/360 [00:55<00:31,  4.20it/s, loss=1.5] 63%|██████▎   | 227/360 [00:55<00:31,  4.19it/s, loss=1.5] 63%|██████▎   | 228/360 [00:56<00:31,  4.21it/s, loss=1.5] 64%|██████▎   | 229/360 [00:56<00:30,  4.23it/s, loss=1.5] 64%|██████▍   | 230/360 [00:56<00:30,  4.24it/s, loss=1.5] 64%|██████▍   | 230/360 [00:56<00:30,  4.24it/s, loss=1.44] 64%|██████▍   | 231/360 [00:56<00:30,  4.25it/s, loss=1.44] 64%|██████▍   | 232/360 [00:57<00:30,  4.26it/s, loss=1.44] 65%|██████▍   | 233/360 [00:57<00:29,  4.26it/s, loss=1.44] 65%|██████▌   | 234/360 [00:57<00:29,  4.27it/s, loss=1.44] 65%|██████▌   | 235/360 [00:57<00:29,  4.27it/s, loss=1.44] 66%|██████▌   | 236/360 [00:57<00:29,  4.27it/s, loss=1.44] 66%|██████▌   | 237/360 [00:58<00:28,  4.26it/s, loss=1.44] 66%|██████▌   | 238/360 [00:58<00:28,  4.25it/s, loss=1.44] 66%|██████▋   | 239/360 [00:58<00:28,  4.26it/s, loss=1.44] 67%|██████▋   | 240/360 [00:58<00:28,  4.27it/s, loss=1.44] 67%|██████▋   | 240/360 [00:59<00:28,  4.27it/s, loss=1.39] 67%|██████▋   | 241/360 [00:59<00:30,  3.85it/s, loss=1.39] 67%|██████▋   | 242/360 [00:59<00:29,  3.97it/s, loss=1.39] 68%|██████▊   | 243/360 [00:59<00:28,  4.06it/s, loss=1.39] 68%|██████▊   | 244/360 [00:59<00:28,  4.13it/s, loss=1.39] 68%|██████▊   | 245/360 [01:00<00:27,  4.18it/s, loss=1.39] 68%|██████▊   | 246/360 [01:00<00:27,  4.21it/s, loss=1.39] 69%|██████▊   | 247/360 [01:00<00:26,  4.23it/s, loss=1.39] 69%|██████▉   | 248/360 [01:00<00:26,  4.24it/s, loss=1.39] 69%|██████▉   | 249/360 [01:01<00:26,  4.25it/s, loss=1.39] 69%|██████▉   | 250/360 [01:01<00:25,  4.26it/s, loss=1.39] 69%|██████▉   | 250/360 [01:01<00:25,  4.26it/s, loss=1.41] 70%|██████▉   | 251/360 [01:01<00:25,  4.27it/s, loss=1.41] 70%|███████   | 252/360 [01:01<00:25,  4.27it/s, loss=1.41] 70%|███████   | 253/360 [01:02<00:25,  4.27it/s, loss=1.41] 71%|███████   | 254/360 [01:02<00:24,  4.27it/s, loss=1.41] 71%|███████   | 255/360 [01:02<00:24,  4.27it/s, loss=1.41] 71%|███████   | 256/360 [01:02<00:24,  4.27it/s, loss=1.41] 71%|███████▏  | 257/360 [01:02<00:24,  4.27it/s, loss=1.41] 72%|███████▏  | 258/360 [01:03<00:23,  4.27it/s, loss=1.41] 72%|███████▏  | 259/360 [01:03<00:23,  4.27it/s, loss=1.41] 72%|███████▏  | 260/360 [01:03<00:23,  4.26it/s, loss=1.41] 72%|███████▏  | 260/360 [01:03<00:23,  4.26it/s, loss=1.41] 72%|███████▎  | 261/360 [01:03<00:23,  4.27it/s, loss=1.41] 73%|███████▎  | 262/360 [01:04<00:22,  4.26it/s, loss=1.41] 73%|███████▎  | 263/360 [01:04<00:23,  4.20it/s, loss=1.41] 73%|███████▎  | 264/360 [01:04<00:22,  4.22it/s, loss=1.41] 74%|███████▎  | 265/360 [01:04<00:22,  4.24it/s, loss=1.41] 74%|███████▍  | 266/360 [01:05<00:22,  4.23it/s, loss=1.41] 74%|███████▍  | 267/360 [01:05<00:21,  4.24it/s, loss=1.41] 74%|███████▍  | 268/360 [01:05<00:24,  3.83it/s, loss=1.41] 75%|███████▍  | 269/360 [01:05<00:22,  3.97it/s, loss=1.41] 75%|███████▌  | 270/360 [01:06<00:22,  4.07it/s, loss=1.41] 75%|███████▌  | 270/360 [01:06<00:22,  4.07it/s, loss=1.42] 75%|███████▌  | 271/360 [01:06<00:21,  4.13it/s, loss=1.42] 76%|███████▌  | 272/360 [01:06<00:21,  4.18it/s, loss=1.42] 76%|███████▌  | 273/360 [01:06<00:20,  4.22it/s, loss=1.42] 76%|███████▌  | 274/360 [01:07<00:20,  4.23it/s, loss=1.42] 76%|███████▋  | 275/360 [01:07<00:20,  4.24it/s, loss=1.42] 77%|███████▋  | 276/360 [01:07<00:19,  4.26it/s, loss=1.42] 77%|███████▋  | 277/360 [01:07<00:19,  4.27it/s, loss=1.42] 77%|███████▋  | 278/360 [01:07<00:19,  4.28it/s, loss=1.42] 78%|███████▊  | 279/360 [01:08<00:18,  4.28it/s, loss=1.42] 78%|███████▊  | 280/360 [01:08<00:18,  4.26it/s, loss=1.42] 78%|███████▊  | 280/360 [01:08<00:18,  4.26it/s, loss=1.47] 78%|███████▊  | 281/360 [01:08<00:18,  4.25it/s, loss=1.47] 78%|███████▊  | 282/360 [01:08<00:18,  4.25it/s, loss=1.47] 79%|███████▊  | 283/360 [01:09<00:18,  4.26it/s, loss=1.47] 79%|███████▉  | 284/360 [01:09<00:17,  4.27it/s, loss=1.47] 79%|███████▉  | 285/360 [01:09<00:17,  4.26it/s, loss=1.47] 79%|███████▉  | 286/360 [01:09<00:17,  4.24it/s, loss=1.47] 80%|███████▉  | 287/360 [01:10<00:17,  4.25it/s, loss=1.47] 80%|████████  | 288/360 [01:10<00:16,  4.26it/s, loss=1.47] 80%|████████  | 289/360 [01:10<00:16,  4.26it/s, loss=1.47] 81%|████████  | 290/360 [01:10<00:16,  4.26it/s, loss=1.47] 81%|████████  | 290/360 [01:11<00:16,  4.26it/s, loss=1.32] 81%|████████  | 291/360 [01:11<00:16,  4.24it/s, loss=1.32] 81%|████████  | 292/360 [01:11<00:16,  4.25it/s, loss=1.32] 81%|████████▏ | 293/360 [01:11<00:15,  4.25it/s, loss=1.32] 82%|████████▏ | 294/360 [01:11<00:15,  4.26it/s, loss=1.32] 82%|████████▏ | 295/360 [01:12<00:16,  3.85it/s, loss=1.32] 82%|████████▏ | 296/360 [01:12<00:16,  3.97it/s, loss=1.32] 82%|████████▎ | 297/360 [01:12<00:15,  4.07it/s, loss=1.32] 83%|████████▎ | 298/360 [01:12<00:15,  4.13it/s, loss=1.32] 83%|████████▎ | 299/360 [01:12<00:14,  4.17it/s, loss=1.32] 83%|████████▎ | 300/360 [01:13<00:14,  4.20it/s, loss=1.32] 83%|████████▎ | 300/360 [01:13<00:14,  4.20it/s, loss=1.39] 84%|████████▎ | 301/360 [01:13<00:13,  4.23it/s, loss=1.39] 84%|████████▍ | 302/360 [01:13<00:13,  4.24it/s, loss=1.39] 84%|████████▍ | 303/360 [01:13<00:13,  4.24it/s, loss=1.39] 84%|████████▍ | 304/360 [01:14<00:13,  4.25it/s, loss=1.39] 85%|████████▍ | 305/360 [01:14<00:13,  4.21it/s, loss=1.39] 85%|████████▌ | 306/360 [01:14<00:12,  4.23it/s, loss=1.39] 85%|████████▌ | 307/360 [01:14<00:12,  4.24it/s, loss=1.39] 86%|████████▌ | 308/360 [01:15<00:12,  4.25it/s, loss=1.39] 86%|████████▌ | 309/360 [01:15<00:11,  4.25it/s, loss=1.39] 86%|████████▌ | 310/360 [01:15<00:11,  4.25it/s, loss=1.39] 86%|████████▌ | 310/360 [01:15<00:11,  4.25it/s, loss=1.28] 86%|████████▋ | 311/360 [01:15<00:11,  4.22it/s, loss=1.28] 87%|████████▋ | 312/360 [01:16<00:11,  4.23it/s, loss=1.28] 87%|████████▋ | 313/360 [01:16<00:11,  4.24it/s, loss=1.28] 87%|████████▋ | 314/360 [01:16<00:10,  4.24it/s, loss=1.28] 88%|████████▊ | 315/360 [01:16<00:10,  4.24it/s, loss=1.28] 88%|████████▊ | 316/360 [01:17<00:10,  4.17it/s, loss=1.28] 88%|████████▊ | 317/360 [01:17<00:10,  4.20it/s, loss=1.28] 88%|████████▊ | 318/360 [01:17<00:09,  4.21it/s, loss=1.28] 89%|████████▊ | 319/360 [01:17<00:09,  4.21it/s, loss=1.28] 89%|████████▉ | 320/360 [01:17<00:09,  4.21it/s, loss=1.28] 89%|████████▉ | 320/360 [01:18<00:09,  4.21it/s, loss=1.23] 89%|████████▉ | 321/360 [01:18<00:09,  4.21it/s, loss=1.23] 89%|████████▉ | 322/360 [01:18<00:10,  3.80it/s, loss=1.23] 90%|████████▉ | 323/360 [01:18<00:09,  3.94it/s, loss=1.23] 90%|█████████ | 324/360 [01:18<00:08,  4.04it/s, loss=1.23] 90%|█████████ | 325/360 [01:19<00:08,  4.11it/s, loss=1.23] 91%|█████████ | 326/360 [01:19<00:08,  4.14it/s, loss=1.23] 91%|█████████ | 327/360 [01:19<00:07,  4.18it/s, loss=1.23] 91%|█████████ | 328/360 [01:19<00:07,  4.21it/s, loss=1.23] 91%|█████████▏| 329/360 [01:20<00:07,  4.22it/s, loss=1.23] 92%|█████████▏| 330/360 [01:20<00:07,  4.22it/s, loss=1.23] 92%|█████████▏| 330/360 [01:20<00:07,  4.22it/s, loss=1.24] 92%|█████████▏| 331/360 [01:20<00:06,  4.23it/s, loss=1.24] 92%|█████████▏| 332/360 [01:20<00:06,  4.24it/s, loss=1.24] 92%|█████████▎| 333/360 [01:21<00:06,  4.23it/s, loss=1.24] 93%|█████████▎| 334/360 [01:21<00:06,  4.24it/s, loss=1.24] 93%|█████████▎| 335/360 [01:21<00:05,  4.25it/s, loss=1.24] 93%|█████████▎| 336/360 [01:21<00:05,  4.26it/s, loss=1.24] 94%|█████████▎| 337/360 [01:22<00:05,  4.28it/s, loss=1.24] 94%|█████████▍| 338/360 [01:22<00:05,  4.31it/s, loss=1.24] 94%|█████████▍| 339/360 [01:22<00:04,  4.34it/s, loss=1.24] 94%|█████████▍| 340/360 [01:22<00:04,  4.35it/s, loss=1.24] 94%|█████████▍| 340/360 [01:22<00:04,  4.35it/s, loss=1.29] 95%|█████████▍| 341/360 [01:22<00:04,  4.36it/s, loss=1.29] 95%|█████████▌| 342/360 [01:23<00:04,  4.37it/s, loss=1.29] 95%|█████████▌| 343/360 [01:23<00:03,  4.38it/s, loss=1.29] 96%|█████████▌| 344/360 [01:23<00:03,  4.38it/s, loss=1.29] 96%|█████████▌| 345/360 [01:23<00:03,  4.38it/s, loss=1.29] 96%|█████████▌| 346/360 [01:24<00:03,  4.38it/s, loss=1.29] 96%|█████████▋| 347/360 [01:24<00:02,  4.38it/s, loss=1.29] 97%|█████████▋| 348/360 [01:24<00:02,  4.38it/s, loss=1.29] 97%|█████████▋| 349/360 [01:24<00:02,  3.97it/s, loss=1.29] 97%|█████████▋| 350/360 [01:25<00:02,  4.09it/s, loss=1.29] 97%|█████████▋| 350/360 [01:25<00:02,  4.09it/s, loss=1.22] 98%|█████████▊| 351/360 [01:25<00:02,  4.18it/s, loss=1.22] 98%|█████████▊| 352/360 [01:25<00:01,  4.24it/s, loss=1.22] 98%|█████████▊| 353/360 [01:25<00:01,  4.29it/s, loss=1.22] 98%|█████████▊| 354/360 [01:25<00:01,  4.32it/s, loss=1.22] 99%|█████████▊| 355/360 [01:26<00:01,  4.34it/s, loss=1.22] 99%|█████████▉| 356/360 [01:26<00:00,  4.36it/s, loss=1.22] 99%|█████████▉| 357/360 [01:26<00:00,  4.36it/s, loss=1.22] 99%|█████████▉| 358/360 [01:26<00:00,  4.37it/s, loss=1.22]100%|█████████▉| 359/360 [01:27<00:00,  4.37it/s, loss=1.22]100%|██████████| 360/360 [01:27<00:00,  4.38it/s, loss=1.22]100%|██████████| 360/360 [01:27<00:00,  4.12it/s, loss=1.22]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:36,  2.73it/s]  2%|▏         | 2/100 [00:00<00:21,  4.63it/s]  3%|▎         | 3/100 [00:00<00:16,  5.99it/s]  4%|▍         | 4/100 [00:00<00:13,  6.91it/s]  5%|▌         | 5/100 [00:00<00:12,  7.58it/s]  6%|▌         | 6/100 [00:00<00:11,  7.94it/s]  7%|▋         | 7/100 [00:01<00:11,  8.23it/s]  8%|▊         | 8/100 [00:01<00:10,  8.46it/s]  9%|▉         | 9/100 [00:01<00:10,  8.63it/s] 10%|█         | 10/100 [00:01<00:10,  8.70it/s] 11%|█         | 11/100 [00:01<00:10,  8.73it/s] 12%|█▏        | 12/100 [00:01<00:09,  8.82it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.92it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.96it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.63it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.75it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.55it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.68it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.49it/s] 20%|██        | 20/100 [00:02<00:09,  8.39it/s] 21%|██        | 21/100 [00:02<00:09,  8.29it/s] 22%|██▏       | 22/100 [00:02<00:09,  8.51it/s] 23%|██▎       | 23/100 [00:02<00:11,  6.81it/s] 24%|██▍       | 24/100 [00:03<00:10,  7.30it/s] 25%|██▌       | 25/100 [00:03<00:09,  7.56it/s] 26%|██▌       | 26/100 [00:03<00:09,  8.01it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.36it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.60it/s] 29%|██▉       | 29/100 [00:03<00:08,  8.78it/s] 30%|███       | 30/100 [00:03<00:07,  8.92it/s] 31%|███       | 31/100 [00:03<00:07,  8.93it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.99it/s] 33%|███▎      | 33/100 [00:04<00:07,  9.02it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.07it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.08it/s] 36%|███▌      | 36/100 [00:04<00:07,  8.93it/s] 37%|███▋      | 37/100 [00:04<00:07,  8.94it/s] 38%|███▊      | 38/100 [00:04<00:06,  8.96it/s] 39%|███▉      | 39/100 [00:04<00:06,  8.99it/s] 40%|████      | 40/100 [00:04<00:06,  8.96it/s] 41%|████      | 41/100 [00:04<00:06,  8.98it/s] 42%|████▏     | 42/100 [00:05<00:06,  9.06it/s] 43%|████▎     | 43/100 [00:05<00:06,  9.03it/s] 44%|████▍     | 44/100 [00:05<00:06,  9.08it/s] 45%|████▌     | 45/100 [00:05<00:06,  9.11it/s] 46%|████▌     | 46/100 [00:05<00:05,  9.15it/s] 47%|████▋     | 47/100 [00:05<00:05,  9.14it/s] 48%|████▊     | 48/100 [00:05<00:05,  9.11it/s] 49%|████▉     | 49/100 [00:05<00:05,  9.12it/s] 50%|█████     | 50/100 [00:05<00:05,  9.10it/s] 51%|█████     | 51/100 [00:06<00:05,  9.11it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.11it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.11it/s] 54%|█████▍    | 54/100 [00:06<00:05,  9.16it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.09it/s] 56%|█████▌    | 56/100 [00:06<00:04,  9.10it/s] 57%|█████▋    | 57/100 [00:06<00:04,  9.09it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.00it/s] 59%|█████▉    | 59/100 [00:06<00:04,  9.04it/s] 60%|██████    | 60/100 [00:07<00:04,  9.03it/s] 61%|██████    | 61/100 [00:07<00:04,  9.06it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.04it/s] 63%|██████▎   | 63/100 [00:07<00:04,  9.04it/s] 64%|██████▍   | 64/100 [00:07<00:04,  7.33it/s] 65%|██████▌   | 65/100 [00:07<00:04,  7.77it/s] 66%|██████▌   | 66/100 [00:07<00:04,  7.81it/s] 67%|██████▋   | 67/100 [00:07<00:04,  8.11it/s] 68%|██████▊   | 68/100 [00:08<00:03,  8.38it/s] 69%|██████▉   | 69/100 [00:08<00:03,  8.52it/s] 70%|███████   | 70/100 [00:08<00:03,  8.64it/s] 71%|███████   | 71/100 [00:08<00:03,  8.65it/s] 72%|███████▏  | 72/100 [00:08<00:03,  8.34it/s] 73%|███████▎  | 73/100 [00:08<00:03,  8.51it/s] 74%|███████▍  | 74/100 [00:08<00:03,  8.62it/s] 75%|███████▌  | 75/100 [00:08<00:02,  8.69it/s] 76%|███████▌  | 76/100 [00:08<00:02,  8.77it/s] 77%|███████▋  | 77/100 [00:09<00:02,  8.90it/s] 78%|███████▊  | 78/100 [00:09<00:02,  8.89it/s] 79%|███████▉  | 79/100 [00:09<00:02,  8.58it/s] 80%|████████  | 80/100 [00:09<00:02,  8.77it/s] 81%|████████  | 81/100 [00:09<00:02,  8.52it/s] 82%|████████▏ | 82/100 [00:09<00:02,  8.41it/s] 83%|████████▎ | 83/100 [00:09<00:02,  8.44it/s] 84%|████████▍ | 84/100 [00:09<00:01,  8.60it/s] 85%|████████▌ | 85/100 [00:10<00:01,  8.84it/s] 86%|████████▌ | 86/100 [00:10<00:01,  9.03it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.17it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.28it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.33it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.36it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.40it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.42it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.44it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.44it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.47it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.47it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.45it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.43it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.44it/s]100%|██████████| 100/100 [00:11<00:00,  9.44it/s]100%|██████████| 100/100 [00:11<00:00,  8.61it/s]
Model tested on 100 tasks. Accuracy: 67.45%
0.0 0.0
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:32,  3.01it/s]  2%|▏         | 2/100 [00:00<00:20,  4.77it/s]  3%|▎         | 3/100 [00:00<00:16,  5.94it/s]  4%|▍         | 4/100 [00:00<00:18,  5.06it/s]  5%|▌         | 5/100 [00:00<00:16,  5.92it/s]  6%|▌         | 6/100 [00:01<00:14,  6.56it/s]  7%|▋         | 7/100 [00:01<00:13,  7.11it/s]  8%|▊         | 8/100 [00:01<00:12,  7.55it/s]  9%|▉         | 9/100 [00:01<00:11,  7.99it/s] 10%|█         | 10/100 [00:01<00:10,  8.30it/s] 11%|█         | 11/100 [00:01<00:10,  8.54it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.70it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.81it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.89it/s] 15%|█▌        | 15/100 [00:02<00:09,  8.96it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.86it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.89it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.95it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.98it/s] 20%|██        | 20/100 [00:02<00:08,  9.01it/s] 21%|██        | 21/100 [00:02<00:08,  9.03it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.06it/s] 23%|██▎       | 23/100 [00:02<00:08,  9.09it/s] 24%|██▍       | 24/100 [00:03<00:08,  9.11it/s] 25%|██▌       | 25/100 [00:03<00:08,  9.11it/s] 26%|██▌       | 26/100 [00:03<00:08,  9.08it/s] 27%|██▋       | 27/100 [00:03<00:08,  9.08it/s] 28%|██▊       | 28/100 [00:03<00:07,  9.05it/s] 29%|██▉       | 29/100 [00:03<00:07,  9.06it/s] 30%|███       | 30/100 [00:03<00:07,  9.08it/s] 31%|███       | 31/100 [00:03<00:07,  9.10it/s] 32%|███▏      | 32/100 [00:03<00:07,  9.12it/s] 33%|███▎      | 33/100 [00:04<00:07,  9.11it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.13it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.12it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.13it/s] 37%|███▋      | 37/100 [00:04<00:06,  9.12it/s] 38%|███▊      | 38/100 [00:04<00:06,  9.12it/s] 39%|███▉      | 39/100 [00:04<00:06,  9.12it/s] 40%|████      | 40/100 [00:04<00:06,  9.12it/s] 41%|████      | 41/100 [00:04<00:06,  9.11it/s] 42%|████▏     | 42/100 [00:05<00:06,  9.09it/s] 43%|████▎     | 43/100 [00:05<00:06,  9.10it/s] 44%|████▍     | 44/100 [00:05<00:06,  9.11it/s] 45%|████▌     | 45/100 [00:05<00:06,  9.10it/s] 46%|████▌     | 46/100 [00:05<00:07,  7.32it/s] 47%|████▋     | 47/100 [00:05<00:06,  7.83it/s] 48%|████▊     | 48/100 [00:05<00:06,  8.22it/s] 49%|████▉     | 49/100 [00:05<00:06,  8.50it/s] 50%|█████     | 50/100 [00:05<00:05,  8.57it/s] 51%|█████     | 51/100 [00:06<00:05,  8.73it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.81it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.90it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.59it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.71it/s] 56%|█████▌    | 56/100 [00:06<00:05,  8.79it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.80it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.82it/s] 59%|█████▉    | 59/100 [00:07<00:04,  8.86it/s] 60%|██████    | 60/100 [00:07<00:04,  8.58it/s] 61%|██████    | 61/100 [00:07<00:04,  8.69it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.78it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.84it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.86it/s] 65%|██████▌   | 65/100 [00:07<00:03,  8.81it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.75it/s] 67%|██████▋   | 67/100 [00:07<00:03,  8.85it/s] 68%|██████▊   | 68/100 [00:08<00:03,  8.92it/s] 69%|██████▉   | 69/100 [00:08<00:03,  8.98it/s] 70%|███████   | 70/100 [00:08<00:03,  9.03it/s] 71%|███████   | 71/100 [00:08<00:03,  9.05it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.04it/s] 73%|███████▎  | 73/100 [00:08<00:03,  8.98it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.01it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.04it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.08it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.20it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.30it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.31it/s] 80%|████████  | 80/100 [00:09<00:02,  9.38it/s] 81%|████████  | 81/100 [00:09<00:02,  9.39it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.41it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.41it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.43it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.43it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.35it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.38it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.43it/s] 89%|████████▉ | 89/100 [00:10<00:01,  7.51it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.04it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.45it/s] 92%|█████████▏| 92/100 [00:10<00:00,  8.75it/s] 93%|█████████▎| 93/100 [00:10<00:00,  8.97it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.12it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.23it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.33it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.40it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.44it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.44it/s]100%|██████████| 100/100 [00:11<00:00,  9.44it/s]100%|██████████| 100/100 [00:11<00:00,  8.64it/s]
Model tested on 100 tasks. Accuracy: 40.57%
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:00<?, ?it/s, loss=6.89]  0%|          | 1/360 [00:00<05:20,  1.12it/s, loss=6.89]  1%|          | 2/360 [00:01<03:18,  1.80it/s, loss=6.89]  1%|          | 3/360 [00:01<02:27,  2.43it/s, loss=6.89]  1%|          | 4/360 [00:01<02:01,  2.92it/s, loss=6.89]  1%|▏         | 5/360 [00:01<01:49,  3.25it/s, loss=6.89]  2%|▏         | 6/360 [00:02<01:41,  3.49it/s, loss=6.89]  2%|▏         | 7/360 [00:02<01:36,  3.66it/s, loss=6.89]  2%|▏         | 8/360 [00:02<01:32,  3.79it/s, loss=6.89]  2%|▎         | 9/360 [00:02<01:30,  3.87it/s, loss=6.89]  3%|▎         | 10/360 [00:03<01:28,  3.96it/s, loss=6.89]  3%|▎         | 10/360 [00:03<01:28,  3.96it/s, loss=4.85]  3%|▎         | 11/360 [00:03<01:26,  4.04it/s, loss=4.85]  3%|▎         | 12/360 [00:03<01:24,  4.10it/s, loss=4.85]  4%|▎         | 13/360 [00:03<01:23,  4.15it/s, loss=4.85]  4%|▍         | 14/360 [00:04<01:22,  4.18it/s, loss=4.85]  4%|▍         | 15/360 [00:04<01:22,  4.20it/s, loss=4.85]  4%|▍         | 16/360 [00:04<01:21,  4.21it/s, loss=4.85]  5%|▍         | 17/360 [00:04<01:21,  4.20it/s, loss=4.85]  5%|▌         | 18/360 [00:05<01:20,  4.22it/s, loss=4.85]  5%|▌         | 19/360 [00:05<01:20,  4.23it/s, loss=4.85]  6%|▌         | 20/360 [00:05<01:20,  4.24it/s, loss=4.85]  6%|▌         | 20/360 [00:05<01:20,  4.24it/s, loss=3.75]  6%|▌         | 21/360 [00:05<01:31,  3.70it/s, loss=3.75]  6%|▌         | 22/360 [00:06<01:27,  3.87it/s, loss=3.75]  6%|▋         | 23/360 [00:06<01:24,  3.98it/s, loss=3.75]  7%|▋         | 24/360 [00:06<01:22,  4.06it/s, loss=3.75]  7%|▋         | 25/360 [00:06<01:21,  4.12it/s, loss=3.75]  7%|▋         | 26/360 [00:07<01:20,  4.17it/s, loss=3.75]  8%|▊         | 27/360 [00:07<01:19,  4.20it/s, loss=3.75]  8%|▊         | 28/360 [00:07<01:18,  4.22it/s, loss=3.75]  8%|▊         | 29/360 [00:07<01:18,  4.23it/s, loss=3.75]  8%|▊         | 30/360 [00:07<01:18,  4.22it/s, loss=3.75]  8%|▊         | 30/360 [00:08<01:18,  4.22it/s, loss=3.18]  9%|▊         | 31/360 [00:08<01:17,  4.22it/s, loss=3.18]  9%|▉         | 32/360 [00:08<01:17,  4.23it/s, loss=3.18]  9%|▉         | 33/360 [00:08<01:17,  4.23it/s, loss=3.18]  9%|▉         | 34/360 [00:08<01:17,  4.18it/s, loss=3.18] 10%|▉         | 35/360 [00:09<01:17,  4.20it/s, loss=3.18] 10%|█         | 36/360 [00:09<01:16,  4.21it/s, loss=3.18] 10%|█         | 37/360 [00:09<01:16,  4.22it/s, loss=3.18] 11%|█         | 38/360 [00:09<01:16,  4.22it/s, loss=3.18] 11%|█         | 39/360 [00:10<01:15,  4.23it/s, loss=3.18] 11%|█         | 40/360 [00:10<01:15,  4.24it/s, loss=3.18] 11%|█         | 40/360 [00:10<01:15,  4.24it/s, loss=2.8]  11%|█▏        | 41/360 [00:10<01:15,  4.22it/s, loss=2.8] 12%|█▏        | 42/360 [00:10<01:15,  4.23it/s, loss=2.8] 12%|█▏        | 43/360 [00:11<01:14,  4.24it/s, loss=2.8] 12%|█▏        | 44/360 [00:11<01:14,  4.25it/s, loss=2.8] 12%|█▎        | 45/360 [00:11<01:14,  4.24it/s, loss=2.8] 13%|█▎        | 46/360 [00:11<01:14,  4.24it/s, loss=2.8] 13%|█▎        | 47/360 [00:11<01:13,  4.25it/s, loss=2.8] 13%|█▎        | 48/360 [00:12<01:13,  4.25it/s, loss=2.8] 14%|█▎        | 49/360 [00:12<01:12,  4.26it/s, loss=2.8] 14%|█▍        | 50/360 [00:12<01:12,  4.27it/s, loss=2.8] 14%|█▍        | 50/360 [00:12<01:12,  4.27it/s, loss=2.86] 14%|█▍        | 51/360 [00:12<01:12,  4.27it/s, loss=2.86] 14%|█▍        | 52/360 [00:13<01:12,  4.26it/s, loss=2.86] 15%|█▍        | 53/360 [00:13<01:12,  4.26it/s, loss=2.86] 15%|█▌        | 54/360 [00:13<01:12,  4.25it/s, loss=2.86] 15%|█▌        | 55/360 [00:13<01:11,  4.26it/s, loss=2.86] 16%|█▌        | 56/360 [00:14<01:20,  3.79it/s, loss=2.86] 16%|█▌        | 57/360 [00:14<01:17,  3.92it/s, loss=2.86] 16%|█▌        | 58/360 [00:14<01:15,  4.03it/s, loss=2.86] 16%|█▋        | 59/360 [00:14<01:13,  4.10it/s, loss=2.86] 17%|█▋        | 60/360 [00:15<01:12,  4.15it/s, loss=2.86] 17%|█▋        | 60/360 [00:15<01:12,  4.15it/s, loss=2.65] 17%|█▋        | 61/360 [00:15<01:11,  4.18it/s, loss=2.65] 17%|█▋        | 62/360 [00:15<01:10,  4.20it/s, loss=2.65] 18%|█▊        | 63/360 [00:15<01:10,  4.22it/s, loss=2.65] 18%|█▊        | 64/360 [00:16<01:09,  4.23it/s, loss=2.65] 18%|█▊        | 65/360 [00:16<01:09,  4.24it/s, loss=2.65] 18%|█▊        | 66/360 [00:16<01:09,  4.24it/s, loss=2.65] 19%|█▊        | 67/360 [00:16<01:08,  4.25it/s, loss=2.65] 19%|█▉        | 68/360 [00:17<01:08,  4.25it/s, loss=2.65] 19%|█▉        | 69/360 [00:17<01:08,  4.25it/s, loss=2.65] 19%|█▉        | 70/360 [00:17<01:08,  4.25it/s, loss=2.65] 19%|█▉        | 70/360 [00:17<01:08,  4.25it/s, loss=2.43] 20%|█▉        | 71/360 [00:17<01:08,  4.24it/s, loss=2.43] 20%|██        | 72/360 [00:17<01:07,  4.24it/s, loss=2.43] 20%|██        | 73/360 [00:18<01:07,  4.25it/s, loss=2.43] 21%|██        | 74/360 [00:18<01:07,  4.25it/s, loss=2.43] 21%|██        | 75/360 [00:18<01:06,  4.26it/s, loss=2.43] 21%|██        | 76/360 [00:18<01:06,  4.26it/s, loss=2.43] 21%|██▏       | 77/360 [00:19<01:06,  4.26it/s, loss=2.43] 22%|██▏       | 78/360 [00:19<01:06,  4.25it/s, loss=2.43] 22%|██▏       | 79/360 [00:19<01:05,  4.26it/s, loss=2.43] 22%|██▏       | 80/360 [00:19<01:05,  4.26it/s, loss=2.43] 22%|██▏       | 80/360 [00:20<01:05,  4.26it/s, loss=2.22] 22%|██▎       | 81/360 [00:20<01:05,  4.25it/s, loss=2.22] 23%|██▎       | 82/360 [00:20<01:05,  4.26it/s, loss=2.22] 23%|██▎       | 83/360 [00:20<01:04,  4.26it/s, loss=2.22] 23%|██▎       | 84/360 [00:20<01:04,  4.26it/s, loss=2.22] 24%|██▎       | 85/360 [00:21<01:04,  4.27it/s, loss=2.22] 24%|██▍       | 86/360 [00:21<01:04,  4.26it/s, loss=2.22] 24%|██▍       | 87/360 [00:21<01:03,  4.27it/s, loss=2.22] 24%|██▍       | 88/360 [00:21<01:11,  3.79it/s, loss=2.22] 25%|██▍       | 89/360 [00:22<01:10,  3.86it/s, loss=2.22] 25%|██▌       | 90/360 [00:22<01:08,  3.97it/s, loss=2.22] 25%|██▌       | 90/360 [00:22<01:08,  3.97it/s, loss=2.27] 25%|██▌       | 91/360 [00:22<01:06,  4.05it/s, loss=2.27] 26%|██▌       | 92/360 [00:22<01:05,  4.08it/s, loss=2.27] 26%|██▌       | 93/360 [00:23<01:04,  4.13it/s, loss=2.27] 26%|██▌       | 94/360 [00:23<01:03,  4.16it/s, loss=2.27] 26%|██▋       | 95/360 [00:23<01:03,  4.18it/s, loss=2.27] 27%|██▋       | 96/360 [00:23<01:03,  4.19it/s, loss=2.27] 27%|██▋       | 97/360 [00:23<01:02,  4.21it/s, loss=2.27] 27%|██▋       | 98/360 [00:24<01:01,  4.23it/s, loss=2.27] 28%|██▊       | 99/360 [00:24<01:01,  4.24it/s, loss=2.27] 28%|██▊       | 100/360 [00:24<01:01,  4.25it/s, loss=2.27] 28%|██▊       | 100/360 [00:24<01:01,  4.25it/s, loss=2.22] 28%|██▊       | 101/360 [00:24<01:00,  4.25it/s, loss=2.22] 28%|██▊       | 102/360 [00:25<01:00,  4.26it/s, loss=2.22] 29%|██▊       | 103/360 [00:25<01:00,  4.26it/s, loss=2.22] 29%|██▉       | 104/360 [00:25<01:00,  4.25it/s, loss=2.22] 29%|██▉       | 105/360 [00:25<01:00,  4.23it/s, loss=2.22] 29%|██▉       | 106/360 [00:26<01:00,  4.22it/s, loss=2.22] 30%|██▉       | 107/360 [00:26<00:59,  4.23it/s, loss=2.22] 30%|███       | 108/360 [00:26<00:59,  4.24it/s, loss=2.22] 30%|███       | 109/360 [00:26<00:59,  4.25it/s, loss=2.22] 31%|███       | 110/360 [00:27<00:58,  4.26it/s, loss=2.22] 31%|███       | 110/360 [00:27<00:58,  4.26it/s, loss=2.1]  31%|███       | 111/360 [00:27<00:58,  4.25it/s, loss=2.1] 31%|███       | 112/360 [00:27<00:58,  4.26it/s, loss=2.1] 31%|███▏      | 113/360 [00:27<00:58,  4.26it/s, loss=2.1] 32%|███▏      | 114/360 [00:27<00:57,  4.26it/s, loss=2.1] 32%|███▏      | 115/360 [00:28<00:57,  4.25it/s, loss=2.1] 32%|███▏      | 116/360 [00:28<00:57,  4.25it/s, loss=2.1] 32%|███▎      | 117/360 [00:28<00:57,  4.26it/s, loss=2.1] 33%|███▎      | 118/360 [00:28<00:56,  4.25it/s, loss=2.1] 33%|███▎      | 119/360 [00:29<00:56,  4.25it/s, loss=2.1] 33%|███▎      | 120/360 [00:29<00:56,  4.25it/s, loss=2.1] 33%|███▎      | 120/360 [00:29<00:56,  4.25it/s, loss=2.02] 34%|███▎      | 121/360 [00:29<00:56,  4.24it/s, loss=2.02] 34%|███▍      | 122/360 [00:29<00:56,  4.25it/s, loss=2.02] 34%|███▍      | 123/360 [00:30<00:55,  4.25it/s, loss=2.02] 34%|███▍      | 124/360 [00:30<00:55,  4.26it/s, loss=2.02] 35%|███▍      | 125/360 [00:30<01:02,  3.78it/s, loss=2.02] 35%|███▌      | 126/360 [00:30<00:59,  3.93it/s, loss=2.02] 35%|███▌      | 127/360 [00:31<00:57,  4.03it/s, loss=2.02] 36%|███▌      | 128/360 [00:31<00:58,  3.99it/s, loss=2.02] 36%|███▌      | 129/360 [00:31<00:56,  4.06it/s, loss=2.02] 36%|███▌      | 130/360 [00:31<00:55,  4.12it/s, loss=2.02] 36%|███▌      | 130/360 [00:32<00:55,  4.12it/s, loss=1.91] 36%|███▋      | 131/360 [00:32<00:56,  4.09it/s, loss=1.91] 37%|███▋      | 132/360 [00:32<00:54,  4.15it/s, loss=1.91] 37%|███▋      | 133/360 [00:32<00:54,  4.18it/s, loss=1.91] 37%|███▋      | 134/360 [00:32<00:53,  4.21it/s, loss=1.91] 38%|███▊      | 135/360 [00:33<00:53,  4.22it/s, loss=1.91] 38%|███▊      | 136/360 [00:33<00:52,  4.24it/s, loss=1.91] 38%|███▊      | 137/360 [00:33<00:52,  4.22it/s, loss=1.91] 38%|███▊      | 138/360 [00:33<00:52,  4.24it/s, loss=1.91] 39%|███▊      | 139/360 [00:33<00:52,  4.24it/s, loss=1.91] 39%|███▉      | 140/360 [00:34<00:52,  4.23it/s, loss=1.91] 39%|███▉      | 140/360 [00:34<00:52,  4.23it/s, loss=1.8]  39%|███▉      | 141/360 [00:34<00:51,  4.23it/s, loss=1.8] 39%|███▉      | 142/360 [00:34<00:51,  4.24it/s, loss=1.8] 40%|███▉      | 143/360 [00:34<00:50,  4.26it/s, loss=1.8] 40%|████      | 144/360 [00:35<00:50,  4.25it/s, loss=1.8] 40%|████      | 145/360 [00:35<00:50,  4.25it/s, loss=1.8] 41%|████      | 146/360 [00:35<00:50,  4.25it/s, loss=1.8] 41%|████      | 147/360 [00:35<00:50,  4.26it/s, loss=1.8] 41%|████      | 148/360 [00:36<00:49,  4.26it/s, loss=1.8] 41%|████▏     | 149/360 [00:36<00:49,  4.26it/s, loss=1.8] 42%|████▏     | 150/360 [00:36<00:49,  4.26it/s, loss=1.8] 42%|████▏     | 150/360 [00:36<00:49,  4.26it/s, loss=1.71] 42%|████▏     | 151/360 [00:36<00:49,  4.26it/s, loss=1.71] 42%|████▏     | 152/360 [00:37<00:48,  4.27it/s, loss=1.71] 42%|████▎     | 153/360 [00:37<00:48,  4.26it/s, loss=1.71] 43%|████▎     | 154/360 [00:37<00:48,  4.27it/s, loss=1.71] 43%|████▎     | 155/360 [00:37<00:47,  4.28it/s, loss=1.71] 43%|████▎     | 156/360 [00:37<00:47,  4.28it/s, loss=1.71] 44%|████▎     | 157/360 [00:38<00:47,  4.28it/s, loss=1.71] 44%|████▍     | 158/360 [00:38<00:47,  4.28it/s, loss=1.71] 44%|████▍     | 159/360 [00:38<00:46,  4.29it/s, loss=1.71] 44%|████▍     | 160/360 [00:38<00:46,  4.30it/s, loss=1.71] 44%|████▍     | 160/360 [00:39<00:46,  4.30it/s, loss=1.82] 45%|████▍     | 161/360 [00:39<00:46,  4.29it/s, loss=1.82] 45%|████▌     | 162/360 [00:39<00:46,  4.30it/s, loss=1.82] 45%|████▌     | 163/360 [00:39<00:45,  4.29it/s, loss=1.82] 46%|████▌     | 164/360 [00:39<00:45,  4.30it/s, loss=1.82] 46%|████▌     | 165/360 [00:40<00:45,  4.30it/s, loss=1.82] 46%|████▌     | 166/360 [00:40<00:45,  4.30it/s, loss=1.82] 46%|████▋     | 167/360 [00:40<00:44,  4.31it/s, loss=1.82] 47%|████▋     | 168/360 [00:40<00:44,  4.31it/s, loss=1.82] 47%|████▋     | 169/360 [00:40<00:44,  4.31it/s, loss=1.82] 47%|████▋     | 170/360 [00:41<00:50,  3.80it/s, loss=1.82] 47%|████▋     | 170/360 [00:41<00:50,  3.80it/s, loss=1.8]  48%|████▊     | 171/360 [00:41<00:47,  3.95it/s, loss=1.8] 48%|████▊     | 172/360 [00:41<00:46,  4.05it/s, loss=1.8] 48%|████▊     | 173/360 [00:41<00:45,  4.12it/s, loss=1.8] 48%|████▊     | 174/360 [00:42<00:44,  4.17it/s, loss=1.8] 49%|████▊     | 175/360 [00:42<00:43,  4.21it/s, loss=1.8] 49%|████▉     | 176/360 [00:42<00:43,  4.21it/s, loss=1.8] 49%|████▉     | 177/360 [00:42<00:43,  4.22it/s, loss=1.8] 49%|████▉     | 178/360 [00:43<00:43,  4.20it/s, loss=1.8] 50%|████▉     | 179/360 [00:43<00:42,  4.22it/s, loss=1.8] 50%|█████     | 180/360 [00:43<00:42,  4.20it/s, loss=1.8] 50%|█████     | 180/360 [00:43<00:42,  4.20it/s, loss=1.56] 50%|█████     | 181/360 [00:43<00:42,  4.22it/s, loss=1.56] 51%|█████     | 182/360 [00:44<00:41,  4.25it/s, loss=1.56] 51%|█████     | 183/360 [00:44<00:41,  4.25it/s, loss=1.56] 51%|█████     | 184/360 [00:44<00:41,  4.25it/s, loss=1.56] 51%|█████▏    | 185/360 [00:44<00:41,  4.25it/s, loss=1.56] 52%|█████▏    | 186/360 [00:45<00:41,  4.22it/s, loss=1.56] 52%|█████▏    | 187/360 [00:45<00:40,  4.23it/s, loss=1.56] 52%|█████▏    | 188/360 [00:45<00:40,  4.24it/s, loss=1.56] 52%|█████▎    | 189/360 [00:45<00:40,  4.24it/s, loss=1.56] 53%|█████▎    | 190/360 [00:45<00:40,  4.25it/s, loss=1.56] 53%|█████▎    | 190/360 [00:46<00:40,  4.25it/s, loss=1.66] 53%|█████▎    | 191/360 [00:46<00:39,  4.25it/s, loss=1.66] 53%|█████▎    | 192/360 [00:46<00:39,  4.24it/s, loss=1.66] 54%|█████▎    | 193/360 [00:46<00:39,  4.25it/s, loss=1.66] 54%|█████▍    | 194/360 [00:46<00:38,  4.27it/s, loss=1.66] 54%|█████▍    | 195/360 [00:47<00:39,  4.20it/s, loss=1.66] 54%|█████▍    | 196/360 [00:47<00:39,  4.17it/s, loss=1.66] 55%|█████▍    | 197/360 [00:47<00:38,  4.21it/s, loss=1.66] 55%|█████▌    | 198/360 [00:47<00:38,  4.21it/s, loss=1.66] 55%|█████▌    | 199/360 [00:48<00:39,  4.11it/s, loss=1.66] 56%|█████▌    | 200/360 [00:48<00:38,  4.15it/s, loss=1.66] 56%|█████▌    | 200/360 [00:48<00:38,  4.15it/s, loss=1.58] 56%|█████▌    | 201/360 [00:48<00:37,  4.19it/s, loss=1.58] 56%|█████▌    | 202/360 [00:48<00:37,  4.22it/s, loss=1.58] 56%|█████▋    | 203/360 [00:49<00:37,  4.24it/s, loss=1.58] 57%|█████▋    | 204/360 [00:49<00:36,  4.26it/s, loss=1.58] 57%|█████▋    | 205/360 [00:49<00:36,  4.26it/s, loss=1.58] 57%|█████▋    | 206/360 [00:49<00:35,  4.28it/s, loss=1.58] 57%|█████▊    | 207/360 [00:50<00:36,  4.22it/s, loss=1.58] 58%|█████▊    | 208/360 [00:50<00:35,  4.23it/s, loss=1.58] 58%|█████▊    | 209/360 [00:50<00:35,  4.25it/s, loss=1.58] 58%|█████▊    | 210/360 [00:50<00:35,  4.25it/s, loss=1.58] 58%|█████▊    | 210/360 [00:50<00:35,  4.25it/s, loss=1.61] 59%|█████▊    | 211/360 [00:50<00:34,  4.26it/s, loss=1.61] 59%|█████▉    | 212/360 [00:51<00:34,  4.27it/s, loss=1.61] 59%|█████▉    | 213/360 [00:51<00:34,  4.27it/s, loss=1.61] 59%|█████▉    | 214/360 [00:51<00:34,  4.25it/s, loss=1.61] 60%|█████▉    | 215/360 [00:52<00:39,  3.67it/s, loss=1.61] 60%|██████    | 216/360 [00:52<00:37,  3.81it/s, loss=1.61] 60%|██████    | 217/360 [00:52<00:36,  3.91it/s, loss=1.61] 61%|██████    | 218/360 [00:52<00:35,  3.95it/s, loss=1.61] 61%|██████    | 219/360 [00:52<00:34,  4.03it/s, loss=1.61] 61%|██████    | 220/360 [00:53<00:34,  4.10it/s, loss=1.61] 61%|██████    | 220/360 [00:53<00:34,  4.10it/s, loss=1.57] 61%|██████▏   | 221/360 [00:53<00:33,  4.15it/s, loss=1.57] 62%|██████▏   | 222/360 [00:53<00:32,  4.19it/s, loss=1.57] 62%|██████▏   | 223/360 [00:53<00:32,  4.21it/s, loss=1.57] 62%|██████▏   | 224/360 [00:54<00:32,  4.22it/s, loss=1.57] 62%|██████▎   | 225/360 [00:54<00:31,  4.23it/s, loss=1.57] 63%|██████▎   | 226/360 [00:54<00:31,  4.25it/s, loss=1.57] 63%|██████▎   | 227/360 [00:54<00:31,  4.25it/s, loss=1.57] 63%|██████▎   | 228/360 [00:55<00:31,  4.25it/s, loss=1.57] 64%|██████▎   | 229/360 [00:55<00:30,  4.26it/s, loss=1.57] 64%|██████▍   | 230/360 [00:55<00:30,  4.25it/s, loss=1.57] 64%|██████▍   | 230/360 [00:55<00:30,  4.25it/s, loss=1.54] 64%|██████▍   | 231/360 [00:55<00:30,  4.21it/s, loss=1.54] 64%|██████▍   | 232/360 [00:56<00:30,  4.23it/s, loss=1.54] 65%|██████▍   | 233/360 [00:56<00:29,  4.23it/s, loss=1.54] 65%|██████▌   | 234/360 [00:56<00:29,  4.25it/s, loss=1.54] 65%|██████▌   | 235/360 [00:56<00:29,  4.26it/s, loss=1.54] 66%|██████▌   | 236/360 [00:56<00:29,  4.26it/s, loss=1.54] 66%|██████▌   | 237/360 [00:57<00:28,  4.26it/s, loss=1.54] 66%|██████▌   | 238/360 [00:57<00:28,  4.26it/s, loss=1.54] 66%|██████▋   | 239/360 [00:57<00:28,  4.26it/s, loss=1.54] 67%|██████▋   | 240/360 [00:57<00:28,  4.26it/s, loss=1.54] 67%|██████▋   | 240/360 [00:58<00:28,  4.26it/s, loss=1.55] 67%|██████▋   | 241/360 [00:58<00:27,  4.27it/s, loss=1.55] 67%|██████▋   | 242/360 [00:58<00:27,  4.27it/s, loss=1.55] 68%|██████▊   | 243/360 [00:58<00:27,  4.26it/s, loss=1.55] 68%|██████▊   | 244/360 [00:58<00:30,  3.76it/s, loss=1.55] 68%|██████▊   | 245/360 [00:59<00:29,  3.91it/s, loss=1.55] 68%|██████▊   | 246/360 [00:59<00:28,  4.00it/s, loss=1.55] 69%|██████▊   | 247/360 [00:59<00:27,  4.08it/s, loss=1.55] 69%|██████▉   | 248/360 [00:59<00:27,  4.14it/s, loss=1.55] 69%|██████▉   | 249/360 [01:00<00:26,  4.18it/s, loss=1.55] 69%|██████▉   | 250/360 [01:00<00:26,  4.21it/s, loss=1.55] 69%|██████▉   | 250/360 [01:00<00:26,  4.21it/s, loss=1.4]  70%|██████▉   | 251/360 [01:00<00:25,  4.22it/s, loss=1.4] 70%|███████   | 252/360 [01:00<00:25,  4.23it/s, loss=1.4] 70%|███████   | 253/360 [01:01<00:25,  4.24it/s, loss=1.4] 71%|███████   | 254/360 [01:01<00:24,  4.24it/s, loss=1.4] 71%|███████   | 255/360 [01:01<00:24,  4.25it/s, loss=1.4] 71%|███████   | 256/360 [01:01<00:24,  4.25it/s, loss=1.4] 71%|███████▏  | 257/360 [01:02<00:24,  4.26it/s, loss=1.4] 72%|███████▏  | 258/360 [01:02<00:23,  4.26it/s, loss=1.4] 72%|███████▏  | 259/360 [01:02<00:23,  4.27it/s, loss=1.4] 72%|███████▏  | 260/360 [01:02<00:23,  4.27it/s, loss=1.4] 72%|███████▏  | 260/360 [01:02<00:23,  4.27it/s, loss=1.39] 72%|███████▎  | 261/360 [01:02<00:23,  4.27it/s, loss=1.39] 73%|███████▎  | 262/360 [01:03<00:22,  4.26it/s, loss=1.39] 73%|███████▎  | 263/360 [01:03<00:22,  4.26it/s, loss=1.39] 73%|███████▎  | 264/360 [01:03<00:22,  4.26it/s, loss=1.39] 74%|███████▎  | 265/360 [01:03<00:22,  4.26it/s, loss=1.39] 74%|███████▍  | 266/360 [01:04<00:22,  4.26it/s, loss=1.39] 74%|███████▍  | 267/360 [01:04<00:21,  4.26it/s, loss=1.39] 74%|███████▍  | 268/360 [01:04<00:21,  4.26it/s, loss=1.39] 75%|███████▍  | 269/360 [01:04<00:21,  4.27it/s, loss=1.39] 75%|███████▌  | 270/360 [01:05<00:21,  4.27it/s, loss=1.39] 75%|███████▌  | 270/360 [01:05<00:21,  4.27it/s, loss=1.35] 75%|███████▌  | 271/360 [01:05<00:20,  4.27it/s, loss=1.35] 76%|███████▌  | 272/360 [01:05<00:20,  4.26it/s, loss=1.35] 76%|███████▌  | 273/360 [01:05<00:23,  3.75it/s, loss=1.35] 76%|███████▌  | 274/360 [01:06<00:22,  3.86it/s, loss=1.35] 76%|███████▋  | 275/360 [01:06<00:21,  3.97it/s, loss=1.35] 77%|███████▋  | 276/360 [01:06<00:20,  4.05it/s, loss=1.35] 77%|███████▋  | 277/360 [01:06<00:20,  4.09it/s, loss=1.35] 77%|███████▋  | 278/360 [01:07<00:19,  4.14it/s, loss=1.35] 78%|███████▊  | 279/360 [01:07<00:19,  4.17it/s, loss=1.35] 78%|███████▊  | 280/360 [01:07<00:19,  4.18it/s, loss=1.35] 78%|███████▊  | 280/360 [01:07<00:19,  4.18it/s, loss=1.42] 78%|███████▊  | 281/360 [01:07<00:18,  4.20it/s, loss=1.42] 78%|███████▊  | 282/360 [01:08<00:18,  4.22it/s, loss=1.42] 79%|███████▊  | 283/360 [01:08<00:18,  4.16it/s, loss=1.42] 79%|███████▉  | 284/360 [01:08<00:18,  4.18it/s, loss=1.42] 79%|███████▉  | 285/360 [01:08<00:18,  4.15it/s, loss=1.42] 79%|███████▉  | 286/360 [01:08<00:17,  4.18it/s, loss=1.42] 80%|███████▉  | 287/360 [01:09<00:17,  4.21it/s, loss=1.42] 80%|████████  | 288/360 [01:09<00:17,  4.23it/s, loss=1.42] 80%|████████  | 289/360 [01:09<00:16,  4.24it/s, loss=1.42] 81%|████████  | 290/360 [01:09<00:16,  4.25it/s, loss=1.42] 81%|████████  | 290/360 [01:10<00:16,  4.25it/s, loss=1.34] 81%|████████  | 291/360 [01:10<00:16,  4.25it/s, loss=1.34] 81%|████████  | 292/360 [01:10<00:15,  4.26it/s, loss=1.34] 81%|████████▏ | 293/360 [01:10<00:15,  4.25it/s, loss=1.34] 82%|████████▏ | 294/360 [01:10<00:15,  4.25it/s, loss=1.34] 82%|████████▏ | 295/360 [01:11<00:15,  4.25it/s, loss=1.34] 82%|████████▏ | 296/360 [01:11<00:15,  4.26it/s, loss=1.34] 82%|████████▎ | 297/360 [01:11<00:14,  4.26it/s, loss=1.34] 83%|████████▎ | 298/360 [01:11<00:14,  4.26it/s, loss=1.34] 83%|████████▎ | 299/360 [01:12<00:14,  4.25it/s, loss=1.34] 83%|████████▎ | 300/360 [01:12<00:14,  4.23it/s, loss=1.34] 83%|████████▎ | 300/360 [01:12<00:14,  4.23it/s, loss=1.38] 84%|████████▎ | 301/360 [01:12<00:13,  4.23it/s, loss=1.38] 84%|████████▍ | 302/360 [01:12<00:13,  4.24it/s, loss=1.38] 84%|████████▍ | 303/360 [01:13<00:15,  3.74it/s, loss=1.38] 84%|████████▍ | 304/360 [01:13<00:14,  3.90it/s, loss=1.38] 85%|████████▍ | 305/360 [01:13<00:13,  4.00it/s, loss=1.38] 85%|████████▌ | 306/360 [01:13<00:13,  4.08it/s, loss=1.38] 85%|████████▌ | 307/360 [01:14<00:12,  4.13it/s, loss=1.38] 86%|████████▌ | 308/360 [01:14<00:12,  4.17it/s, loss=1.38] 86%|████████▌ | 309/360 [01:14<00:12,  4.20it/s, loss=1.38] 86%|████████▌ | 310/360 [01:14<00:11,  4.21it/s, loss=1.38] 86%|████████▌ | 310/360 [01:14<00:11,  4.21it/s, loss=1.32] 86%|████████▋ | 311/360 [01:14<00:11,  4.21it/s, loss=1.32] 87%|████████▋ | 312/360 [01:15<00:11,  4.21it/s, loss=1.32] 87%|████████▋ | 313/360 [01:15<00:11,  4.22it/s, loss=1.32] 87%|████████▋ | 314/360 [01:15<00:10,  4.24it/s, loss=1.32] 88%|████████▊ | 315/360 [01:15<00:10,  4.25it/s, loss=1.32] 88%|████████▊ | 316/360 [01:16<00:10,  4.26it/s, loss=1.32] 88%|████████▊ | 317/360 [01:16<00:10,  4.26it/s, loss=1.32] 88%|████████▊ | 318/360 [01:16<00:09,  4.27it/s, loss=1.32] 89%|████████▊ | 319/360 [01:16<00:09,  4.24it/s, loss=1.32] 89%|████████▉ | 320/360 [01:17<00:09,  4.15it/s, loss=1.32] 89%|████████▉ | 320/360 [01:17<00:09,  4.15it/s, loss=1.33] 89%|████████▉ | 321/360 [01:17<00:09,  4.18it/s, loss=1.33] 89%|████████▉ | 322/360 [01:17<00:09,  4.19it/s, loss=1.33] 90%|████████▉ | 323/360 [01:17<00:08,  4.22it/s, loss=1.33] 90%|█████████ | 324/360 [01:18<00:08,  4.24it/s, loss=1.33] 90%|█████████ | 325/360 [01:18<00:08,  4.25it/s, loss=1.33] 91%|█████████ | 326/360 [01:18<00:07,  4.26it/s, loss=1.33] 91%|█████████ | 327/360 [01:18<00:07,  4.26it/s, loss=1.33] 91%|█████████ | 328/360 [01:18<00:07,  4.26it/s, loss=1.33] 91%|█████████▏| 329/360 [01:19<00:07,  4.24it/s, loss=1.33] 92%|█████████▏| 330/360 [01:19<00:07,  4.04it/s, loss=1.33] 92%|█████████▏| 330/360 [01:19<00:07,  4.04it/s, loss=1.24] 92%|█████████▏| 331/360 [01:19<00:07,  4.12it/s, loss=1.24] 92%|█████████▏| 332/360 [01:19<00:06,  4.13it/s, loss=1.24] 92%|█████████▎| 333/360 [01:20<00:07,  3.66it/s, loss=1.24] 93%|█████████▎| 334/360 [01:20<00:06,  3.84it/s, loss=1.24] 93%|█████████▎| 335/360 [01:20<00:06,  3.97it/s, loss=1.24] 93%|█████████▎| 336/360 [01:20<00:05,  4.05it/s, loss=1.24] 94%|█████████▎| 337/360 [01:21<00:05,  4.14it/s, loss=1.24] 94%|█████████▍| 338/360 [01:21<00:05,  4.21it/s, loss=1.24] 94%|█████████▍| 339/360 [01:21<00:04,  4.26it/s, loss=1.24] 94%|█████████▍| 340/360 [01:21<00:04,  4.29it/s, loss=1.24] 94%|█████████▍| 340/360 [01:22<00:04,  4.29it/s, loss=1.27] 95%|█████████▍| 341/360 [01:22<00:04,  4.32it/s, loss=1.27] 95%|█████████▌| 342/360 [01:22<00:04,  4.33it/s, loss=1.27] 95%|█████████▌| 343/360 [01:22<00:03,  4.35it/s, loss=1.27] 96%|█████████▌| 344/360 [01:22<00:03,  4.36it/s, loss=1.27] 96%|█████████▌| 345/360 [01:23<00:03,  4.36it/s, loss=1.27] 96%|█████████▌| 346/360 [01:23<00:03,  4.37it/s, loss=1.27] 96%|█████████▋| 347/360 [01:23<00:02,  4.38it/s, loss=1.27] 97%|█████████▋| 348/360 [01:23<00:02,  4.38it/s, loss=1.27] 97%|█████████▋| 349/360 [01:23<00:02,  4.39it/s, loss=1.27] 97%|█████████▋| 350/360 [01:24<00:02,  4.39it/s, loss=1.27] 97%|█████████▋| 350/360 [01:24<00:02,  4.39it/s, loss=1.31] 98%|█████████▊| 351/360 [01:24<00:02,  4.39it/s, loss=1.31] 98%|█████████▊| 352/360 [01:24<00:01,  4.39it/s, loss=1.31] 98%|█████████▊| 353/360 [01:24<00:01,  4.39it/s, loss=1.31] 98%|█████████▊| 354/360 [01:25<00:01,  4.39it/s, loss=1.31] 99%|█████████▊| 355/360 [01:25<00:01,  4.39it/s, loss=1.31] 99%|█████████▉| 356/360 [01:25<00:00,  4.39it/s, loss=1.31] 99%|█████████▉| 357/360 [01:25<00:00,  4.39it/s, loss=1.31] 99%|█████████▉| 358/360 [01:26<00:00,  4.39it/s, loss=1.31]100%|█████████▉| 359/360 [01:26<00:00,  4.39it/s, loss=1.31]100%|██████████| 360/360 [01:26<00:00,  4.39it/s, loss=1.31]100%|██████████| 360/360 [01:26<00:00,  4.16it/s, loss=1.31]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:29,  3.36it/s]  2%|▏         | 2/100 [00:00<00:18,  5.26it/s]  3%|▎         | 3/100 [00:00<00:20,  4.72it/s]  4%|▍         | 4/100 [00:00<00:16,  5.84it/s]  5%|▌         | 5/100 [00:00<00:14,  6.75it/s]  6%|▌         | 6/100 [00:00<00:12,  7.42it/s]  7%|▋         | 7/100 [00:01<00:11,  7.91it/s]  8%|▊         | 8/100 [00:01<00:11,  8.18it/s]  9%|▉         | 9/100 [00:01<00:10,  8.48it/s] 10%|█         | 10/100 [00:01<00:10,  8.55it/s] 11%|█         | 11/100 [00:01<00:10,  8.69it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.77it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.70it/s] 14%|█▍        | 14/100 [00:01<00:10,  8.49it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.64it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.72it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.82it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.80it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.89it/s] 20%|██        | 20/100 [00:02<00:08,  8.95it/s] 21%|██        | 21/100 [00:02<00:08,  9.00it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.05it/s] 23%|██▎       | 23/100 [00:02<00:08,  9.07it/s] 24%|██▍       | 24/100 [00:03<00:08,  8.73it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.86it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.94it/s] 27%|██▋       | 27/100 [00:03<00:08,  9.01it/s] 28%|██▊       | 28/100 [00:03<00:07,  9.04it/s] 29%|██▉       | 29/100 [00:03<00:07,  9.07it/s] 30%|███       | 30/100 [00:03<00:07,  9.08it/s] 31%|███       | 31/100 [00:03<00:07,  9.10it/s] 32%|███▏      | 32/100 [00:03<00:07,  9.11it/s] 33%|███▎      | 33/100 [00:03<00:07,  9.11it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.09it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.05it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.06it/s] 37%|███▋      | 37/100 [00:04<00:07,  8.74it/s] 38%|███▊      | 38/100 [00:04<00:07,  8.80it/s] 39%|███▉      | 39/100 [00:04<00:07,  8.56it/s] 40%|████      | 40/100 [00:04<00:06,  8.72it/s] 41%|████      | 41/100 [00:04<00:06,  8.86it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.92it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.97it/s] 44%|████▍     | 44/100 [00:05<00:06,  9.00it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.91it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.96it/s] 47%|████▋     | 47/100 [00:05<00:05,  9.02it/s] 48%|████▊     | 48/100 [00:05<00:05,  9.01it/s] 49%|████▉     | 49/100 [00:05<00:07,  7.09it/s] 50%|█████     | 50/100 [00:05<00:06,  7.62it/s] 51%|█████     | 51/100 [00:06<00:06,  8.01it/s] 52%|█████▏    | 52/100 [00:06<00:06,  7.96it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.27it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.52it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.70it/s] 56%|█████▌    | 56/100 [00:06<00:04,  8.84it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.91it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.95it/s] 59%|█████▉    | 59/100 [00:07<00:04,  8.96it/s] 60%|██████    | 60/100 [00:07<00:04,  8.96it/s] 61%|██████    | 61/100 [00:07<00:04,  9.02it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.80it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.89it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.48it/s] 65%|██████▌   | 65/100 [00:07<00:04,  8.69it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.82it/s] 67%|██████▋   | 67/100 [00:07<00:03,  8.90it/s] 68%|██████▊   | 68/100 [00:08<00:03,  8.96it/s] 69%|██████▉   | 69/100 [00:08<00:03,  8.97it/s] 70%|███████   | 70/100 [00:08<00:03,  9.01it/s] 71%|███████   | 71/100 [00:08<00:03,  9.03it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.05it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.08it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.08it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.09it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.10it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.17it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.25it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.32it/s] 80%|████████  | 80/100 [00:09<00:02,  9.37it/s] 81%|████████  | 81/100 [00:09<00:02,  9.41it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.44it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.46it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.48it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.50it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.51it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.51it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.51it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.53it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.53it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.53it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.53it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.55it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.54it/s] 95%|█████████▌| 95/100 [00:11<00:00,  7.35it/s] 96%|█████████▌| 96/100 [00:11<00:00,  7.91it/s] 97%|█████████▋| 97/100 [00:11<00:00,  8.36it/s] 98%|█████████▊| 98/100 [00:11<00:00,  8.70it/s] 99%|█████████▉| 99/100 [00:11<00:00,  8.94it/s]100%|██████████| 100/100 [00:11<00:00,  9.12it/s]100%|██████████| 100/100 [00:11<00:00,  8.65it/s]
Model tested on 100 tasks. Accuracy: 64.72%
0.10000000149011612 0.10000000149011612
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:30,  3.29it/s]  2%|▏         | 2/100 [00:00<00:19,  5.06it/s]  3%|▎         | 3/100 [00:00<00:15,  6.12it/s]  4%|▍         | 4/100 [00:00<00:13,  6.87it/s]  5%|▌         | 5/100 [00:00<00:12,  7.33it/s]  6%|▌         | 6/100 [00:00<00:12,  7.71it/s]  7%|▋         | 7/100 [00:01<00:12,  7.60it/s]  8%|▊         | 8/100 [00:01<00:11,  7.80it/s]  9%|▉         | 9/100 [00:01<00:11,  8.07it/s] 10%|█         | 10/100 [00:01<00:10,  8.37it/s] 11%|█         | 11/100 [00:01<00:10,  8.57it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.66it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.76it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.87it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.91it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.94it/s] 17%|█▋        | 17/100 [00:02<00:09,  9.00it/s] 18%|█▊        | 18/100 [00:02<00:09,  9.04it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.94it/s] 20%|██        | 20/100 [00:02<00:09,  8.67it/s] 21%|██        | 21/100 [00:02<00:08,  8.79it/s] 22%|██▏       | 22/100 [00:02<00:08,  8.89it/s] 23%|██▎       | 23/100 [00:02<00:08,  8.97it/s] 24%|██▍       | 24/100 [00:02<00:08,  8.63it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.68it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.81it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.77it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.56it/s] 29%|██▉       | 29/100 [00:03<00:08,  8.69it/s] 30%|███       | 30/100 [00:03<00:07,  8.83it/s] 31%|███       | 31/100 [00:03<00:07,  8.91it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.97it/s] 33%|███▎      | 33/100 [00:03<00:07,  9.01it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.07it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.08it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.07it/s] 37%|███▋      | 37/100 [00:04<00:06,  9.06it/s] 38%|███▊      | 38/100 [00:04<00:06,  9.09it/s] 39%|███▉      | 39/100 [00:04<00:06,  9.09it/s] 40%|████      | 40/100 [00:04<00:08,  6.76it/s] 41%|████      | 41/100 [00:04<00:08,  7.33it/s] 42%|████▏     | 42/100 [00:05<00:07,  7.80it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.17it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.43it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.65it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.67it/s] 47%|████▋     | 47/100 [00:05<00:06,  8.81it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.91it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.98it/s] 50%|█████     | 50/100 [00:05<00:05,  8.70it/s] 51%|█████     | 51/100 [00:06<00:05,  8.83it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.92it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.98it/s] 54%|█████▍    | 54/100 [00:06<00:05,  9.03it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.04it/s] 56%|█████▌    | 56/100 [00:06<00:04,  8.88it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.94it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.78it/s] 59%|█████▉    | 59/100 [00:06<00:04,  8.58it/s] 60%|██████    | 60/100 [00:07<00:04,  8.66it/s] 61%|██████    | 61/100 [00:07<00:04,  8.74it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.57it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.72it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.83it/s] 65%|██████▌   | 65/100 [00:07<00:04,  8.64it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.73it/s] 67%|██████▋   | 67/100 [00:07<00:03,  8.81it/s] 68%|██████▊   | 68/100 [00:08<00:03,  8.91it/s] 69%|██████▉   | 69/100 [00:08<00:03,  8.96it/s] 70%|███████   | 70/100 [00:08<00:03,  8.96it/s] 71%|███████   | 71/100 [00:08<00:03,  9.00it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.03it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.06it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.10it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.11it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.12it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.23it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.33it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.38it/s] 80%|████████  | 80/100 [00:09<00:02,  9.42it/s] 81%|████████  | 81/100 [00:09<00:02,  9.45it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.49it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.50it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.50it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.51it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.51it/s] 87%|████████▋ | 87/100 [00:10<00:01,  7.28it/s] 88%|████████▊ | 88/100 [00:10<00:01,  7.85it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.30it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.65it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.90it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.08it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.21it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.32it/s] 95%|█████████▌| 95/100 [00:10<00:00,  9.39it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.42it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.45it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.49it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.50it/s]100%|██████████| 100/100 [00:11<00:00,  9.45it/s]100%|██████████| 100/100 [00:11<00:00,  8.65it/s]
Model tested on 100 tasks. Accuracy: 41.60%
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:01<?, ?it/s, loss=7.84]  0%|          | 1/360 [00:01<06:28,  1.08s/it, loss=7.84]  1%|          | 2/360 [00:01<03:40,  1.63it/s, loss=7.84]  1%|          | 3/360 [00:01<02:39,  2.24it/s, loss=7.84]  1%|          | 4/360 [00:01<02:10,  2.72it/s, loss=7.84]  1%|▏         | 5/360 [00:02<01:54,  3.09it/s, loss=7.84]  2%|▏         | 6/360 [00:02<01:44,  3.40it/s, loss=7.84]  2%|▏         | 7/360 [00:02<01:37,  3.62it/s, loss=7.84]  2%|▏         | 8/360 [00:02<01:34,  3.72it/s, loss=7.84]  2%|▎         | 9/360 [00:03<01:30,  3.86it/s, loss=7.84]  3%|▎         | 10/360 [00:03<01:28,  3.96it/s, loss=7.84]  3%|▎         | 10/360 [00:03<01:28,  3.96it/s, loss=4.97]  3%|▎         | 11/360 [00:03<01:26,  4.03it/s, loss=4.97]  3%|▎         | 12/360 [00:03<01:24,  4.09it/s, loss=4.97]  4%|▎         | 13/360 [00:04<01:24,  4.12it/s, loss=4.97]  4%|▍         | 14/360 [00:04<01:23,  4.14it/s, loss=4.97]  4%|▍         | 15/360 [00:04<01:22,  4.17it/s, loss=4.97]  4%|▍         | 16/360 [00:04<01:21,  4.20it/s, loss=4.97]  5%|▍         | 17/360 [00:04<01:21,  4.19it/s, loss=4.97]  5%|▌         | 18/360 [00:05<01:21,  4.19it/s, loss=4.97]  5%|▌         | 19/360 [00:05<01:21,  4.20it/s, loss=4.97]  6%|▌         | 20/360 [00:05<01:20,  4.21it/s, loss=4.97]  6%|▌         | 20/360 [00:05<01:20,  4.21it/s, loss=3.8]   6%|▌         | 21/360 [00:05<01:20,  4.22it/s, loss=3.8]  6%|▌         | 22/360 [00:06<01:19,  4.23it/s, loss=3.8]  6%|▋         | 23/360 [00:06<01:19,  4.23it/s, loss=3.8]  7%|▋         | 24/360 [00:06<01:19,  4.24it/s, loss=3.8]  7%|▋         | 25/360 [00:06<01:18,  4.25it/s, loss=3.8]  7%|▋         | 26/360 [00:07<01:18,  4.25it/s, loss=3.8]  8%|▊         | 27/360 [00:07<01:18,  4.24it/s, loss=3.8]  8%|▊         | 28/360 [00:07<01:32,  3.60it/s, loss=3.8]  8%|▊         | 29/360 [00:07<01:27,  3.79it/s, loss=3.8]  8%|▊         | 30/360 [00:08<01:24,  3.92it/s, loss=3.8]  8%|▊         | 30/360 [00:08<01:24,  3.92it/s, loss=3.55]  9%|▊         | 31/360 [00:08<01:22,  4.01it/s, loss=3.55]  9%|▉         | 32/360 [00:08<01:20,  4.08it/s, loss=3.55]  9%|▉         | 33/360 [00:08<01:19,  4.14it/s, loss=3.55]  9%|▉         | 34/360 [00:09<01:18,  4.17it/s, loss=3.55] 10%|▉         | 35/360 [00:09<01:17,  4.20it/s, loss=3.55] 10%|█         | 36/360 [00:09<01:16,  4.21it/s, loss=3.55] 10%|█         | 37/360 [00:09<01:16,  4.22it/s, loss=3.55] 11%|█         | 38/360 [00:10<01:16,  4.24it/s, loss=3.55] 11%|█         | 39/360 [00:10<01:15,  4.24it/s, loss=3.55] 11%|█         | 40/360 [00:10<01:15,  4.23it/s, loss=3.55] 11%|█         | 40/360 [00:10<01:15,  4.23it/s, loss=3.12] 11%|█▏        | 41/360 [00:10<01:15,  4.24it/s, loss=3.12] 12%|█▏        | 42/360 [00:10<01:14,  4.25it/s, loss=3.12] 12%|█▏        | 43/360 [00:11<01:14,  4.25it/s, loss=3.12] 12%|█▏        | 44/360 [00:11<01:14,  4.25it/s, loss=3.12] 12%|█▎        | 45/360 [00:11<01:14,  4.25it/s, loss=3.12] 13%|█▎        | 46/360 [00:11<01:13,  4.24it/s, loss=3.12] 13%|█▎        | 47/360 [00:12<01:13,  4.24it/s, loss=3.12] 13%|█▎        | 48/360 [00:12<01:13,  4.24it/s, loss=3.12] 14%|█▎        | 49/360 [00:12<01:13,  4.25it/s, loss=3.12] 14%|█▍        | 50/360 [00:12<01:13,  4.25it/s, loss=3.12] 14%|█▍        | 50/360 [00:13<01:13,  4.25it/s, loss=2.93] 14%|█▍        | 51/360 [00:13<01:12,  4.25it/s, loss=2.93] 14%|█▍        | 52/360 [00:13<01:12,  4.26it/s, loss=2.93] 15%|█▍        | 53/360 [00:13<01:12,  4.26it/s, loss=2.93] 15%|█▌        | 54/360 [00:13<01:11,  4.27it/s, loss=2.93] 15%|█▌        | 55/360 [00:14<01:11,  4.27it/s, loss=2.93] 16%|█▌        | 56/360 [00:14<01:11,  4.27it/s, loss=2.93] 16%|█▌        | 57/360 [00:14<01:11,  4.26it/s, loss=2.93] 16%|█▌        | 58/360 [00:14<01:10,  4.26it/s, loss=2.93] 16%|█▋        | 59/360 [00:14<01:10,  4.26it/s, loss=2.93] 17%|█▋        | 60/360 [00:15<01:10,  4.26it/s, loss=2.93] 17%|█▋        | 60/360 [00:15<01:10,  4.26it/s, loss=2.69] 17%|█▋        | 61/360 [00:15<01:10,  4.26it/s, loss=2.69] 17%|█▋        | 62/360 [00:15<01:09,  4.27it/s, loss=2.69] 18%|█▊        | 63/360 [00:15<01:09,  4.28it/s, loss=2.69] 18%|█▊        | 64/360 [00:16<01:10,  4.23it/s, loss=2.69] 18%|█▊        | 65/360 [00:16<01:10,  4.19it/s, loss=2.69] 18%|█▊        | 66/360 [00:16<01:09,  4.22it/s, loss=2.69] 19%|█▊        | 67/360 [00:16<01:09,  4.24it/s, loss=2.69] 19%|█▉        | 68/360 [00:17<01:08,  4.24it/s, loss=2.69] 19%|█▉        | 69/360 [00:17<01:09,  4.18it/s, loss=2.69] 19%|█▉        | 70/360 [00:17<01:09,  4.20it/s, loss=2.69] 19%|█▉        | 70/360 [00:17<01:09,  4.20it/s, loss=2.48] 20%|█▉        | 71/360 [00:17<01:08,  4.22it/s, loss=2.48] 20%|██        | 72/360 [00:18<01:08,  4.23it/s, loss=2.48] 20%|██        | 73/360 [00:18<01:08,  4.22it/s, loss=2.48] 21%|██        | 74/360 [00:18<01:07,  4.24it/s, loss=2.48] 21%|██        | 75/360 [00:18<01:07,  4.25it/s, loss=2.48] 21%|██        | 76/360 [00:19<01:06,  4.26it/s, loss=2.48] 21%|██▏       | 77/360 [00:19<01:16,  3.70it/s, loss=2.48] 22%|██▏       | 78/360 [00:19<01:12,  3.88it/s, loss=2.48] 22%|██▏       | 79/360 [00:19<01:10,  3.98it/s, loss=2.48] 22%|██▏       | 80/360 [00:20<01:08,  4.06it/s, loss=2.48] 22%|██▏       | 80/360 [00:20<01:08,  4.06it/s, loss=2.35] 22%|██▎       | 81/360 [00:20<01:07,  4.12it/s, loss=2.35] 23%|██▎       | 82/360 [00:20<01:06,  4.16it/s, loss=2.35] 23%|██▎       | 83/360 [00:20<01:06,  4.18it/s, loss=2.35] 23%|██▎       | 84/360 [00:21<01:05,  4.20it/s, loss=2.35] 24%|██▎       | 85/360 [00:21<01:05,  4.21it/s, loss=2.35] 24%|██▍       | 86/360 [00:21<01:04,  4.22it/s, loss=2.35] 24%|██▍       | 87/360 [00:21<01:04,  4.23it/s, loss=2.35] 24%|██▍       | 88/360 [00:21<01:04,  4.24it/s, loss=2.35] 25%|██▍       | 89/360 [00:22<01:03,  4.24it/s, loss=2.35] 25%|██▌       | 90/360 [00:22<01:03,  4.24it/s, loss=2.35] 25%|██▌       | 90/360 [00:22<01:03,  4.24it/s, loss=2.22] 25%|██▌       | 91/360 [00:22<01:03,  4.24it/s, loss=2.22] 26%|██▌       | 92/360 [00:22<01:03,  4.24it/s, loss=2.22] 26%|██▌       | 93/360 [00:23<01:02,  4.24it/s, loss=2.22] 26%|██▌       | 94/360 [00:23<01:02,  4.24it/s, loss=2.22] 26%|██▋       | 95/360 [00:23<01:02,  4.24it/s, loss=2.22] 27%|██▋       | 96/360 [00:23<01:02,  4.24it/s, loss=2.22] 27%|██▋       | 97/360 [00:24<01:02,  4.24it/s, loss=2.22] 27%|██▋       | 98/360 [00:24<01:01,  4.24it/s, loss=2.22] 28%|██▊       | 99/360 [00:24<01:01,  4.24it/s, loss=2.22] 28%|██▊       | 100/360 [00:24<01:01,  4.25it/s, loss=2.22] 28%|██▊       | 100/360 [00:25<01:01,  4.25it/s, loss=2.04] 28%|██▊       | 101/360 [00:25<01:00,  4.25it/s, loss=2.04] 28%|██▊       | 102/360 [00:25<01:00,  4.26it/s, loss=2.04] 29%|██▊       | 103/360 [00:25<01:00,  4.26it/s, loss=2.04] 29%|██▉       | 104/360 [00:25<01:00,  4.26it/s, loss=2.04] 29%|██▉       | 105/360 [00:25<00:59,  4.26it/s, loss=2.04] 29%|██▉       | 106/360 [00:26<00:59,  4.27it/s, loss=2.04] 30%|██▉       | 107/360 [00:26<00:59,  4.26it/s, loss=2.04] 30%|███       | 108/360 [00:26<00:59,  4.26it/s, loss=2.04] 30%|███       | 109/360 [00:26<00:58,  4.26it/s, loss=2.04] 31%|███       | 110/360 [00:27<00:58,  4.27it/s, loss=2.04] 31%|███       | 110/360 [00:27<00:58,  4.27it/s, loss=2.07] 31%|███       | 111/360 [00:27<00:58,  4.26it/s, loss=2.07] 31%|███       | 112/360 [00:27<00:58,  4.24it/s, loss=2.07] 31%|███▏      | 113/360 [00:27<00:58,  4.23it/s, loss=2.07] 32%|███▏      | 114/360 [00:28<00:57,  4.25it/s, loss=2.07] 32%|███▏      | 115/360 [00:28<00:57,  4.26it/s, loss=2.07] 32%|███▏      | 116/360 [00:28<00:57,  4.27it/s, loss=2.07] 32%|███▎      | 117/360 [00:28<00:56,  4.27it/s, loss=2.07] 33%|███▎      | 118/360 [00:28<00:56,  4.28it/s, loss=2.07] 33%|███▎      | 119/360 [00:29<00:56,  4.27it/s, loss=2.07] 33%|███▎      | 120/360 [00:29<00:56,  4.28it/s, loss=2.07] 33%|███▎      | 120/360 [00:29<00:56,  4.28it/s, loss=2.1]  34%|███▎      | 121/360 [00:29<00:55,  4.28it/s, loss=2.1] 34%|███▍      | 122/360 [00:29<00:55,  4.28it/s, loss=2.1] 34%|███▍      | 123/360 [00:30<00:55,  4.29it/s, loss=2.1] 34%|███▍      | 124/360 [00:30<00:55,  4.28it/s, loss=2.1] 35%|███▍      | 125/360 [00:30<00:54,  4.28it/s, loss=2.1] 35%|███▌      | 126/360 [00:30<01:02,  3.72it/s, loss=2.1] 35%|███▌      | 127/360 [00:31<00:59,  3.88it/s, loss=2.1] 36%|███▌      | 128/360 [00:31<00:58,  3.99it/s, loss=2.1] 36%|███▌      | 129/360 [00:31<00:56,  4.07it/s, loss=2.1] 36%|███▌      | 130/360 [00:31<00:55,  4.12it/s, loss=2.1] 36%|███▌      | 130/360 [00:32<00:55,  4.12it/s, loss=1.95] 36%|███▋      | 131/360 [00:32<00:55,  4.16it/s, loss=1.95] 37%|███▋      | 132/360 [00:32<00:54,  4.19it/s, loss=1.95] 37%|███▋      | 133/360 [00:32<00:53,  4.21it/s, loss=1.95] 37%|███▋      | 134/360 [00:32<00:53,  4.22it/s, loss=1.95] 38%|███▊      | 135/360 [00:33<00:53,  4.22it/s, loss=1.95] 38%|███▊      | 136/360 [00:33<00:53,  4.23it/s, loss=1.95] 38%|███▊      | 137/360 [00:33<00:52,  4.23it/s, loss=1.95] 38%|███▊      | 138/360 [00:33<00:52,  4.23it/s, loss=1.95] 39%|███▊      | 139/360 [00:34<00:52,  4.23it/s, loss=1.95] 39%|███▉      | 140/360 [00:34<00:51,  4.23it/s, loss=1.95] 39%|███▉      | 140/360 [00:34<00:51,  4.23it/s, loss=1.77] 39%|███▉      | 141/360 [00:34<00:51,  4.23it/s, loss=1.77] 39%|███▉      | 142/360 [00:34<00:51,  4.23it/s, loss=1.77] 40%|███▉      | 143/360 [00:34<00:51,  4.22it/s, loss=1.77] 40%|████      | 144/360 [00:35<00:51,  4.23it/s, loss=1.77] 40%|████      | 145/360 [00:35<00:50,  4.23it/s, loss=1.77] 41%|████      | 146/360 [00:35<00:50,  4.24it/s, loss=1.77] 41%|████      | 147/360 [00:35<00:50,  4.21it/s, loss=1.77] 41%|████      | 148/360 [00:36<00:50,  4.18it/s, loss=1.77] 41%|████▏     | 149/360 [00:36<00:50,  4.20it/s, loss=1.77] 42%|████▏     | 150/360 [00:36<00:49,  4.22it/s, loss=1.77] 42%|████▏     | 150/360 [00:36<00:49,  4.22it/s, loss=1.91] 42%|████▏     | 151/360 [00:36<00:49,  4.22it/s, loss=1.91] 42%|████▏     | 152/360 [00:37<00:49,  4.22it/s, loss=1.91] 42%|████▎     | 153/360 [00:37<00:48,  4.22it/s, loss=1.91] 43%|████▎     | 154/360 [00:37<00:48,  4.22it/s, loss=1.91] 43%|████▎     | 155/360 [00:37<00:48,  4.22it/s, loss=1.91] 43%|████▎     | 156/360 [00:38<00:48,  4.23it/s, loss=1.91] 44%|████▎     | 157/360 [00:38<00:48,  4.22it/s, loss=1.91] 44%|████▍     | 158/360 [00:38<00:55,  3.66it/s, loss=1.91] 44%|████▍     | 159/360 [00:38<00:52,  3.82it/s, loss=1.91] 44%|████▍     | 160/360 [00:39<00:50,  3.95it/s, loss=1.91] 44%|████▍     | 160/360 [00:39<00:50,  3.95it/s, loss=1.73] 45%|████▍     | 161/360 [00:39<00:49,  4.03it/s, loss=1.73] 45%|████▌     | 162/360 [00:39<00:48,  4.10it/s, loss=1.73] 45%|████▌     | 163/360 [00:39<00:47,  4.14it/s, loss=1.73] 46%|████▌     | 164/360 [00:40<00:46,  4.17it/s, loss=1.73] 46%|████▌     | 165/360 [00:40<00:46,  4.19it/s, loss=1.73] 46%|████▌     | 166/360 [00:40<00:46,  4.20it/s, loss=1.73] 46%|████▋     | 167/360 [00:40<00:45,  4.21it/s, loss=1.73] 47%|████▋     | 168/360 [00:41<00:45,  4.22it/s, loss=1.73] 47%|████▋     | 169/360 [00:41<00:45,  4.22it/s, loss=1.73] 47%|████▋     | 170/360 [00:41<00:44,  4.23it/s, loss=1.73] 47%|████▋     | 170/360 [00:41<00:44,  4.23it/s, loss=1.71] 48%|████▊     | 171/360 [00:41<00:44,  4.23it/s, loss=1.71] 48%|████▊     | 172/360 [00:41<00:44,  4.23it/s, loss=1.71] 48%|████▊     | 173/360 [00:42<00:44,  4.23it/s, loss=1.71] 48%|████▊     | 174/360 [00:42<00:43,  4.23it/s, loss=1.71] 49%|████▊     | 175/360 [00:42<00:43,  4.23it/s, loss=1.71] 49%|████▉     | 176/360 [00:42<00:43,  4.23it/s, loss=1.71] 49%|████▉     | 177/360 [00:43<00:43,  4.24it/s, loss=1.71] 49%|████▉     | 178/360 [00:43<00:42,  4.24it/s, loss=1.71] 50%|████▉     | 179/360 [00:43<00:42,  4.24it/s, loss=1.71] 50%|█████     | 180/360 [00:43<00:42,  4.24it/s, loss=1.71] 50%|█████     | 180/360 [00:44<00:42,  4.24it/s, loss=1.69] 50%|█████     | 181/360 [00:44<00:42,  4.24it/s, loss=1.69] 51%|█████     | 182/360 [00:44<00:41,  4.24it/s, loss=1.69] 51%|█████     | 183/360 [00:44<00:41,  4.24it/s, loss=1.69] 51%|█████     | 184/360 [00:44<00:41,  4.24it/s, loss=1.69] 51%|█████▏    | 185/360 [00:45<00:41,  4.24it/s, loss=1.69] 52%|█████▏    | 186/360 [00:45<00:41,  4.24it/s, loss=1.69] 52%|█████▏    | 187/360 [00:45<00:41,  4.21it/s, loss=1.69] 52%|█████▏    | 188/360 [00:45<00:40,  4.22it/s, loss=1.69] 52%|█████▎    | 189/360 [00:45<00:40,  4.21it/s, loss=1.69] 53%|█████▎    | 190/360 [00:46<00:46,  3.65it/s, loss=1.69] 53%|█████▎    | 190/360 [00:46<00:46,  3.65it/s, loss=1.69] 53%|█████▎    | 191/360 [00:46<00:44,  3.83it/s, loss=1.69] 53%|█████▎    | 192/360 [00:46<00:42,  3.95it/s, loss=1.69] 54%|█████▎    | 193/360 [00:47<00:41,  4.03it/s, loss=1.69] 54%|█████▍    | 194/360 [00:47<00:40,  4.09it/s, loss=1.69] 54%|█████▍    | 195/360 [00:47<00:39,  4.13it/s, loss=1.69] 54%|█████▍    | 196/360 [00:47<00:39,  4.16it/s, loss=1.69] 55%|█████▍    | 197/360 [00:47<00:38,  4.18it/s, loss=1.69] 55%|█████▌    | 198/360 [00:48<00:38,  4.18it/s, loss=1.69] 55%|█████▌    | 199/360 [00:48<00:38,  4.17it/s, loss=1.69] 56%|█████▌    | 200/360 [00:48<00:38,  4.19it/s, loss=1.69] 56%|█████▌    | 200/360 [00:48<00:38,  4.19it/s, loss=1.63] 56%|█████▌    | 201/360 [00:48<00:37,  4.19it/s, loss=1.63] 56%|█████▌    | 202/360 [00:49<00:37,  4.20it/s, loss=1.63] 56%|█████▋    | 203/360 [00:49<00:37,  4.18it/s, loss=1.63] 57%|█████▋    | 204/360 [00:49<00:37,  4.15it/s, loss=1.63] 57%|█████▋    | 205/360 [00:49<00:37,  4.17it/s, loss=1.63] 57%|█████▋    | 206/360 [00:50<00:37,  4.11it/s, loss=1.63] 57%|█████▊    | 207/360 [00:50<00:36,  4.15it/s, loss=1.63] 58%|█████▊    | 208/360 [00:50<00:36,  4.17it/s, loss=1.63] 58%|█████▊    | 209/360 [00:50<00:36,  4.12it/s, loss=1.63] 58%|█████▊    | 210/360 [00:51<00:36,  4.11it/s, loss=1.63] 58%|█████▊    | 210/360 [00:51<00:36,  4.11it/s, loss=1.72] 59%|█████▊    | 211/360 [00:51<00:35,  4.15it/s, loss=1.72] 59%|█████▉    | 212/360 [00:51<00:35,  4.17it/s, loss=1.72] 59%|█████▉    | 213/360 [00:51<00:35,  4.18it/s, loss=1.72] 59%|█████▉    | 214/360 [00:52<00:34,  4.20it/s, loss=1.72] 60%|█████▉    | 215/360 [00:52<00:34,  4.17it/s, loss=1.72] 60%|██████    | 216/360 [00:52<00:34,  4.19it/s, loss=1.72] 60%|██████    | 217/360 [00:52<00:34,  4.20it/s, loss=1.72] 61%|██████    | 218/360 [00:53<00:33,  4.21it/s, loss=1.72] 61%|██████    | 219/360 [00:53<00:33,  4.20it/s, loss=1.72] 61%|██████    | 220/360 [00:53<00:33,  4.21it/s, loss=1.72] 61%|██████    | 220/360 [00:53<00:33,  4.21it/s, loss=1.65] 61%|██████▏   | 221/360 [00:53<00:32,  4.22it/s, loss=1.65] 62%|██████▏   | 222/360 [00:54<00:38,  3.62it/s, loss=1.65] 62%|██████▏   | 223/360 [00:54<00:36,  3.80it/s, loss=1.65] 62%|██████▏   | 224/360 [00:54<00:34,  3.91it/s, loss=1.65] 62%|██████▎   | 225/360 [00:54<00:33,  4.00it/s, loss=1.65] 63%|██████▎   | 226/360 [00:55<00:32,  4.06it/s, loss=1.65] 63%|██████▎   | 227/360 [00:55<00:32,  4.11it/s, loss=1.65] 63%|██████▎   | 228/360 [00:55<00:31,  4.14it/s, loss=1.65] 64%|██████▎   | 229/360 [00:55<00:31,  4.17it/s, loss=1.65] 64%|██████▍   | 230/360 [00:55<00:31,  4.19it/s, loss=1.65] 64%|██████▍   | 230/360 [00:56<00:31,  4.19it/s, loss=1.64] 64%|██████▍   | 231/360 [00:56<00:31,  4.16it/s, loss=1.64] 64%|██████▍   | 232/360 [00:56<00:30,  4.18it/s, loss=1.64] 65%|██████▍   | 233/360 [00:56<00:30,  4.19it/s, loss=1.64] 65%|██████▌   | 234/360 [00:56<00:29,  4.21it/s, loss=1.64] 65%|██████▌   | 235/360 [00:57<00:29,  4.22it/s, loss=1.64] 66%|██████▌   | 236/360 [00:57<00:29,  4.23it/s, loss=1.64] 66%|██████▌   | 237/360 [00:57<00:29,  4.23it/s, loss=1.64] 66%|██████▌   | 238/360 [00:57<00:28,  4.24it/s, loss=1.64] 66%|██████▋   | 239/360 [00:58<00:28,  4.24it/s, loss=1.64] 67%|██████▋   | 240/360 [00:58<00:28,  4.23it/s, loss=1.64] 67%|██████▋   | 240/360 [00:58<00:28,  4.23it/s, loss=1.58] 67%|██████▋   | 241/360 [00:58<00:28,  4.22it/s, loss=1.58] 67%|██████▋   | 242/360 [00:58<00:27,  4.23it/s, loss=1.58] 68%|██████▊   | 243/360 [00:59<00:27,  4.24it/s, loss=1.58] 68%|██████▊   | 244/360 [00:59<00:27,  4.24it/s, loss=1.58] 68%|██████▊   | 245/360 [00:59<00:27,  4.25it/s, loss=1.58] 68%|██████▊   | 246/360 [00:59<00:26,  4.24it/s, loss=1.58] 69%|██████▊   | 247/360 [01:00<00:26,  4.24it/s, loss=1.58] 69%|██████▉   | 248/360 [01:00<00:26,  4.25it/s, loss=1.58] 69%|██████▉   | 249/360 [01:00<00:26,  4.25it/s, loss=1.58] 69%|██████▉   | 250/360 [01:00<00:25,  4.25it/s, loss=1.58] 69%|██████▉   | 250/360 [01:00<00:25,  4.25it/s, loss=1.55] 70%|██████▉   | 251/360 [01:00<00:25,  4.25it/s, loss=1.55] 70%|███████   | 252/360 [01:01<00:25,  4.25it/s, loss=1.55] 70%|███████   | 253/360 [01:01<00:25,  4.25it/s, loss=1.55] 71%|███████   | 254/360 [01:01<00:28,  3.66it/s, loss=1.55] 71%|███████   | 255/360 [01:02<00:27,  3.84it/s, loss=1.55] 71%|███████   | 256/360 [01:02<00:26,  3.96it/s, loss=1.55] 71%|███████▏  | 257/360 [01:02<00:25,  4.04it/s, loss=1.55] 72%|███████▏  | 258/360 [01:02<00:24,  4.10it/s, loss=1.55] 72%|███████▏  | 259/360 [01:02<00:24,  4.15it/s, loss=1.55] 72%|███████▏  | 260/360 [01:03<00:23,  4.17it/s, loss=1.55] 72%|███████▏  | 260/360 [01:03<00:23,  4.17it/s, loss=1.49] 72%|███████▎  | 261/360 [01:03<00:23,  4.18it/s, loss=1.49] 73%|███████▎  | 262/360 [01:03<00:23,  4.19it/s, loss=1.49] 73%|███████▎  | 263/360 [01:03<00:23,  4.20it/s, loss=1.49] 73%|███████▎  | 264/360 [01:04<00:22,  4.19it/s, loss=1.49] 74%|███████▎  | 265/360 [01:04<00:22,  4.21it/s, loss=1.49] 74%|███████▍  | 266/360 [01:04<00:22,  4.22it/s, loss=1.49] 74%|███████▍  | 267/360 [01:04<00:22,  4.22it/s, loss=1.49] 74%|███████▍  | 268/360 [01:05<00:21,  4.19it/s, loss=1.49] 75%|███████▍  | 269/360 [01:05<00:21,  4.21it/s, loss=1.49] 75%|███████▌  | 270/360 [01:05<00:21,  4.22it/s, loss=1.49] 75%|███████▌  | 270/360 [01:05<00:21,  4.22it/s, loss=1.35] 75%|███████▌  | 271/360 [01:05<00:21,  4.23it/s, loss=1.35] 76%|███████▌  | 272/360 [01:06<00:20,  4.23it/s, loss=1.35] 76%|███████▌  | 273/360 [01:06<00:20,  4.23it/s, loss=1.35] 76%|███████▌  | 274/360 [01:06<00:20,  4.24it/s, loss=1.35] 76%|███████▋  | 275/360 [01:06<00:20,  4.24it/s, loss=1.35] 77%|███████▋  | 276/360 [01:06<00:19,  4.24it/s, loss=1.35] 77%|███████▋  | 277/360 [01:07<00:19,  4.25it/s, loss=1.35] 77%|███████▋  | 278/360 [01:07<00:19,  4.24it/s, loss=1.35] 78%|███████▊  | 279/360 [01:07<00:19,  4.23it/s, loss=1.35] 78%|███████▊  | 280/360 [01:07<00:19,  4.20it/s, loss=1.35] 78%|███████▊  | 280/360 [01:08<00:19,  4.20it/s, loss=1.46] 78%|███████▊  | 281/360 [01:08<00:18,  4.22it/s, loss=1.46] 78%|███████▊  | 282/360 [01:08<00:18,  4.22it/s, loss=1.46] 79%|███████▊  | 283/360 [01:08<00:18,  4.22it/s, loss=1.46] 79%|███████▉  | 284/360 [01:08<00:18,  4.22it/s, loss=1.46] 79%|███████▉  | 285/360 [01:09<00:17,  4.22it/s, loss=1.46] 79%|███████▉  | 286/360 [01:09<00:20,  3.63it/s, loss=1.46] 80%|███████▉  | 287/360 [01:09<00:19,  3.81it/s, loss=1.46] 80%|████████  | 288/360 [01:09<00:18,  3.92it/s, loss=1.46] 80%|████████  | 289/360 [01:10<00:17,  3.96it/s, loss=1.46] 81%|████████  | 290/360 [01:10<00:17,  4.03it/s, loss=1.46] 81%|████████  | 290/360 [01:10<00:17,  4.03it/s, loss=1.36] 81%|████████  | 291/360 [01:10<00:17,  4.03it/s, loss=1.36] 81%|████████  | 292/360 [01:10<00:16,  4.09it/s, loss=1.36] 81%|████████▏ | 293/360 [01:11<00:16,  4.14it/s, loss=1.36] 82%|████████▏ | 294/360 [01:11<00:15,  4.17it/s, loss=1.36] 82%|████████▏ | 295/360 [01:11<00:15,  4.19it/s, loss=1.36] 82%|████████▏ | 296/360 [01:11<00:15,  4.20it/s, loss=1.36] 82%|████████▎ | 297/360 [01:12<00:15,  4.20it/s, loss=1.36] 83%|████████▎ | 298/360 [01:12<00:14,  4.21it/s, loss=1.36] 83%|████████▎ | 299/360 [01:12<00:14,  4.22it/s, loss=1.36] 83%|████████▎ | 300/360 [01:12<00:14,  4.22it/s, loss=1.36] 83%|████████▎ | 300/360 [01:13<00:14,  4.22it/s, loss=1.39] 84%|████████▎ | 301/360 [01:13<00:13,  4.23it/s, loss=1.39] 84%|████████▍ | 302/360 [01:13<00:13,  4.23it/s, loss=1.39] 84%|████████▍ | 303/360 [01:13<00:13,  4.23it/s, loss=1.39] 84%|████████▍ | 304/360 [01:13<00:13,  4.22it/s, loss=1.39] 85%|████████▍ | 305/360 [01:13<00:13,  4.23it/s, loss=1.39] 85%|████████▌ | 306/360 [01:14<00:12,  4.24it/s, loss=1.39] 85%|████████▌ | 307/360 [01:14<00:12,  4.23it/s, loss=1.39] 86%|████████▌ | 308/360 [01:14<00:12,  4.24it/s, loss=1.39] 86%|████████▌ | 309/360 [01:14<00:12,  4.24it/s, loss=1.39] 86%|████████▌ | 310/360 [01:15<00:11,  4.24it/s, loss=1.39] 86%|████████▌ | 310/360 [01:15<00:11,  4.24it/s, loss=1.37] 86%|████████▋ | 311/360 [01:15<00:11,  4.23it/s, loss=1.37] 87%|████████▋ | 312/360 [01:15<00:11,  4.23it/s, loss=1.37] 87%|████████▋ | 313/360 [01:15<00:11,  4.23it/s, loss=1.37] 87%|████████▋ | 314/360 [01:16<00:10,  4.23it/s, loss=1.37] 88%|████████▊ | 315/360 [01:16<00:10,  4.24it/s, loss=1.37] 88%|████████▊ | 316/360 [01:16<00:10,  4.24it/s, loss=1.37] 88%|████████▊ | 317/360 [01:16<00:10,  4.24it/s, loss=1.37] 88%|████████▊ | 318/360 [01:17<00:09,  4.25it/s, loss=1.37] 89%|████████▊ | 319/360 [01:17<00:11,  3.65it/s, loss=1.37] 89%|████████▉ | 320/360 [01:17<00:10,  3.83it/s, loss=1.37] 89%|████████▉ | 320/360 [01:17<00:10,  3.83it/s, loss=1.28] 89%|████████▉ | 321/360 [01:17<00:09,  3.95it/s, loss=1.28] 89%|████████▉ | 322/360 [01:18<00:09,  4.04it/s, loss=1.28] 90%|████████▉ | 323/360 [01:18<00:09,  4.10it/s, loss=1.28] 90%|█████████ | 324/360 [01:18<00:08,  4.14it/s, loss=1.28] 90%|█████████ | 325/360 [01:18<00:08,  4.14it/s, loss=1.28] 91%|█████████ | 326/360 [01:19<00:08,  4.17it/s, loss=1.28] 91%|█████████ | 327/360 [01:19<00:08,  4.05it/s, loss=1.28] 91%|█████████ | 328/360 [01:19<00:07,  4.10it/s, loss=1.28] 91%|█████████▏| 329/360 [01:19<00:07,  4.08it/s, loss=1.28] 92%|█████████▏| 330/360 [01:20<00:07,  4.03it/s, loss=1.28] 92%|█████████▏| 330/360 [01:20<00:07,  4.03it/s, loss=1.32] 92%|█████████▏| 331/360 [01:20<00:07,  4.07it/s, loss=1.32] 92%|█████████▏| 332/360 [01:20<00:06,  4.11it/s, loss=1.32] 92%|█████████▎| 333/360 [01:20<00:06,  4.14it/s, loss=1.32] 93%|█████████▎| 334/360 [01:21<00:06,  4.16it/s, loss=1.32] 93%|█████████▎| 335/360 [01:21<00:05,  4.19it/s, loss=1.32] 93%|█████████▎| 336/360 [01:21<00:05,  4.18it/s, loss=1.32] 94%|█████████▎| 337/360 [01:21<00:05,  4.20it/s, loss=1.32] 94%|█████████▍| 338/360 [01:21<00:05,  4.16it/s, loss=1.32] 94%|█████████▍| 339/360 [01:22<00:04,  4.21it/s, loss=1.32] 94%|█████████▍| 340/360 [01:22<00:04,  4.25it/s, loss=1.32] 94%|█████████▍| 340/360 [01:22<00:04,  4.25it/s, loss=1.24] 95%|█████████▍| 341/360 [01:22<00:04,  4.27it/s, loss=1.24] 95%|█████████▌| 342/360 [01:22<00:04,  4.20it/s, loss=1.24] 95%|█████████▌| 343/360 [01:23<00:04,  4.12it/s, loss=1.24] 96%|█████████▌| 344/360 [01:23<00:03,  4.19it/s, loss=1.24] 96%|█████████▌| 345/360 [01:23<00:03,  4.23it/s, loss=1.24] 96%|█████████▌| 346/360 [01:23<00:03,  4.26it/s, loss=1.24] 96%|█████████▋| 347/360 [01:24<00:03,  4.21it/s, loss=1.24] 97%|█████████▋| 348/360 [01:24<00:02,  4.25it/s, loss=1.24] 97%|█████████▋| 349/360 [01:24<00:02,  4.19it/s, loss=1.24] 97%|█████████▋| 350/360 [01:24<00:02,  4.20it/s, loss=1.24] 97%|█████████▋| 350/360 [01:25<00:02,  4.20it/s, loss=1.18] 98%|█████████▊| 351/360 [01:25<00:02,  4.24it/s, loss=1.18] 98%|█████████▊| 352/360 [01:25<00:02,  3.72it/s, loss=1.18] 98%|█████████▊| 353/360 [01:25<00:01,  3.89it/s, loss=1.18] 98%|█████████▊| 354/360 [01:25<00:01,  4.01it/s, loss=1.18] 99%|█████████▊| 355/360 [01:26<00:01,  4.10it/s, loss=1.18] 99%|█████████▉| 356/360 [01:26<00:00,  4.19it/s, loss=1.18] 99%|█████████▉| 357/360 [01:26<00:00,  4.24it/s, loss=1.18] 99%|█████████▉| 358/360 [01:26<00:00,  4.28it/s, loss=1.18]100%|█████████▉| 359/360 [01:26<00:00,  4.31it/s, loss=1.18]100%|██████████| 360/360 [01:27<00:00,  4.33it/s, loss=1.18]100%|██████████| 360/360 [01:27<00:00,  4.13it/s, loss=1.18]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:27,  3.66it/s]  2%|▏         | 2/100 [00:00<00:17,  5.50it/s]  3%|▎         | 3/100 [00:00<00:14,  6.56it/s]  4%|▍         | 4/100 [00:00<00:13,  7.32it/s]  5%|▌         | 5/100 [00:00<00:12,  7.82it/s]  6%|▌         | 6/100 [00:00<00:11,  8.17it/s]  7%|▋         | 7/100 [00:00<00:11,  8.43it/s]  8%|▊         | 8/100 [00:01<00:11,  8.20it/s]  9%|▉         | 9/100 [00:01<00:11,  8.12it/s] 10%|█         | 10/100 [00:01<00:10,  8.35it/s] 11%|█         | 11/100 [00:01<00:10,  8.56it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.73it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.83it/s] 14%|█▍        | 14/100 [00:01<00:10,  8.48it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.57it/s] 16%|█▌        | 16/100 [00:02<00:10,  8.36it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.59it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.65it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.77it/s] 20%|██        | 20/100 [00:02<00:09,  8.72it/s] 21%|██        | 21/100 [00:02<00:09,  8.49it/s] 22%|██▏       | 22/100 [00:02<00:09,  8.62it/s] 23%|██▎       | 23/100 [00:02<00:08,  8.78it/s] 24%|██▍       | 24/100 [00:02<00:08,  8.85it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.92it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.60it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.68it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.82it/s] 29%|██▉       | 29/100 [00:03<00:07,  8.91it/s] 30%|███       | 30/100 [00:03<00:07,  8.97it/s] 31%|███       | 31/100 [00:03<00:07,  9.01it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.72it/s] 33%|███▎      | 33/100 [00:03<00:07,  8.63it/s] 34%|███▍      | 34/100 [00:04<00:07,  8.65it/s] 35%|███▌      | 35/100 [00:04<00:07,  8.42it/s] 36%|███▌      | 36/100 [00:04<00:07,  8.56it/s] 37%|███▋      | 37/100 [00:04<00:09,  6.34it/s] 38%|███▊      | 38/100 [00:04<00:08,  6.95it/s] 39%|███▉      | 39/100 [00:04<00:08,  7.23it/s] 40%|████      | 40/100 [00:04<00:07,  7.53it/s] 41%|████      | 41/100 [00:05<00:07,  7.99it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.32it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.49it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.64it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.75it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.75it/s] 47%|████▋     | 47/100 [00:05<00:05,  8.88it/s] 48%|████▊     | 48/100 [00:05<00:06,  8.55it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.73it/s] 50%|█████     | 50/100 [00:06<00:05,  8.87it/s] 51%|█████     | 51/100 [00:06<00:05,  8.95it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.01it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.07it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.70it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.82it/s] 56%|█████▌    | 56/100 [00:06<00:04,  8.90it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.98it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.99it/s] 59%|█████▉    | 59/100 [00:07<00:04,  9.01it/s] 60%|██████    | 60/100 [00:07<00:04,  9.02it/s] 61%|██████    | 61/100 [00:07<00:04,  9.04it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.06it/s] 63%|██████▎   | 63/100 [00:07<00:04,  9.07it/s] 64%|██████▍   | 64/100 [00:07<00:03,  9.08it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.11it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.13it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.13it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.12it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.14it/s] 70%|███████   | 70/100 [00:08<00:03,  9.13it/s] 71%|███████   | 71/100 [00:08<00:03,  9.14it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.14it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.14it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.15it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.16it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.13it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.26it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.33it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.38it/s] 80%|████████  | 80/100 [00:09<00:02,  9.42it/s] 81%|████████  | 81/100 [00:09<00:02,  9.45it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.46it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.46it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.48it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.51it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.51it/s] 87%|████████▋ | 87/100 [00:10<00:01,  7.08it/s] 88%|████████▊ | 88/100 [00:10<00:01,  7.66it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.15it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.54it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.83it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.04it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.19it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.30it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.37it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.41it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.44it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.48it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.48it/s]100%|██████████| 100/100 [00:11<00:00,  9.48it/s]100%|██████████| 100/100 [00:11<00:00,  8.64it/s]
Model tested on 100 tasks. Accuracy: 64.24%
0.20000000298023224 0.20000000298023224
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:28,  3.53it/s]  2%|▏         | 2/100 [00:00<00:19,  5.15it/s]  3%|▎         | 3/100 [00:00<00:15,  6.17it/s]  4%|▍         | 4/100 [00:00<00:13,  6.91it/s]  5%|▌         | 5/100 [00:00<00:12,  7.41it/s]  6%|▌         | 6/100 [00:00<00:12,  7.71it/s]  7%|▋         | 7/100 [00:01<00:11,  7.92it/s]  8%|▊         | 8/100 [00:01<00:11,  8.07it/s]  9%|▉         | 9/100 [00:01<00:11,  8.22it/s] 10%|█         | 10/100 [00:01<00:10,  8.43it/s] 11%|█         | 11/100 [00:01<00:10,  8.60it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.63it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.72it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.80it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.88it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.92it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.96it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.99it/s] 19%|█▉        | 19/100 [00:02<00:08,  9.03it/s] 20%|██        | 20/100 [00:02<00:08,  9.05it/s] 21%|██        | 21/100 [00:02<00:08,  9.06it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.06it/s] 23%|██▎       | 23/100 [00:02<00:08,  9.09it/s] 24%|██▍       | 24/100 [00:02<00:08,  8.98it/s] 25%|██▌       | 25/100 [00:03<00:08,  9.00it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.96it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.95it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.93it/s] 29%|██▉       | 29/100 [00:03<00:07,  8.96it/s] 30%|███       | 30/100 [00:03<00:08,  8.69it/s] 31%|███       | 31/100 [00:03<00:07,  8.81it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.87it/s] 33%|███▎      | 33/100 [00:03<00:07,  8.92it/s] 34%|███▍      | 34/100 [00:04<00:07,  8.95it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.02it/s] 36%|███▌      | 36/100 [00:04<00:09,  6.53it/s] 37%|███▋      | 37/100 [00:04<00:08,  7.17it/s] 38%|███▊      | 38/100 [00:04<00:08,  7.68it/s] 39%|███▉      | 39/100 [00:04<00:07,  8.10it/s] 40%|████      | 40/100 [00:04<00:07,  8.40it/s] 41%|████      | 41/100 [00:04<00:06,  8.57it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.73it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.85it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.80it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.87it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.93it/s] 47%|████▋     | 47/100 [00:05<00:05,  8.99it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.99it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.72it/s] 50%|█████     | 50/100 [00:05<00:05,  8.81it/s] 51%|█████     | 51/100 [00:06<00:05,  8.90it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.83it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.88it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.91it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.96it/s] 56%|█████▌    | 56/100 [00:06<00:04,  8.89it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.88it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.90it/s] 59%|█████▉    | 59/100 [00:06<00:04,  8.96it/s] 60%|██████    | 60/100 [00:07<00:04,  8.99it/s] 61%|██████    | 61/100 [00:07<00:04,  8.83it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.90it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.85it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.85it/s] 65%|██████▌   | 65/100 [00:07<00:03,  8.88it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.89it/s] 67%|██████▋   | 67/100 [00:07<00:03,  8.93it/s] 68%|██████▊   | 68/100 [00:07<00:03,  8.95it/s] 69%|██████▉   | 69/100 [00:08<00:03,  8.97it/s] 70%|███████   | 70/100 [00:08<00:03,  8.97it/s] 71%|███████   | 71/100 [00:08<00:03,  8.62it/s] 72%|███████▏  | 72/100 [00:08<00:03,  8.74it/s] 73%|███████▎  | 73/100 [00:08<00:03,  8.85it/s] 74%|███████▍  | 74/100 [00:08<00:02,  8.92it/s] 75%|███████▌  | 75/100 [00:08<00:02,  8.98it/s] 76%|███████▌  | 76/100 [00:08<00:02,  8.88it/s] 77%|███████▋  | 77/100 [00:08<00:02,  9.04it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.15it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.24it/s] 80%|████████  | 80/100 [00:09<00:02,  9.31it/s] 81%|████████  | 81/100 [00:09<00:02,  9.36it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.38it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.41it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.41it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.43it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.44it/s] 87%|████████▋ | 87/100 [00:10<00:01,  7.02it/s] 88%|████████▊ | 88/100 [00:10<00:01,  7.63it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.13it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.50it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.80it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.01it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.15it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.26it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.35it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.39it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.37it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.39it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.42it/s]100%|██████████| 100/100 [00:11<00:00,  9.43it/s]100%|██████████| 100/100 [00:11<00:00,  8.64it/s]
Model tested on 100 tasks. Accuracy: 39.31%
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:00<?, ?it/s, loss=7.2]  0%|          | 1/360 [00:00<05:51,  1.02it/s, loss=7.2]  1%|          | 2/360 [00:01<03:35,  1.66it/s, loss=7.2]  1%|          | 3/360 [00:01<02:37,  2.27it/s, loss=7.2]  1%|          | 4/360 [00:01<02:10,  2.74it/s, loss=7.2]  1%|▏         | 5/360 [00:02<01:54,  3.11it/s, loss=7.2]  2%|▏         | 6/360 [00:02<01:43,  3.41it/s, loss=7.2]  2%|▏         | 7/360 [00:02<01:37,  3.62it/s, loss=7.2]  2%|▏         | 8/360 [00:02<01:33,  3.78it/s, loss=7.2]  2%|▎         | 9/360 [00:03<01:29,  3.91it/s, loss=7.2]  3%|▎         | 10/360 [00:03<01:27,  3.99it/s, loss=7.2]  3%|▎         | 10/360 [00:03<01:27,  3.99it/s, loss=5.09]  3%|▎         | 11/360 [00:03<01:26,  4.05it/s, loss=5.09]  3%|▎         | 12/360 [00:03<01:25,  4.08it/s, loss=5.09]  4%|▎         | 13/360 [00:03<01:24,  4.11it/s, loss=5.09]  4%|▍         | 14/360 [00:04<01:24,  4.12it/s, loss=5.09]  4%|▍         | 15/360 [00:04<01:23,  4.14it/s, loss=5.09]  4%|▍         | 16/360 [00:04<01:22,  4.15it/s, loss=5.09]  5%|▍         | 17/360 [00:04<01:22,  4.16it/s, loss=5.09]  5%|▌         | 18/360 [00:05<01:22,  4.17it/s, loss=5.09]  5%|▌         | 19/360 [00:05<01:21,  4.18it/s, loss=5.09]  6%|▌         | 20/360 [00:05<01:21,  4.18it/s, loss=5.09]  6%|▌         | 20/360 [00:05<01:21,  4.18it/s, loss=3.85]  6%|▌         | 21/360 [00:05<01:20,  4.19it/s, loss=3.85]  6%|▌         | 22/360 [00:06<01:20,  4.20it/s, loss=3.85]  6%|▋         | 23/360 [00:06<01:20,  4.20it/s, loss=3.85]  7%|▋         | 24/360 [00:06<01:19,  4.20it/s, loss=3.85]  7%|▋         | 25/360 [00:06<01:19,  4.21it/s, loss=3.85]  7%|▋         | 26/360 [00:07<01:35,  3.50it/s, loss=3.85]  8%|▊         | 27/360 [00:07<01:29,  3.71it/s, loss=3.85]  8%|▊         | 28/360 [00:07<01:26,  3.85it/s, loss=3.85]  8%|▊         | 29/360 [00:07<01:23,  3.95it/s, loss=3.85]  8%|▊         | 30/360 [00:08<01:21,  4.03it/s, loss=3.85]  8%|▊         | 30/360 [00:08<01:21,  4.03it/s, loss=3.29]  9%|▊         | 31/360 [00:08<01:20,  4.08it/s, loss=3.29]  9%|▉         | 32/360 [00:08<01:19,  4.13it/s, loss=3.29]  9%|▉         | 33/360 [00:08<01:18,  4.16it/s, loss=3.29]  9%|▉         | 34/360 [00:09<01:18,  4.17it/s, loss=3.29] 10%|▉         | 35/360 [00:09<01:17,  4.19it/s, loss=3.29] 10%|█         | 36/360 [00:09<01:17,  4.20it/s, loss=3.29] 10%|█         | 37/360 [00:09<01:16,  4.21it/s, loss=3.29] 11%|█         | 38/360 [00:10<01:16,  4.21it/s, loss=3.29] 11%|█         | 39/360 [00:10<01:16,  4.18it/s, loss=3.29] 11%|█         | 40/360 [00:10<01:16,  4.19it/s, loss=3.29] 11%|█         | 40/360 [00:10<01:16,  4.19it/s, loss=2.94] 11%|█▏        | 41/360 [00:10<01:16,  4.19it/s, loss=2.94] 12%|█▏        | 42/360 [00:11<01:15,  4.19it/s, loss=2.94] 12%|█▏        | 43/360 [00:11<01:15,  4.20it/s, loss=2.94] 12%|█▏        | 44/360 [00:11<01:15,  4.19it/s, loss=2.94] 12%|█▎        | 45/360 [00:11<01:15,  4.20it/s, loss=2.94] 13%|█▎        | 46/360 [00:11<01:14,  4.21it/s, loss=2.94] 13%|█▎        | 47/360 [00:12<01:14,  4.21it/s, loss=2.94] 13%|█▎        | 48/360 [00:12<01:15,  4.15it/s, loss=2.94] 14%|█▎        | 49/360 [00:12<01:14,  4.17it/s, loss=2.94] 14%|█▍        | 50/360 [00:12<01:14,  4.17it/s, loss=2.94] 14%|█▍        | 50/360 [00:13<01:14,  4.17it/s, loss=2.8]  14%|█▍        | 51/360 [00:13<01:13,  4.18it/s, loss=2.8] 14%|█▍        | 52/360 [00:13<01:13,  4.18it/s, loss=2.8] 15%|█▍        | 53/360 [00:13<01:13,  4.20it/s, loss=2.8] 15%|█▌        | 54/360 [00:13<01:12,  4.20it/s, loss=2.8] 15%|█▌        | 55/360 [00:14<01:12,  4.18it/s, loss=2.8] 16%|█▌        | 56/360 [00:14<01:12,  4.20it/s, loss=2.8] 16%|█▌        | 57/360 [00:14<01:12,  4.20it/s, loss=2.8] 16%|█▌        | 58/360 [00:14<01:11,  4.20it/s, loss=2.8] 16%|█▋        | 59/360 [00:15<01:11,  4.20it/s, loss=2.8] 17%|█▋        | 60/360 [00:15<01:11,  4.21it/s, loss=2.8] 17%|█▋        | 60/360 [00:15<01:11,  4.21it/s, loss=2.73] 17%|█▋        | 61/360 [00:15<01:10,  4.22it/s, loss=2.73] 17%|█▋        | 62/360 [00:15<01:10,  4.22it/s, loss=2.73] 18%|█▊        | 63/360 [00:16<01:10,  4.22it/s, loss=2.73] 18%|█▊        | 64/360 [00:16<01:09,  4.23it/s, loss=2.73] 18%|█▊        | 65/360 [00:16<01:22,  3.60it/s, loss=2.73] 18%|█▊        | 66/360 [00:16<01:17,  3.79it/s, loss=2.73] 19%|█▊        | 67/360 [00:17<01:14,  3.91it/s, loss=2.73] 19%|█▉        | 68/360 [00:17<01:12,  4.01it/s, loss=2.73] 19%|█▉        | 69/360 [00:17<01:11,  4.07it/s, loss=2.73] 19%|█▉        | 70/360 [00:17<01:10,  4.10it/s, loss=2.73] 19%|█▉        | 70/360 [00:18<01:10,  4.10it/s, loss=2.47] 20%|█▉        | 71/360 [00:18<01:10,  4.11it/s, loss=2.47] 20%|██        | 72/360 [00:18<01:09,  4.13it/s, loss=2.47] 20%|██        | 73/360 [00:18<01:09,  4.15it/s, loss=2.47] 21%|██        | 74/360 [00:18<01:08,  4.16it/s, loss=2.47] 21%|██        | 75/360 [00:19<01:08,  4.18it/s, loss=2.47] 21%|██        | 76/360 [00:19<01:07,  4.19it/s, loss=2.47] 21%|██▏       | 77/360 [00:19<01:07,  4.17it/s, loss=2.47] 22%|██▏       | 78/360 [00:19<01:07,  4.19it/s, loss=2.47] 22%|██▏       | 79/360 [00:19<01:07,  4.18it/s, loss=2.47] 22%|██▏       | 80/360 [00:20<01:07,  4.16it/s, loss=2.47] 22%|██▏       | 80/360 [00:20<01:07,  4.16it/s, loss=2.33] 22%|██▎       | 81/360 [00:20<01:06,  4.17it/s, loss=2.33] 23%|██▎       | 82/360 [00:20<01:06,  4.18it/s, loss=2.33] 23%|██▎       | 83/360 [00:20<01:06,  4.18it/s, loss=2.33] 23%|██▎       | 84/360 [00:21<01:05,  4.18it/s, loss=2.33] 24%|██▎       | 85/360 [00:21<01:05,  4.18it/s, loss=2.33] 24%|██▍       | 86/360 [00:21<01:05,  4.17it/s, loss=2.33] 24%|██▍       | 87/360 [00:21<01:05,  4.17it/s, loss=2.33] 24%|██▍       | 88/360 [00:22<01:05,  4.17it/s, loss=2.33] 25%|██▍       | 89/360 [00:22<01:04,  4.18it/s, loss=2.33] 25%|██▌       | 90/360 [00:22<01:04,  4.18it/s, loss=2.33] 25%|██▌       | 90/360 [00:22<01:04,  4.18it/s, loss=2.18] 25%|██▌       | 91/360 [00:22<01:04,  4.19it/s, loss=2.18] 26%|██▌       | 92/360 [00:23<01:03,  4.20it/s, loss=2.18] 26%|██▌       | 93/360 [00:23<01:03,  4.21it/s, loss=2.18] 26%|██▌       | 94/360 [00:23<01:03,  4.21it/s, loss=2.18] 26%|██▋       | 95/360 [00:23<01:02,  4.21it/s, loss=2.18] 27%|██▋       | 96/360 [00:24<01:02,  4.22it/s, loss=2.18] 27%|██▋       | 97/360 [00:24<01:02,  4.22it/s, loss=2.18] 27%|██▋       | 98/360 [00:24<01:02,  4.22it/s, loss=2.18] 28%|██▊       | 99/360 [00:24<01:12,  3.58it/s, loss=2.18] 28%|██▊       | 100/360 [00:25<01:09,  3.77it/s, loss=2.18] 28%|██▊       | 100/360 [00:25<01:09,  3.77it/s, loss=2.17] 28%|██▊       | 101/360 [00:25<01:06,  3.89it/s, loss=2.17] 28%|██▊       | 102/360 [00:25<01:04,  3.99it/s, loss=2.17] 29%|██▊       | 103/360 [00:25<01:03,  4.06it/s, loss=2.17] 29%|██▉       | 104/360 [00:26<01:02,  4.11it/s, loss=2.17] 29%|██▉       | 105/360 [00:26<01:01,  4.15it/s, loss=2.17] 29%|██▉       | 106/360 [00:26<01:01,  4.13it/s, loss=2.17] 30%|██▉       | 107/360 [00:26<01:00,  4.15it/s, loss=2.17] 30%|███       | 108/360 [00:27<01:00,  4.15it/s, loss=2.17] 30%|███       | 109/360 [00:27<01:00,  4.13it/s, loss=2.17] 31%|███       | 110/360 [00:27<01:00,  4.16it/s, loss=2.17] 31%|███       | 110/360 [00:27<01:00,  4.16it/s, loss=2.14] 31%|███       | 111/360 [00:27<00:59,  4.17it/s, loss=2.14] 31%|███       | 112/360 [00:27<00:59,  4.18it/s, loss=2.14] 31%|███▏      | 113/360 [00:28<00:59,  4.18it/s, loss=2.14] 32%|███▏      | 114/360 [00:28<00:58,  4.18it/s, loss=2.14] 32%|███▏      | 115/360 [00:28<00:58,  4.19it/s, loss=2.14] 32%|███▏      | 116/360 [00:28<00:58,  4.20it/s, loss=2.14] 32%|███▎      | 117/360 [00:29<00:58,  4.14it/s, loss=2.14] 33%|███▎      | 118/360 [00:29<00:58,  4.16it/s, loss=2.14] 33%|███▎      | 119/360 [00:29<00:57,  4.18it/s, loss=2.14] 33%|███▎      | 120/360 [00:29<00:57,  4.17it/s, loss=2.14] 33%|███▎      | 120/360 [00:30<00:57,  4.17it/s, loss=1.99] 34%|███▎      | 121/360 [00:30<00:57,  4.18it/s, loss=1.99] 34%|███▍      | 122/360 [00:30<00:56,  4.19it/s, loss=1.99] 34%|███▍      | 123/360 [00:30<00:56,  4.16it/s, loss=1.99] 34%|███▍      | 124/360 [00:30<00:56,  4.19it/s, loss=1.99] 35%|███▍      | 125/360 [00:31<00:55,  4.20it/s, loss=1.99] 35%|███▌      | 126/360 [00:31<00:55,  4.20it/s, loss=1.99] 35%|███▌      | 127/360 [00:31<00:55,  4.19it/s, loss=1.99] 36%|███▌      | 128/360 [00:31<00:55,  4.21it/s, loss=1.99] 36%|███▌      | 129/360 [00:32<00:55,  4.18it/s, loss=1.99] 36%|███▌      | 130/360 [00:32<00:54,  4.20it/s, loss=1.99] 36%|███▌      | 130/360 [00:32<00:54,  4.20it/s, loss=1.92] 36%|███▋      | 131/360 [00:32<00:54,  4.21it/s, loss=1.92] 37%|███▋      | 132/360 [00:32<00:54,  4.22it/s, loss=1.92] 37%|███▋      | 133/360 [00:33<01:03,  3.57it/s, loss=1.92] 37%|███▋      | 134/360 [00:33<00:59,  3.77it/s, loss=1.92] 38%|███▊      | 135/360 [00:33<00:58,  3.87it/s, loss=1.92] 38%|███▊      | 136/360 [00:33<00:56,  3.97it/s, loss=1.92] 38%|███▊      | 137/360 [00:34<00:55,  4.04it/s, loss=1.92] 38%|███▊      | 138/360 [00:34<00:54,  4.09it/s, loss=1.92] 39%|███▊      | 139/360 [00:34<00:53,  4.13it/s, loss=1.92] 39%|███▉      | 140/360 [00:34<00:52,  4.16it/s, loss=1.92] 39%|███▉      | 140/360 [00:35<00:52,  4.16it/s, loss=1.86] 39%|███▉      | 141/360 [00:35<00:52,  4.17it/s, loss=1.86] 39%|███▉      | 142/360 [00:35<00:52,  4.18it/s, loss=1.86] 40%|███▉      | 143/360 [00:35<00:51,  4.19it/s, loss=1.86] 40%|████      | 144/360 [00:35<00:51,  4.20it/s, loss=1.86] 40%|████      | 145/360 [00:35<00:51,  4.20it/s, loss=1.86] 41%|████      | 146/360 [00:36<00:50,  4.20it/s, loss=1.86] 41%|████      | 147/360 [00:36<00:50,  4.19it/s, loss=1.86] 41%|████      | 148/360 [00:36<00:50,  4.18it/s, loss=1.86] 41%|████▏     | 149/360 [00:36<00:50,  4.19it/s, loss=1.86] 42%|████▏     | 150/360 [00:37<00:50,  4.20it/s, loss=1.86] 42%|████▏     | 150/360 [00:37<00:50,  4.20it/s, loss=1.91] 42%|████▏     | 151/360 [00:37<00:49,  4.20it/s, loss=1.91] 42%|████▏     | 152/360 [00:37<00:49,  4.20it/s, loss=1.91] 42%|████▎     | 153/360 [00:37<00:49,  4.20it/s, loss=1.91] 43%|████▎     | 154/360 [00:38<00:49,  4.20it/s, loss=1.91] 43%|████▎     | 155/360 [00:38<00:48,  4.19it/s, loss=1.91] 43%|████▎     | 156/360 [00:38<00:48,  4.19it/s, loss=1.91] 44%|████▎     | 157/360 [00:38<00:48,  4.20it/s, loss=1.91] 44%|████▍     | 158/360 [00:39<00:48,  4.20it/s, loss=1.91] 44%|████▍     | 159/360 [00:39<00:48,  4.19it/s, loss=1.91] 44%|████▍     | 160/360 [00:39<00:47,  4.18it/s, loss=1.91] 44%|████▍     | 160/360 [00:39<00:47,  4.18it/s, loss=1.82] 45%|████▍     | 161/360 [00:39<00:47,  4.19it/s, loss=1.82] 45%|████▌     | 162/360 [00:40<00:47,  4.20it/s, loss=1.82] 45%|████▌     | 163/360 [00:40<00:46,  4.21it/s, loss=1.82] 46%|████▌     | 164/360 [00:40<00:46,  4.21it/s, loss=1.82] 46%|████▌     | 165/360 [00:40<00:46,  4.20it/s, loss=1.82] 46%|████▌     | 166/360 [00:40<00:46,  4.21it/s, loss=1.82] 46%|████▋     | 167/360 [00:41<00:47,  4.09it/s, loss=1.82] 47%|████▋     | 168/360 [00:41<00:54,  3.52it/s, loss=1.82] 47%|████▋     | 169/360 [00:41<00:51,  3.72it/s, loss=1.82] 47%|████▋     | 170/360 [00:42<00:49,  3.86it/s, loss=1.82] 47%|████▋     | 170/360 [00:42<00:49,  3.86it/s, loss=1.7]  48%|████▊     | 171/360 [00:42<00:47,  3.97it/s, loss=1.7] 48%|████▊     | 172/360 [00:42<00:46,  4.04it/s, loss=1.7] 48%|████▊     | 173/360 [00:42<00:45,  4.10it/s, loss=1.7] 48%|████▊     | 174/360 [00:43<00:44,  4.14it/s, loss=1.7] 49%|████▊     | 175/360 [00:43<00:44,  4.17it/s, loss=1.7] 49%|████▉     | 176/360 [00:43<00:43,  4.18it/s, loss=1.7] 49%|████▉     | 177/360 [00:43<00:43,  4.20it/s, loss=1.7] 49%|████▉     | 178/360 [00:43<00:43,  4.21it/s, loss=1.7] 50%|████▉     | 179/360 [00:44<00:42,  4.21it/s, loss=1.7] 50%|█████     | 180/360 [00:44<00:42,  4.21it/s, loss=1.7] 50%|█████     | 180/360 [00:44<00:42,  4.21it/s, loss=1.74] 50%|█████     | 181/360 [00:44<00:42,  4.21it/s, loss=1.74] 51%|█████     | 182/360 [00:44<00:42,  4.21it/s, loss=1.74] 51%|█████     | 183/360 [00:45<00:42,  4.21it/s, loss=1.74] 51%|█████     | 184/360 [00:45<00:41,  4.21it/s, loss=1.74] 51%|█████▏    | 185/360 [00:45<00:41,  4.20it/s, loss=1.74] 52%|█████▏    | 186/360 [00:45<00:41,  4.20it/s, loss=1.74] 52%|█████▏    | 187/360 [00:46<00:41,  4.20it/s, loss=1.74] 52%|█████▏    | 188/360 [00:46<00:40,  4.21it/s, loss=1.74] 52%|█████▎    | 189/360 [00:46<00:40,  4.21it/s, loss=1.74] 53%|█████▎    | 190/360 [00:46<00:40,  4.20it/s, loss=1.74] 53%|█████▎    | 190/360 [00:47<00:40,  4.20it/s, loss=1.66] 53%|█████▎    | 191/360 [00:47<00:40,  4.19it/s, loss=1.66] 53%|█████▎    | 192/360 [00:47<00:40,  4.19it/s, loss=1.66] 54%|█████▎    | 193/360 [00:47<00:39,  4.20it/s, loss=1.66] 54%|█████▍    | 194/360 [00:47<00:39,  4.20it/s, loss=1.66] 54%|█████▍    | 195/360 [00:48<00:39,  4.20it/s, loss=1.66] 54%|█████▍    | 196/360 [00:48<00:38,  4.21it/s, loss=1.66] 55%|█████▍    | 197/360 [00:48<00:38,  4.21it/s, loss=1.66] 55%|█████▌    | 198/360 [00:48<00:38,  4.21it/s, loss=1.66] 55%|█████▌    | 199/360 [00:48<00:38,  4.21it/s, loss=1.66] 56%|█████▌    | 200/360 [00:49<00:37,  4.22it/s, loss=1.66] 56%|█████▌    | 200/360 [00:49<00:37,  4.22it/s, loss=1.63] 56%|█████▌    | 201/360 [00:49<00:37,  4.22it/s, loss=1.63] 56%|█████▌    | 202/360 [00:49<00:37,  4.23it/s, loss=1.63] 56%|█████▋    | 203/360 [00:50<00:43,  3.58it/s, loss=1.63] 57%|█████▋    | 204/360 [00:50<00:41,  3.77it/s, loss=1.63] 57%|█████▋    | 205/360 [00:50<00:39,  3.90it/s, loss=1.63] 57%|█████▋    | 206/360 [00:50<00:38,  3.99it/s, loss=1.63] 57%|█████▊    | 207/360 [00:50<00:37,  4.05it/s, loss=1.63] 58%|█████▊    | 208/360 [00:51<00:37,  4.10it/s, loss=1.63] 58%|█████▊    | 209/360 [00:51<00:36,  4.13it/s, loss=1.63] 58%|█████▊    | 210/360 [00:51<00:36,  4.16it/s, loss=1.63] 58%|█████▊    | 210/360 [00:51<00:36,  4.16it/s, loss=1.53] 59%|█████▊    | 211/360 [00:51<00:35,  4.17it/s, loss=1.53] 59%|█████▉    | 212/360 [00:52<00:35,  4.18it/s, loss=1.53] 59%|█████▉    | 213/360 [00:52<00:35,  4.19it/s, loss=1.53] 59%|█████▉    | 214/360 [00:52<00:34,  4.21it/s, loss=1.53] 60%|█████▉    | 215/360 [00:52<00:34,  4.21it/s, loss=1.53] 60%|██████    | 216/360 [00:53<00:34,  4.21it/s, loss=1.53] 60%|██████    | 217/360 [00:53<00:33,  4.22it/s, loss=1.53] 61%|██████    | 218/360 [00:53<00:33,  4.21it/s, loss=1.53] 61%|██████    | 219/360 [00:53<00:33,  4.21it/s, loss=1.53] 61%|██████    | 220/360 [00:54<00:33,  4.21it/s, loss=1.53] 61%|██████    | 220/360 [00:54<00:33,  4.21it/s, loss=1.65] 61%|██████▏   | 221/360 [00:54<00:32,  4.22it/s, loss=1.65] 62%|██████▏   | 222/360 [00:54<00:32,  4.22it/s, loss=1.65] 62%|██████▏   | 223/360 [00:54<00:32,  4.23it/s, loss=1.65] 62%|██████▏   | 224/360 [00:55<00:32,  4.24it/s, loss=1.65] 62%|██████▎   | 225/360 [00:55<00:31,  4.24it/s, loss=1.65] 63%|██████▎   | 226/360 [00:55<00:31,  4.25it/s, loss=1.65] 63%|██████▎   | 227/360 [00:55<00:31,  4.25it/s, loss=1.65] 63%|██████▎   | 228/360 [00:55<00:31,  4.25it/s, loss=1.65] 64%|██████▎   | 229/360 [00:56<00:30,  4.25it/s, loss=1.65] 64%|██████▍   | 230/360 [00:56<00:30,  4.25it/s, loss=1.65] 64%|██████▍   | 230/360 [00:56<00:30,  4.25it/s, loss=1.51] 64%|██████▍   | 231/360 [00:56<00:30,  4.25it/s, loss=1.51] 64%|██████▍   | 232/360 [00:56<00:30,  4.25it/s, loss=1.51] 65%|██████▍   | 233/360 [00:57<00:29,  4.25it/s, loss=1.51] 65%|██████▌   | 234/360 [00:57<00:29,  4.23it/s, loss=1.51] 65%|██████▌   | 235/360 [00:57<00:29,  4.23it/s, loss=1.51] 66%|██████▌   | 236/360 [00:57<00:29,  4.24it/s, loss=1.51] 66%|██████▌   | 237/360 [00:58<00:29,  4.24it/s, loss=1.51] 66%|██████▌   | 238/360 [00:58<00:28,  4.24it/s, loss=1.51] 66%|██████▋   | 239/360 [00:58<00:28,  4.21it/s, loss=1.51] 67%|██████▋   | 240/360 [00:58<00:28,  4.17it/s, loss=1.51] 67%|██████▋   | 240/360 [00:59<00:28,  4.17it/s, loss=1.5]  67%|██████▋   | 241/360 [00:59<00:28,  4.19it/s, loss=1.5] 67%|██████▋   | 242/360 [00:59<00:28,  4.21it/s, loss=1.5] 68%|██████▊   | 243/360 [00:59<00:27,  4.22it/s, loss=1.5] 68%|██████▊   | 244/360 [00:59<00:27,  4.22it/s, loss=1.5] 68%|██████▊   | 245/360 [00:59<00:27,  4.24it/s, loss=1.5] 68%|██████▊   | 246/360 [01:00<00:26,  4.24it/s, loss=1.5] 69%|██████▊   | 247/360 [01:00<00:26,  4.23it/s, loss=1.5] 69%|██████▉   | 248/360 [01:00<00:26,  4.15it/s, loss=1.5] 69%|██████▉   | 249/360 [01:00<00:26,  4.19it/s, loss=1.5] 69%|██████▉   | 250/360 [01:01<00:26,  4.21it/s, loss=1.5] 69%|██████▉   | 250/360 [01:01<00:26,  4.21it/s, loss=1.39] 70%|██████▉   | 251/360 [01:01<00:30,  3.58it/s, loss=1.39] 70%|███████   | 252/360 [01:01<00:28,  3.77it/s, loss=1.39] 70%|███████   | 253/360 [01:02<00:27,  3.90it/s, loss=1.39] 71%|███████   | 254/360 [01:02<00:26,  3.99it/s, loss=1.39] 71%|███████   | 255/360 [01:02<00:25,  4.06it/s, loss=1.39] 71%|███████   | 256/360 [01:02<00:25,  4.12it/s, loss=1.39] 71%|███████▏  | 257/360 [01:02<00:24,  4.15it/s, loss=1.39] 72%|███████▏  | 258/360 [01:03<00:24,  4.17it/s, loss=1.39] 72%|███████▏  | 259/360 [01:03<00:24,  4.15it/s, loss=1.39] 72%|███████▏  | 260/360 [01:03<00:23,  4.17it/s, loss=1.39] 72%|███████▏  | 260/360 [01:03<00:23,  4.17it/s, loss=1.38] 72%|███████▎  | 261/360 [01:03<00:23,  4.17it/s, loss=1.38] 73%|███████▎  | 262/360 [01:04<00:23,  4.16it/s, loss=1.38] 73%|███████▎  | 263/360 [01:04<00:23,  4.17it/s, loss=1.38] 73%|███████▎  | 264/360 [01:04<00:22,  4.18it/s, loss=1.38] 74%|███████▎  | 265/360 [01:04<00:22,  4.19it/s, loss=1.38] 74%|███████▍  | 266/360 [01:05<00:22,  4.19it/s, loss=1.38] 74%|███████▍  | 267/360 [01:05<00:22,  4.19it/s, loss=1.38] 74%|███████▍  | 268/360 [01:05<00:21,  4.19it/s, loss=1.38] 75%|███████▍  | 269/360 [01:05<00:21,  4.18it/s, loss=1.38] 75%|███████▌  | 270/360 [01:06<00:21,  4.18it/s, loss=1.38] 75%|███████▌  | 270/360 [01:06<00:21,  4.18it/s, loss=1.48] 75%|███████▌  | 271/360 [01:06<00:21,  4.19it/s, loss=1.48] 76%|███████▌  | 272/360 [01:06<00:20,  4.20it/s, loss=1.48] 76%|███████▌  | 273/360 [01:06<00:20,  4.17it/s, loss=1.48] 76%|███████▌  | 274/360 [01:07<00:20,  4.18it/s, loss=1.48] 76%|███████▋  | 275/360 [01:07<00:20,  4.18it/s, loss=1.48] 77%|███████▋  | 276/360 [01:07<00:20,  4.20it/s, loss=1.48] 77%|███████▋  | 277/360 [01:07<00:19,  4.21it/s, loss=1.48] 77%|███████▋  | 278/360 [01:07<00:19,  4.22it/s, loss=1.48] 78%|███████▊  | 279/360 [01:08<00:19,  4.23it/s, loss=1.48] 78%|███████▊  | 280/360 [01:08<00:18,  4.23it/s, loss=1.48] 78%|███████▊  | 280/360 [01:08<00:18,  4.23it/s, loss=1.38] 78%|███████▊  | 281/360 [01:08<00:18,  4.24it/s, loss=1.38] 78%|███████▊  | 282/360 [01:08<00:18,  4.24it/s, loss=1.38] 79%|███████▊  | 283/360 [01:09<00:18,  4.25it/s, loss=1.38] 79%|███████▉  | 284/360 [01:09<00:17,  4.23it/s, loss=1.38] 79%|███████▉  | 285/360 [01:09<00:17,  4.23it/s, loss=1.38] 79%|███████▉  | 286/360 [01:09<00:17,  4.24it/s, loss=1.38] 80%|███████▉  | 287/360 [01:10<00:17,  4.24it/s, loss=1.38] 80%|████████  | 288/360 [01:10<00:17,  4.18it/s, loss=1.38] 80%|████████  | 289/360 [01:10<00:16,  4.20it/s, loss=1.38] 81%|████████  | 290/360 [01:10<00:16,  4.20it/s, loss=1.38] 81%|████████  | 290/360 [01:11<00:16,  4.20it/s, loss=1.38] 81%|████████  | 291/360 [01:11<00:16,  4.22it/s, loss=1.38] 81%|████████  | 292/360 [01:11<00:16,  4.23it/s, loss=1.38] 81%|████████▏ | 293/360 [01:11<00:15,  4.24it/s, loss=1.38] 82%|████████▏ | 294/360 [01:11<00:15,  4.24it/s, loss=1.38] 82%|████████▏ | 295/360 [01:11<00:15,  4.24it/s, loss=1.38] 82%|████████▏ | 296/360 [01:12<00:15,  4.18it/s, loss=1.38] 82%|████████▎ | 297/360 [01:12<00:14,  4.20it/s, loss=1.38] 83%|████████▎ | 298/360 [01:12<00:14,  4.22it/s, loss=1.38] 83%|████████▎ | 299/360 [01:12<00:14,  4.17it/s, loss=1.38] 83%|████████▎ | 300/360 [01:13<00:14,  4.20it/s, loss=1.38] 83%|████████▎ | 300/360 [01:13<00:14,  4.20it/s, loss=1.41] 84%|████████▎ | 301/360 [01:13<00:13,  4.22it/s, loss=1.41] 84%|████████▍ | 302/360 [01:13<00:13,  4.24it/s, loss=1.41] 84%|████████▍ | 303/360 [01:13<00:13,  4.25it/s, loss=1.41] 84%|████████▍ | 304/360 [01:14<00:13,  4.26it/s, loss=1.41] 85%|████████▍ | 305/360 [01:14<00:15,  3.59it/s, loss=1.41] 85%|████████▌ | 306/360 [01:14<00:14,  3.78it/s, loss=1.41] 85%|████████▌ | 307/360 [01:14<00:13,  3.91it/s, loss=1.41] 86%|████████▌ | 308/360 [01:15<00:12,  4.00it/s, loss=1.41] 86%|████████▌ | 309/360 [01:15<00:12,  4.06it/s, loss=1.41] 86%|████████▌ | 310/360 [01:15<00:12,  4.12it/s, loss=1.41] 86%|████████▌ | 310/360 [01:15<00:12,  4.12it/s, loss=1.27] 86%|████████▋ | 311/360 [01:15<00:11,  4.15it/s, loss=1.27] 87%|████████▋ | 312/360 [01:16<00:11,  4.17it/s, loss=1.27] 87%|████████▋ | 313/360 [01:16<00:11,  4.19it/s, loss=1.27] 87%|████████▋ | 314/360 [01:16<00:10,  4.19it/s, loss=1.27] 88%|████████▊ | 315/360 [01:16<00:10,  4.16it/s, loss=1.27] 88%|████████▊ | 316/360 [01:17<00:10,  4.18it/s, loss=1.27] 88%|████████▊ | 317/360 [01:17<00:10,  4.17it/s, loss=1.27] 88%|████████▊ | 318/360 [01:17<00:10,  4.18it/s, loss=1.27] 89%|████████▊ | 319/360 [01:17<00:09,  4.18it/s, loss=1.27] 89%|████████▉ | 320/360 [01:18<00:09,  4.19it/s, loss=1.27] 89%|████████▉ | 320/360 [01:18<00:09,  4.19it/s, loss=1.36] 89%|████████▉ | 321/360 [01:18<00:09,  4.19it/s, loss=1.36] 89%|████████▉ | 322/360 [01:18<00:09,  4.20it/s, loss=1.36] 90%|████████▉ | 323/360 [01:18<00:08,  4.20it/s, loss=1.36] 90%|█████████ | 324/360 [01:19<00:08,  4.19it/s, loss=1.36] 90%|█████████ | 325/360 [01:19<00:08,  4.19it/s, loss=1.36] 91%|█████████ | 326/360 [01:19<00:08,  4.19it/s, loss=1.36] 91%|█████████ | 327/360 [01:19<00:07,  4.20it/s, loss=1.36] 91%|█████████ | 328/360 [01:19<00:07,  4.19it/s, loss=1.36] 91%|█████████▏| 329/360 [01:20<00:07,  4.19it/s, loss=1.36] 92%|█████████▏| 330/360 [01:20<00:07,  4.20it/s, loss=1.36] 92%|█████████▏| 330/360 [01:20<00:07,  4.20it/s, loss=1.31] 92%|█████████▏| 331/360 [01:20<00:06,  4.20it/s, loss=1.31] 92%|█████████▏| 332/360 [01:20<00:06,  4.20it/s, loss=1.31] 92%|█████████▎| 333/360 [01:21<00:06,  4.19it/s, loss=1.31] 93%|█████████▎| 334/360 [01:21<00:06,  4.20it/s, loss=1.31] 93%|█████████▎| 335/360 [01:21<00:05,  4.20it/s, loss=1.31] 93%|█████████▎| 336/360 [01:21<00:05,  4.18it/s, loss=1.31] 94%|█████████▎| 337/360 [01:22<00:05,  4.22it/s, loss=1.31] 94%|█████████▍| 338/360 [01:22<00:05,  4.26it/s, loss=1.31] 94%|█████████▍| 339/360 [01:22<00:04,  4.29it/s, loss=1.31] 94%|█████████▍| 340/360 [01:22<00:05,  3.67it/s, loss=1.31] 94%|█████████▍| 340/360 [01:23<00:05,  3.67it/s, loss=1.26] 95%|█████████▍| 341/360 [01:23<00:04,  3.86it/s, loss=1.26] 95%|█████████▌| 342/360 [01:23<00:04,  4.00it/s, loss=1.26] 95%|█████████▌| 343/360 [01:23<00:04,  4.10it/s, loss=1.26] 96%|█████████▌| 344/360 [01:23<00:03,  4.18it/s, loss=1.26] 96%|█████████▌| 345/360 [01:24<00:03,  4.23it/s, loss=1.26] 96%|█████████▌| 346/360 [01:24<00:03,  4.27it/s, loss=1.26] 96%|█████████▋| 347/360 [01:24<00:03,  4.30it/s, loss=1.26] 97%|█████████▋| 348/360 [01:24<00:02,  4.32it/s, loss=1.26] 97%|█████████▋| 349/360 [01:25<00:02,  4.33it/s, loss=1.26] 97%|█████████▋| 350/360 [01:25<00:02,  4.34it/s, loss=1.26] 97%|█████████▋| 350/360 [01:25<00:02,  4.34it/s, loss=1.3]  98%|█████████▊| 351/360 [01:25<00:02,  4.34it/s, loss=1.3] 98%|█████████▊| 352/360 [01:25<00:01,  4.35it/s, loss=1.3] 98%|█████████▊| 353/360 [01:25<00:01,  4.35it/s, loss=1.3] 98%|█████████▊| 354/360 [01:26<00:01,  4.35it/s, loss=1.3] 99%|█████████▊| 355/360 [01:26<00:01,  4.35it/s, loss=1.3] 99%|█████████▉| 356/360 [01:26<00:00,  4.36it/s, loss=1.3] 99%|█████████▉| 357/360 [01:26<00:00,  4.36it/s, loss=1.3] 99%|█████████▉| 358/360 [01:27<00:00,  4.36it/s, loss=1.3]100%|█████████▉| 359/360 [01:27<00:00,  4.36it/s, loss=1.3]100%|██████████| 360/360 [01:27<00:00,  4.36it/s, loss=1.3]100%|██████████| 360/360 [01:27<00:00,  4.11it/s, loss=1.3]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:28,  3.44it/s]  2%|▏         | 2/100 [00:00<00:18,  5.24it/s]  3%|▎         | 3/100 [00:00<00:15,  6.29it/s]  4%|▍         | 4/100 [00:00<00:13,  7.09it/s]  5%|▌         | 5/100 [00:00<00:12,  7.69it/s]  6%|▌         | 6/100 [00:00<00:11,  8.08it/s]  7%|▋         | 7/100 [00:00<00:11,  8.34it/s]  8%|▊         | 8/100 [00:01<00:10,  8.52it/s]  9%|▉         | 9/100 [00:01<00:10,  8.66it/s] 10%|█         | 10/100 [00:01<00:10,  8.40it/s] 11%|█         | 11/100 [00:01<00:10,  8.57it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.67it/s] 13%|█▎        | 13/100 [00:01<00:10,  8.48it/s] 14%|█▍        | 14/100 [00:01<00:10,  8.58it/s] 15%|█▌        | 15/100 [00:01<00:10,  8.40it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.53it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.68it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.78it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.87it/s] 20%|██        | 20/100 [00:02<00:08,  8.93it/s] 21%|██        | 21/100 [00:02<00:08,  8.97it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.01it/s] 23%|██▎       | 23/100 [00:02<00:12,  6.31it/s] 24%|██▍       | 24/100 [00:03<00:10,  6.97it/s] 25%|██▌       | 25/100 [00:03<00:09,  7.52it/s] 26%|██▌       | 26/100 [00:03<00:09,  7.95it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.28it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.51it/s] 29%|██▉       | 29/100 [00:03<00:08,  8.57it/s] 30%|███       | 30/100 [00:03<00:07,  8.75it/s] 31%|███       | 31/100 [00:03<00:07,  8.87it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.93it/s] 33%|███▎      | 33/100 [00:04<00:07,  8.94it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.00it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.02it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.02it/s] 37%|███▋      | 37/100 [00:04<00:06,  9.02it/s] 38%|███▊      | 38/100 [00:04<00:06,  9.04it/s] 39%|███▉      | 39/100 [00:04<00:06,  9.03it/s] 40%|████      | 40/100 [00:04<00:06,  9.01it/s] 41%|████      | 41/100 [00:04<00:06,  9.01it/s] 42%|████▏     | 42/100 [00:05<00:06,  9.02it/s] 43%|████▎     | 43/100 [00:05<00:06,  9.02it/s] 44%|████▍     | 44/100 [00:05<00:06,  9.00it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.98it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.67it/s] 47%|████▋     | 47/100 [00:05<00:06,  8.76it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.83it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.88it/s] 50%|█████     | 50/100 [00:05<00:05,  8.93it/s] 51%|█████     | 51/100 [00:06<00:05,  8.97it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.99it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.01it/s] 54%|█████▍    | 54/100 [00:06<00:05,  9.03it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.05it/s] 56%|█████▌    | 56/100 [00:06<00:04,  9.06it/s] 57%|█████▋    | 57/100 [00:06<00:04,  9.07it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.09it/s] 59%|█████▉    | 59/100 [00:06<00:04,  9.05it/s] 60%|██████    | 60/100 [00:07<00:04,  9.05it/s] 61%|██████    | 61/100 [00:07<00:04,  9.05it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.07it/s] 63%|██████▎   | 63/100 [00:07<00:04,  9.07it/s] 64%|██████▍   | 64/100 [00:07<00:03,  9.07it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.08it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.11it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.11it/s] 68%|██████▊   | 68/100 [00:07<00:03,  9.11it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.13it/s] 70%|███████   | 70/100 [00:08<00:03,  9.13it/s] 71%|███████   | 71/100 [00:08<00:03,  9.12it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.11it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.11it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.12it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.09it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.06it/s] 77%|███████▋  | 77/100 [00:09<00:03,  6.66it/s] 78%|███████▊  | 78/100 [00:09<00:03,  7.33it/s] 79%|███████▉  | 79/100 [00:09<00:02,  7.89it/s] 80%|████████  | 80/100 [00:09<00:02,  8.31it/s] 81%|████████  | 81/100 [00:09<00:02,  8.64it/s] 82%|████████▏ | 82/100 [00:09<00:02,  8.88it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.07it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.19it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.27it/s] 86%|████████▌ | 86/100 [00:10<00:01,  9.32it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.39it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.41it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.42it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.41it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.43it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.43it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.42it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.42it/s] 95%|█████████▌| 95/100 [00:10<00:00,  9.43it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.42it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.40it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.39it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.40it/s]100%|██████████| 100/100 [00:11<00:00,  9.39it/s]100%|██████████| 100/100 [00:11<00:00,  8.67it/s]
Model tested on 100 tasks. Accuracy: 63.72%
0.30000001192092896 0.30000001192092896
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:30,  3.27it/s]  2%|▏         | 2/100 [00:00<00:19,  4.99it/s]  3%|▎         | 3/100 [00:00<00:16,  6.04it/s]  4%|▍         | 4/100 [00:00<00:14,  6.82it/s]  5%|▌         | 5/100 [00:00<00:12,  7.31it/s]  6%|▌         | 6/100 [00:00<00:12,  7.63it/s]  7%|▋         | 7/100 [00:01<00:11,  7.84it/s]  8%|▊         | 8/100 [00:01<00:11,  8.02it/s]  9%|▉         | 9/100 [00:01<00:11,  8.22it/s] 10%|█         | 10/100 [00:01<00:10,  8.40it/s] 11%|█         | 11/100 [00:01<00:10,  8.51it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.68it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.78it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.82it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.87it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.79it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.48it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.61it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.58it/s] 20%|██        | 20/100 [00:02<00:09,  8.72it/s] 21%|██        | 21/100 [00:02<00:08,  8.81it/s] 22%|██▏       | 22/100 [00:02<00:08,  8.86it/s] 23%|██▎       | 23/100 [00:02<00:08,  8.92it/s] 24%|██▍       | 24/100 [00:02<00:08,  8.94it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.91it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.96it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.98it/s] 28%|██▊       | 28/100 [00:03<00:07,  9.01it/s] 29%|██▉       | 29/100 [00:03<00:07,  9.03it/s] 30%|███       | 30/100 [00:03<00:07,  9.05it/s] 31%|███       | 31/100 [00:03<00:10,  6.31it/s] 32%|███▏      | 32/100 [00:04<00:09,  6.96it/s] 33%|███▎      | 33/100 [00:04<00:08,  7.50it/s] 34%|███▍      | 34/100 [00:04<00:08,  7.93it/s] 35%|███▌      | 35/100 [00:04<00:07,  8.26it/s] 36%|███▌      | 36/100 [00:04<00:07,  8.36it/s] 37%|███▋      | 37/100 [00:04<00:07,  8.59it/s] 38%|███▊      | 38/100 [00:04<00:07,  8.75it/s] 39%|███▉      | 39/100 [00:04<00:07,  8.71it/s] 40%|████      | 40/100 [00:04<00:07,  8.55it/s] 41%|████      | 41/100 [00:05<00:06,  8.69it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.80it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.88it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.59it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.72it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.79it/s] 47%|████▋     | 47/100 [00:05<00:05,  8.85it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.89it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.85it/s] 50%|█████     | 50/100 [00:06<00:05,  8.90it/s] 51%|█████     | 51/100 [00:06<00:05,  8.93it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.95it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.97it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.96it/s] 55%|█████▌    | 55/100 [00:06<00:05,  8.98it/s] 56%|█████▌    | 56/100 [00:06<00:04,  8.98it/s] 57%|█████▋    | 57/100 [00:06<00:04,  8.99it/s] 58%|█████▊    | 58/100 [00:06<00:04,  8.99it/s] 59%|█████▉    | 59/100 [00:07<00:04,  8.98it/s] 60%|██████    | 60/100 [00:07<00:04,  8.99it/s] 61%|██████    | 61/100 [00:07<00:04,  8.93it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.95it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.97it/s] 64%|██████▍   | 64/100 [00:07<00:04,  9.00it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.01it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.03it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.04it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.02it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.02it/s] 70%|███████   | 70/100 [00:08<00:03,  9.01it/s] 71%|███████   | 71/100 [00:08<00:03,  9.02it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.01it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.03it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.03it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.03it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.04it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.16it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.24it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.29it/s] 80%|████████  | 80/100 [00:09<00:02,  9.33it/s] 81%|████████  | 81/100 [00:09<00:02,  9.37it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.38it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.40it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.38it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.41it/s] 86%|████████▌ | 86/100 [00:10<00:02,  6.74it/s] 87%|████████▋ | 87/100 [00:10<00:01,  7.39it/s] 88%|████████▊ | 88/100 [00:10<00:01,  7.92it/s] 89%|████████▉ | 89/100 [00:10<00:01,  8.34it/s] 90%|█████████ | 90/100 [00:10<00:01,  8.64it/s] 91%|█████████ | 91/100 [00:10<00:01,  8.86it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.03it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.17it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.25it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.30it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.34it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.38it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.38it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.38it/s]100%|██████████| 100/100 [00:11<00:00,  9.32it/s]100%|██████████| 100/100 [00:11<00:00,  8.59it/s]
Model tested on 100 tasks. Accuracy: 40.40%
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:01<?, ?it/s, loss=7.26]  0%|          | 1/360 [00:01<06:06,  1.02s/it, loss=7.26]  1%|          | 2/360 [00:01<03:37,  1.65it/s, loss=7.26]  1%|          | 3/360 [00:01<02:38,  2.25it/s, loss=7.26]  1%|          | 4/360 [00:01<02:11,  2.72it/s, loss=7.26]  1%|▏         | 5/360 [00:02<01:54,  3.09it/s, loss=7.26]  2%|▏         | 6/360 [00:02<01:44,  3.39it/s, loss=7.26]  2%|▏         | 7/360 [00:02<01:37,  3.61it/s, loss=7.26]  2%|▏         | 8/360 [00:02<01:33,  3.78it/s, loss=7.26]  2%|▎         | 9/360 [00:03<01:29,  3.91it/s, loss=7.26]  3%|▎         | 10/360 [00:03<01:27,  3.99it/s, loss=7.26]  3%|▎         | 10/360 [00:03<01:27,  3.99it/s, loss=5.18]  3%|▎         | 11/360 [00:03<01:26,  4.03it/s, loss=5.18]  3%|▎         | 12/360 [00:03<01:25,  4.08it/s, loss=5.18]  4%|▎         | 13/360 [00:03<01:24,  4.10it/s, loss=5.18]  4%|▍         | 14/360 [00:04<01:23,  4.13it/s, loss=5.18]  4%|▍         | 15/360 [00:04<01:24,  4.08it/s, loss=5.18]  4%|▍         | 16/360 [00:04<01:23,  4.12it/s, loss=5.18]  5%|▍         | 17/360 [00:04<01:25,  4.01it/s, loss=5.18]  5%|▌         | 18/360 [00:05<01:24,  4.05it/s, loss=5.18]  5%|▌         | 19/360 [00:05<01:23,  4.09it/s, loss=5.18]  6%|▌         | 20/360 [00:05<01:22,  4.11it/s, loss=5.18]  6%|▌         | 20/360 [00:05<01:22,  4.11it/s, loss=3.84]  6%|▌         | 21/360 [00:05<01:22,  4.12it/s, loss=3.84]  6%|▌         | 22/360 [00:06<01:21,  4.16it/s, loss=3.84]  6%|▋         | 23/360 [00:06<01:20,  4.17it/s, loss=3.84]  7%|▋         | 24/360 [00:06<01:20,  4.18it/s, loss=3.84]  7%|▋         | 25/360 [00:06<01:20,  4.18it/s, loss=3.84]  7%|▋         | 26/360 [00:07<01:19,  4.19it/s, loss=3.84]  8%|▊         | 27/360 [00:07<01:19,  4.19it/s, loss=3.84]  8%|▊         | 28/360 [00:07<01:36,  3.44it/s, loss=3.84]  8%|▊         | 29/360 [00:08<01:30,  3.66it/s, loss=3.84]  8%|▊         | 30/360 [00:08<01:28,  3.75it/s, loss=3.84]  8%|▊         | 30/360 [00:08<01:28,  3.75it/s, loss=3.44]  9%|▊         | 31/360 [00:08<01:24,  3.88it/s, loss=3.44]  9%|▉         | 32/360 [00:08<01:22,  3.97it/s, loss=3.44]  9%|▉         | 33/360 [00:08<01:21,  4.04it/s, loss=3.44]  9%|▉         | 34/360 [00:09<01:20,  4.07it/s, loss=3.44] 10%|▉         | 35/360 [00:09<01:18,  4.11it/s, loss=3.44] 10%|█         | 36/360 [00:09<01:18,  4.14it/s, loss=3.44] 10%|█         | 37/360 [00:09<01:17,  4.16it/s, loss=3.44] 11%|█         | 38/360 [00:10<01:17,  4.17it/s, loss=3.44] 11%|█         | 39/360 [00:10<01:16,  4.18it/s, loss=3.44] 11%|█         | 40/360 [00:10<01:16,  4.19it/s, loss=3.44] 11%|█         | 40/360 [00:10<01:16,  4.19it/s, loss=3.13] 11%|█▏        | 41/360 [00:10<01:16,  4.19it/s, loss=3.13] 12%|█▏        | 42/360 [00:11<01:15,  4.20it/s, loss=3.13] 12%|█▏        | 43/360 [00:11<01:15,  4.17it/s, loss=3.13] 12%|█▏        | 44/360 [00:11<01:15,  4.18it/s, loss=3.13] 12%|█▎        | 45/360 [00:11<01:15,  4.19it/s, loss=3.13] 13%|█▎        | 46/360 [00:12<01:15,  4.15it/s, loss=3.13] 13%|█▎        | 47/360 [00:12<01:15,  4.17it/s, loss=3.13] 13%|█▎        | 48/360 [00:12<01:14,  4.17it/s, loss=3.13] 14%|█▎        | 49/360 [00:12<01:14,  4.17it/s, loss=3.13] 14%|█▍        | 50/360 [00:13<01:14,  4.18it/s, loss=3.13] 14%|█▍        | 50/360 [00:13<01:14,  4.18it/s, loss=2.83] 14%|█▍        | 51/360 [00:13<01:14,  4.17it/s, loss=2.83] 14%|█▍        | 52/360 [00:13<01:13,  4.17it/s, loss=2.83] 15%|█▍        | 53/360 [00:13<01:13,  4.19it/s, loss=2.83] 15%|█▌        | 54/360 [00:14<01:13,  4.16it/s, loss=2.83] 15%|█▌        | 55/360 [00:14<01:13,  4.17it/s, loss=2.83] 16%|█▌        | 56/360 [00:14<01:12,  4.19it/s, loss=2.83] 16%|█▌        | 57/360 [00:14<01:12,  4.20it/s, loss=2.83] 16%|█▌        | 58/360 [00:14<01:11,  4.21it/s, loss=2.83] 16%|█▋        | 59/360 [00:15<01:11,  4.21it/s, loss=2.83] 17%|█▋        | 60/360 [00:15<01:11,  4.21it/s, loss=2.83] 17%|█▋        | 60/360 [00:15<01:11,  4.21it/s, loss=2.65] 17%|█▋        | 61/360 [00:15<01:11,  4.18it/s, loss=2.65] 17%|█▋        | 62/360 [00:15<01:10,  4.20it/s, loss=2.65] 18%|█▊        | 63/360 [00:16<01:10,  4.20it/s, loss=2.65] 18%|█▊        | 64/360 [00:16<01:10,  4.21it/s, loss=2.65] 18%|█▊        | 65/360 [00:16<01:09,  4.22it/s, loss=2.65] 18%|█▊        | 66/360 [00:16<01:09,  4.22it/s, loss=2.65] 19%|█▊        | 67/360 [00:17<01:09,  4.22it/s, loss=2.65] 19%|█▉        | 68/360 [00:17<01:09,  4.22it/s, loss=2.65] 19%|█▉        | 69/360 [00:17<01:08,  4.23it/s, loss=2.65] 19%|█▉        | 70/360 [00:17<01:08,  4.23it/s, loss=2.65] 19%|█▉        | 70/360 [00:18<01:08,  4.23it/s, loss=2.48] 20%|█▉        | 71/360 [00:18<01:08,  4.23it/s, loss=2.48] 20%|██        | 72/360 [00:18<01:08,  4.21it/s, loss=2.48] 20%|██        | 73/360 [00:18<01:08,  4.22it/s, loss=2.48] 21%|██        | 74/360 [00:18<01:21,  3.52it/s, loss=2.48] 21%|██        | 75/360 [00:19<01:16,  3.72it/s, loss=2.48] 21%|██        | 76/360 [00:19<01:13,  3.86it/s, loss=2.48] 21%|██▏       | 77/360 [00:19<01:11,  3.96it/s, loss=2.48] 22%|██▏       | 78/360 [00:19<01:09,  4.03it/s, loss=2.48] 22%|██▏       | 79/360 [00:20<01:08,  4.08it/s, loss=2.48] 22%|██▏       | 80/360 [00:20<01:08,  4.11it/s, loss=2.48] 22%|██▏       | 80/360 [00:20<01:08,  4.11it/s, loss=2.37] 22%|██▎       | 81/360 [00:20<01:07,  4.13it/s, loss=2.37] 23%|██▎       | 82/360 [00:20<01:07,  4.13it/s, loss=2.37] 23%|██▎       | 83/360 [00:21<01:06,  4.15it/s, loss=2.37] 23%|██▎       | 84/360 [00:21<01:06,  4.16it/s, loss=2.37] 24%|██▎       | 85/360 [00:21<01:05,  4.17it/s, loss=2.37] 24%|██▍       | 86/360 [00:21<01:05,  4.18it/s, loss=2.37] 24%|██▍       | 87/360 [00:22<01:05,  4.19it/s, loss=2.37] 24%|██▍       | 88/360 [00:22<01:04,  4.19it/s, loss=2.37] 25%|██▍       | 89/360 [00:22<01:04,  4.20it/s, loss=2.37] 25%|██▌       | 90/360 [00:22<01:04,  4.20it/s, loss=2.37] 25%|██▌       | 90/360 [00:22<01:04,  4.20it/s, loss=2.26] 25%|██▌       | 91/360 [00:22<01:03,  4.21it/s, loss=2.26] 26%|██▌       | 92/360 [00:23<01:03,  4.21it/s, loss=2.26] 26%|██▌       | 93/360 [00:23<01:03,  4.21it/s, loss=2.26] 26%|██▌       | 94/360 [00:23<01:04,  4.14it/s, loss=2.26] 26%|██▋       | 95/360 [00:23<01:03,  4.14it/s, loss=2.26] 27%|██▋       | 96/360 [00:24<01:03,  4.17it/s, loss=2.26] 27%|██▋       | 97/360 [00:24<01:02,  4.18it/s, loss=2.26] 27%|██▋       | 98/360 [00:24<01:02,  4.19it/s, loss=2.26] 28%|██▊       | 99/360 [00:24<01:02,  4.20it/s, loss=2.26] 28%|██▊       | 100/360 [00:25<01:01,  4.21it/s, loss=2.26] 28%|██▊       | 100/360 [00:25<01:01,  4.21it/s, loss=2.24] 28%|██▊       | 101/360 [00:25<01:01,  4.21it/s, loss=2.24] 28%|██▊       | 102/360 [00:25<01:01,  4.21it/s, loss=2.24] 29%|██▊       | 103/360 [00:25<01:00,  4.21it/s, loss=2.24] 29%|██▉       | 104/360 [00:26<01:00,  4.22it/s, loss=2.24] 29%|██▉       | 105/360 [00:26<01:00,  4.22it/s, loss=2.24] 29%|██▉       | 106/360 [00:26<01:00,  4.23it/s, loss=2.24] 30%|██▉       | 107/360 [00:26<00:59,  4.23it/s, loss=2.24] 30%|███       | 108/360 [00:27<01:00,  4.20it/s, loss=2.24] 30%|███       | 109/360 [00:27<00:59,  4.20it/s, loss=2.24] 31%|███       | 110/360 [00:27<00:59,  4.21it/s, loss=2.24] 31%|███       | 110/360 [00:27<00:59,  4.21it/s, loss=2.09] 31%|███       | 111/360 [00:27<00:59,  4.21it/s, loss=2.09] 31%|███       | 112/360 [00:27<00:58,  4.22it/s, loss=2.09] 31%|███▏      | 113/360 [00:28<00:58,  4.22it/s, loss=2.09] 32%|███▏      | 114/360 [00:28<00:58,  4.22it/s, loss=2.09] 32%|███▏      | 115/360 [00:28<00:58,  4.21it/s, loss=2.09] 32%|███▏      | 116/360 [00:29<01:10,  3.48it/s, loss=2.09] 32%|███▎      | 117/360 [00:29<01:05,  3.70it/s, loss=2.09] 33%|███▎      | 118/360 [00:29<01:02,  3.84it/s, loss=2.09] 33%|███▎      | 119/360 [00:29<01:00,  3.95it/s, loss=2.09] 33%|███▎      | 120/360 [00:30<00:59,  4.04it/s, loss=2.09] 33%|███▎      | 120/360 [00:30<00:59,  4.04it/s, loss=1.99] 34%|███▎      | 121/360 [00:30<00:58,  4.10it/s, loss=1.99] 34%|███▍      | 122/360 [00:30<00:57,  4.14it/s, loss=1.99] 34%|███▍      | 123/360 [00:30<00:56,  4.16it/s, loss=1.99] 34%|███▍      | 124/360 [00:30<00:56,  4.18it/s, loss=1.99] 35%|███▍      | 125/360 [00:31<00:56,  4.19it/s, loss=1.99] 35%|███▌      | 126/360 [00:31<00:55,  4.20it/s, loss=1.99] 35%|███▌      | 127/360 [00:31<00:55,  4.20it/s, loss=1.99] 36%|███▌      | 128/360 [00:31<00:55,  4.21it/s, loss=1.99] 36%|███▌      | 129/360 [00:32<00:54,  4.21it/s, loss=1.99] 36%|███▌      | 130/360 [00:32<00:54,  4.22it/s, loss=1.99] 36%|███▌      | 130/360 [00:32<00:54,  4.22it/s, loss=1.98] 36%|███▋      | 131/360 [00:32<00:54,  4.18it/s, loss=1.98] 37%|███▋      | 132/360 [00:32<00:54,  4.18it/s, loss=1.98] 37%|███▋      | 133/360 [00:33<00:54,  4.18it/s, loss=1.98] 37%|███▋      | 134/360 [00:33<00:54,  4.18it/s, loss=1.98] 38%|███▊      | 135/360 [00:33<00:53,  4.18it/s, loss=1.98] 38%|███▊      | 136/360 [00:33<00:54,  4.11it/s, loss=1.98] 38%|███▊      | 137/360 [00:34<00:53,  4.15it/s, loss=1.98] 38%|███▊      | 138/360 [00:34<00:53,  4.17it/s, loss=1.98] 39%|███▊      | 139/360 [00:34<00:52,  4.18it/s, loss=1.98] 39%|███▉      | 140/360 [00:34<00:52,  4.19it/s, loss=1.98] 39%|███▉      | 140/360 [00:35<00:52,  4.19it/s, loss=1.95] 39%|███▉      | 141/360 [00:35<00:52,  4.18it/s, loss=1.95] 39%|███▉      | 142/360 [00:35<00:52,  4.19it/s, loss=1.95] 40%|███▉      | 143/360 [00:35<00:51,  4.20it/s, loss=1.95] 40%|████      | 144/360 [00:35<00:51,  4.16it/s, loss=1.95] 40%|████      | 145/360 [00:35<00:51,  4.19it/s, loss=1.95] 41%|████      | 146/360 [00:36<00:51,  4.19it/s, loss=1.95] 41%|████      | 147/360 [00:36<00:50,  4.20it/s, loss=1.95] 41%|████      | 148/360 [00:36<00:50,  4.20it/s, loss=1.95] 41%|████▏     | 149/360 [00:36<00:50,  4.20it/s, loss=1.95] 42%|████▏     | 150/360 [00:37<00:50,  4.20it/s, loss=1.95] 42%|████▏     | 150/360 [00:37<00:50,  4.20it/s, loss=1.84] 42%|████▏     | 151/360 [00:37<00:49,  4.21it/s, loss=1.84] 42%|████▏     | 152/360 [00:37<00:49,  4.22it/s, loss=1.84] 42%|████▎     | 153/360 [00:37<00:49,  4.22it/s, loss=1.84] 43%|████▎     | 154/360 [00:38<00:48,  4.21it/s, loss=1.84] 43%|████▎     | 155/360 [00:38<00:48,  4.20it/s, loss=1.84] 43%|████▎     | 156/360 [00:38<00:48,  4.20it/s, loss=1.84] 44%|████▎     | 157/360 [00:38<00:48,  4.20it/s, loss=1.84] 44%|████▍     | 158/360 [00:39<00:48,  4.19it/s, loss=1.84] 44%|████▍     | 159/360 [00:39<00:47,  4.21it/s, loss=1.84] 44%|████▍     | 160/360 [00:39<00:47,  4.22it/s, loss=1.84] 44%|████▍     | 160/360 [00:39<00:47,  4.22it/s, loss=1.83] 45%|████▍     | 161/360 [00:39<00:47,  4.21it/s, loss=1.83] 45%|████▌     | 162/360 [00:40<00:46,  4.22it/s, loss=1.83] 45%|████▌     | 163/360 [00:40<00:46,  4.22it/s, loss=1.83] 46%|████▌     | 164/360 [00:40<00:56,  3.50it/s, loss=1.83] 46%|████▌     | 165/360 [00:40<00:52,  3.71it/s, loss=1.83] 46%|████▌     | 166/360 [00:41<00:50,  3.85it/s, loss=1.83] 46%|████▋     | 167/360 [00:41<00:48,  3.96it/s, loss=1.83] 47%|████▋     | 168/360 [00:41<00:47,  4.02it/s, loss=1.83] 47%|████▋     | 169/360 [00:41<00:46,  4.08it/s, loss=1.83] 47%|████▋     | 170/360 [00:42<00:46,  4.13it/s, loss=1.83] 47%|████▋     | 170/360 [00:42<00:46,  4.13it/s, loss=1.82] 48%|████▊     | 171/360 [00:42<00:45,  4.13it/s, loss=1.82] 48%|████▊     | 172/360 [00:42<00:45,  4.14it/s, loss=1.82] 48%|████▊     | 173/360 [00:42<00:45,  4.14it/s, loss=1.82] 48%|████▊     | 174/360 [00:43<00:44,  4.17it/s, loss=1.82] 49%|████▊     | 175/360 [00:43<00:44,  4.18it/s, loss=1.82] 49%|████▉     | 176/360 [00:43<00:43,  4.19it/s, loss=1.82] 49%|████▉     | 177/360 [00:43<00:43,  4.20it/s, loss=1.82] 49%|████▉     | 178/360 [00:43<00:43,  4.20it/s, loss=1.82] 50%|████▉     | 179/360 [00:44<00:42,  4.21it/s, loss=1.82] 50%|█████     | 180/360 [00:44<00:43,  4.15it/s, loss=1.82] 50%|█████     | 180/360 [00:44<00:43,  4.15it/s, loss=1.73] 50%|█████     | 181/360 [00:44<00:42,  4.17it/s, loss=1.73] 51%|█████     | 182/360 [00:44<00:42,  4.18it/s, loss=1.73] 51%|█████     | 183/360 [00:45<00:42,  4.20it/s, loss=1.73] 51%|█████     | 184/360 [00:45<00:41,  4.20it/s, loss=1.73] 51%|█████▏    | 185/360 [00:45<00:41,  4.20it/s, loss=1.73] 52%|█████▏    | 186/360 [00:45<00:41,  4.22it/s, loss=1.73] 52%|█████▏    | 187/360 [00:46<00:40,  4.22it/s, loss=1.73] 52%|█████▏    | 188/360 [00:46<00:40,  4.22it/s, loss=1.73] 52%|█████▎    | 189/360 [00:46<00:40,  4.22it/s, loss=1.73] 53%|█████▎    | 190/360 [00:46<00:40,  4.22it/s, loss=1.73] 53%|█████▎    | 190/360 [00:47<00:40,  4.22it/s, loss=1.83] 53%|█████▎    | 191/360 [00:47<00:40,  4.22it/s, loss=1.83] 53%|█████▎    | 192/360 [00:47<00:39,  4.21it/s, loss=1.83] 54%|█████▎    | 193/360 [00:47<00:40,  4.17it/s, loss=1.83] 54%|█████▍    | 194/360 [00:47<00:39,  4.20it/s, loss=1.83] 54%|█████▍    | 195/360 [00:48<00:39,  4.21it/s, loss=1.83] 54%|█████▍    | 196/360 [00:48<00:38,  4.22it/s, loss=1.83] 55%|█████▍    | 197/360 [00:48<00:38,  4.23it/s, loss=1.83] 55%|█████▌    | 198/360 [00:48<00:38,  4.23it/s, loss=1.83] 55%|█████▌    | 199/360 [00:48<00:37,  4.24it/s, loss=1.83] 56%|█████▌    | 200/360 [00:49<00:37,  4.24it/s, loss=1.83] 56%|█████▌    | 200/360 [00:49<00:37,  4.24it/s, loss=1.74] 56%|█████▌    | 201/360 [00:49<00:37,  4.23it/s, loss=1.74] 56%|█████▌    | 202/360 [00:49<00:37,  4.23it/s, loss=1.74] 56%|█████▋    | 203/360 [00:49<00:37,  4.23it/s, loss=1.74] 57%|█████▋    | 204/360 [00:50<00:36,  4.23it/s, loss=1.74] 57%|█████▋    | 205/360 [00:50<00:36,  4.23it/s, loss=1.74] 57%|█████▋    | 206/360 [00:50<00:36,  4.22it/s, loss=1.74] 57%|█████▊    | 207/360 [00:50<00:36,  4.23it/s, loss=1.74] 58%|█████▊    | 208/360 [00:51<00:35,  4.24it/s, loss=1.74] 58%|█████▊    | 209/360 [00:51<00:35,  4.24it/s, loss=1.74] 58%|█████▊    | 210/360 [00:51<00:35,  4.24it/s, loss=1.74] 58%|█████▊    | 210/360 [00:51<00:35,  4.24it/s, loss=1.7]  59%|█████▊    | 211/360 [00:51<00:35,  4.24it/s, loss=1.7] 59%|█████▉    | 212/360 [00:52<00:34,  4.25it/s, loss=1.7] 59%|█████▉    | 213/360 [00:52<00:34,  4.25it/s, loss=1.7] 59%|█████▉    | 214/360 [00:52<00:34,  4.25it/s, loss=1.7] 60%|█████▉    | 215/360 [00:52<00:34,  4.25it/s, loss=1.7] 60%|██████    | 216/360 [00:52<00:33,  4.25it/s, loss=1.7] 60%|██████    | 217/360 [00:53<00:33,  4.25it/s, loss=1.7] 61%|██████    | 218/360 [00:53<00:33,  4.26it/s, loss=1.7] 61%|██████    | 219/360 [00:53<00:33,  4.26it/s, loss=1.7] 61%|██████    | 220/360 [00:53<00:32,  4.26it/s, loss=1.7] 61%|██████    | 220/360 [00:54<00:32,  4.26it/s, loss=1.54] 61%|██████▏   | 221/360 [00:54<00:32,  4.26it/s, loss=1.54] 62%|██████▏   | 222/360 [00:54<00:32,  4.26it/s, loss=1.54] 62%|██████▏   | 223/360 [00:54<00:38,  3.54it/s, loss=1.54] 62%|██████▏   | 224/360 [00:55<00:36,  3.74it/s, loss=1.54] 62%|██████▎   | 225/360 [00:55<00:34,  3.86it/s, loss=1.54] 63%|██████▎   | 226/360 [00:55<00:33,  3.96it/s, loss=1.54] 63%|██████▎   | 227/360 [00:55<00:32,  4.04it/s, loss=1.54] 63%|██████▎   | 228/360 [00:55<00:32,  4.10it/s, loss=1.54] 64%|██████▎   | 229/360 [00:56<00:31,  4.12it/s, loss=1.54] 64%|██████▍   | 230/360 [00:56<00:31,  4.15it/s, loss=1.54] 64%|██████▍   | 230/360 [00:56<00:31,  4.15it/s, loss=1.66] 64%|██████▍   | 231/360 [00:56<00:30,  4.17it/s, loss=1.66] 64%|██████▍   | 232/360 [00:56<00:30,  4.19it/s, loss=1.66] 65%|██████▍   | 233/360 [00:57<00:30,  4.19it/s, loss=1.66] 65%|██████▌   | 234/360 [00:57<00:30,  4.12it/s, loss=1.66] 65%|██████▌   | 235/360 [00:57<00:30,  4.14it/s, loss=1.66] 66%|██████▌   | 236/360 [00:57<00:30,  4.04it/s, loss=1.66] 66%|██████▌   | 237/360 [00:58<00:30,  4.09it/s, loss=1.66] 66%|██████▌   | 238/360 [00:58<00:29,  4.13it/s, loss=1.66] 66%|██████▋   | 239/360 [00:58<00:29,  4.15it/s, loss=1.66] 67%|██████▋   | 240/360 [00:58<00:28,  4.17it/s, loss=1.66] 67%|██████▋   | 240/360 [00:59<00:28,  4.17it/s, loss=1.56] 67%|██████▋   | 241/360 [00:59<00:28,  4.17it/s, loss=1.56] 67%|██████▋   | 242/360 [00:59<00:28,  4.17it/s, loss=1.56] 68%|██████▊   | 243/360 [00:59<00:28,  4.10it/s, loss=1.56] 68%|██████▊   | 244/360 [00:59<00:28,  4.13it/s, loss=1.56] 68%|██████▊   | 245/360 [01:00<00:27,  4.13it/s, loss=1.56] 68%|██████▊   | 246/360 [01:00<00:27,  4.14it/s, loss=1.56] 69%|██████▊   | 247/360 [01:00<00:27,  4.15it/s, loss=1.56] 69%|██████▉   | 248/360 [01:00<00:26,  4.16it/s, loss=1.56] 69%|██████▉   | 249/360 [01:01<00:26,  4.17it/s, loss=1.56] 69%|██████▉   | 250/360 [01:01<00:26,  4.18it/s, loss=1.56] 69%|██████▉   | 250/360 [01:01<00:26,  4.18it/s, loss=1.65] 70%|██████▉   | 251/360 [01:01<00:26,  4.19it/s, loss=1.65] 70%|███████   | 252/360 [01:01<00:25,  4.19it/s, loss=1.65] 70%|███████   | 253/360 [01:01<00:25,  4.19it/s, loss=1.65] 71%|███████   | 254/360 [01:02<00:25,  4.20it/s, loss=1.65] 71%|███████   | 255/360 [01:02<00:25,  4.20it/s, loss=1.65] 71%|███████   | 256/360 [01:02<00:24,  4.20it/s, loss=1.65] 71%|███████▏  | 257/360 [01:02<00:24,  4.20it/s, loss=1.65] 72%|███████▏  | 258/360 [01:03<00:24,  4.21it/s, loss=1.65] 72%|███████▏  | 259/360 [01:03<00:24,  4.21it/s, loss=1.65] 72%|███████▏  | 260/360 [01:03<00:23,  4.20it/s, loss=1.65] 72%|███████▏  | 260/360 [01:04<00:23,  4.20it/s, loss=1.63] 72%|███████▎  | 261/360 [01:04<00:28,  3.48it/s, loss=1.63] 73%|███████▎  | 262/360 [01:04<00:26,  3.69it/s, loss=1.63] 73%|███████▎  | 263/360 [01:04<00:25,  3.84it/s, loss=1.63] 73%|███████▎  | 264/360 [01:04<00:24,  3.94it/s, loss=1.63] 74%|███████▎  | 265/360 [01:04<00:23,  4.02it/s, loss=1.63] 74%|███████▍  | 266/360 [01:05<00:23,  4.08it/s, loss=1.63] 74%|███████▍  | 267/360 [01:05<00:22,  4.12it/s, loss=1.63] 74%|███████▍  | 268/360 [01:05<00:22,  4.14it/s, loss=1.63] 75%|███████▍  | 269/360 [01:05<00:21,  4.16it/s, loss=1.63] 75%|███████▌  | 270/360 [01:06<00:21,  4.18it/s, loss=1.63] 75%|███████▌  | 270/360 [01:06<00:21,  4.18it/s, loss=1.48] 75%|███████▌  | 271/360 [01:06<00:21,  4.19it/s, loss=1.48] 76%|███████▌  | 272/360 [01:06<00:20,  4.19it/s, loss=1.48] 76%|███████▌  | 273/360 [01:06<00:20,  4.20it/s, loss=1.48] 76%|███████▌  | 274/360 [01:07<00:20,  4.20it/s, loss=1.48] 76%|███████▋  | 275/360 [01:07<00:20,  4.21it/s, loss=1.48] 77%|███████▋  | 276/360 [01:07<00:19,  4.21it/s, loss=1.48] 77%|███████▋  | 277/360 [01:07<00:19,  4.21it/s, loss=1.48] 77%|███████▋  | 278/360 [01:08<00:19,  4.22it/s, loss=1.48] 78%|███████▊  | 279/360 [01:08<00:19,  4.22it/s, loss=1.48] 78%|███████▊  | 280/360 [01:08<00:18,  4.21it/s, loss=1.48] 78%|███████▊  | 280/360 [01:08<00:18,  4.21it/s, loss=1.49] 78%|███████▊  | 281/360 [01:08<00:18,  4.21it/s, loss=1.49] 78%|███████▊  | 282/360 [01:09<00:18,  4.21it/s, loss=1.49] 79%|███████▊  | 283/360 [01:09<00:18,  4.21it/s, loss=1.49] 79%|███████▉  | 284/360 [01:09<00:18,  4.16it/s, loss=1.49] 79%|███████▉  | 285/360 [01:09<00:17,  4.18it/s, loss=1.49] 79%|███████▉  | 286/360 [01:09<00:17,  4.19it/s, loss=1.49] 80%|███████▉  | 287/360 [01:10<00:17,  4.18it/s, loss=1.49] 80%|████████  | 288/360 [01:10<00:17,  4.14it/s, loss=1.49] 80%|████████  | 289/360 [01:10<00:17,  4.15it/s, loss=1.49] 81%|████████  | 290/360 [01:10<00:16,  4.17it/s, loss=1.49] 81%|████████  | 290/360 [01:11<00:16,  4.17it/s, loss=1.35] 81%|████████  | 291/360 [01:11<00:16,  4.18it/s, loss=1.35] 81%|████████  | 292/360 [01:11<00:16,  4.19it/s, loss=1.35] 81%|████████▏ | 293/360 [01:11<00:15,  4.20it/s, loss=1.35] 82%|████████▏ | 294/360 [01:11<00:15,  4.18it/s, loss=1.35] 82%|████████▏ | 295/360 [01:12<00:15,  4.17it/s, loss=1.35] 82%|████████▏ | 296/360 [01:12<00:15,  4.18it/s, loss=1.35] 82%|████████▎ | 297/360 [01:12<00:15,  4.19it/s, loss=1.35] 83%|████████▎ | 298/360 [01:12<00:14,  4.17it/s, loss=1.35] 83%|████████▎ | 299/360 [01:13<00:17,  3.46it/s, loss=1.35] 83%|████████▎ | 300/360 [01:13<00:16,  3.68it/s, loss=1.35] 83%|████████▎ | 300/360 [01:13<00:16,  3.68it/s, loss=1.37] 84%|████████▎ | 301/360 [01:13<00:15,  3.83it/s, loss=1.37] 84%|████████▍ | 302/360 [01:13<00:14,  3.94it/s, loss=1.37] 84%|████████▍ | 303/360 [01:14<00:14,  4.03it/s, loss=1.37] 84%|████████▍ | 304/360 [01:14<00:13,  4.09it/s, loss=1.37] 85%|████████▍ | 305/360 [01:14<00:13,  4.13it/s, loss=1.37] 85%|████████▌ | 306/360 [01:14<00:13,  4.15it/s, loss=1.37] 85%|████████▌ | 307/360 [01:15<00:12,  4.17it/s, loss=1.37] 86%|████████▌ | 308/360 [01:15<00:12,  4.18it/s, loss=1.37] 86%|████████▌ | 309/360 [01:15<00:12,  4.19it/s, loss=1.37] 86%|████████▌ | 310/360 [01:15<00:11,  4.18it/s, loss=1.37] 86%|████████▌ | 310/360 [01:16<00:11,  4.18it/s, loss=1.5]  86%|████████▋ | 311/360 [01:16<00:11,  4.17it/s, loss=1.5] 87%|████████▋ | 312/360 [01:16<00:11,  4.17it/s, loss=1.5] 87%|████████▋ | 313/360 [01:16<00:11,  4.17it/s, loss=1.5] 87%|████████▋ | 314/360 [01:16<00:11,  4.17it/s, loss=1.5] 88%|████████▊ | 315/360 [01:17<00:10,  4.15it/s, loss=1.5] 88%|████████▊ | 316/360 [01:17<00:10,  4.11it/s, loss=1.5] 88%|████████▊ | 317/360 [01:17<00:10,  4.13it/s, loss=1.5] 88%|████████▊ | 318/360 [01:17<00:10,  4.14it/s, loss=1.5] 89%|████████▊ | 319/360 [01:18<00:09,  4.16it/s, loss=1.5] 89%|████████▉ | 320/360 [01:18<00:09,  4.17it/s, loss=1.5] 89%|████████▉ | 320/360 [01:18<00:09,  4.17it/s, loss=1.43] 89%|████████▉ | 321/360 [01:18<00:09,  4.18it/s, loss=1.43] 89%|████████▉ | 322/360 [01:18<00:09,  4.18it/s, loss=1.43] 90%|████████▉ | 323/360 [01:18<00:08,  4.19it/s, loss=1.43] 90%|█████████ | 324/360 [01:19<00:08,  4.20it/s, loss=1.43] 90%|█████████ | 325/360 [01:19<00:08,  4.20it/s, loss=1.43] 91%|█████████ | 326/360 [01:19<00:08,  4.20it/s, loss=1.43] 91%|█████████ | 327/360 [01:19<00:07,  4.20it/s, loss=1.43] 91%|█████████ | 328/360 [01:20<00:07,  4.20it/s, loss=1.43] 91%|█████████▏| 329/360 [01:20<00:07,  4.20it/s, loss=1.43] 92%|█████████▏| 330/360 [01:20<00:07,  4.19it/s, loss=1.43] 92%|█████████▏| 330/360 [01:20<00:07,  4.19it/s, loss=1.41] 92%|█████████▏| 331/360 [01:20<00:06,  4.19it/s, loss=1.41] 92%|█████████▏| 332/360 [01:21<00:06,  4.20it/s, loss=1.41] 92%|█████████▎| 333/360 [01:21<00:06,  4.20it/s, loss=1.41] 93%|█████████▎| 334/360 [01:21<00:06,  4.21it/s, loss=1.41] 93%|█████████▎| 335/360 [01:21<00:05,  4.21it/s, loss=1.41] 93%|█████████▎| 336/360 [01:22<00:05,  4.21it/s, loss=1.41] 94%|█████████▎| 337/360 [01:22<00:06,  3.54it/s, loss=1.41] 94%|█████████▍| 338/360 [01:22<00:05,  3.76it/s, loss=1.41] 94%|█████████▍| 339/360 [01:22<00:05,  3.93it/s, loss=1.41] 94%|█████████▍| 340/360 [01:23<00:04,  4.05it/s, loss=1.41] 94%|█████████▍| 340/360 [01:23<00:04,  4.05it/s, loss=1.26] 95%|█████████▍| 341/360 [01:23<00:04,  4.14it/s, loss=1.26] 95%|█████████▌| 342/360 [01:23<00:04,  4.20it/s, loss=1.26] 95%|█████████▌| 343/360 [01:23<00:04,  4.25it/s, loss=1.26] 96%|█████████▌| 344/360 [01:24<00:03,  4.28it/s, loss=1.26] 96%|█████████▌| 345/360 [01:24<00:03,  4.30it/s, loss=1.26] 96%|█████████▌| 346/360 [01:24<00:03,  4.31it/s, loss=1.26] 96%|█████████▋| 347/360 [01:24<00:03,  4.32it/s, loss=1.26] 97%|█████████▋| 348/360 [01:24<00:02,  4.33it/s, loss=1.26] 97%|█████████▋| 349/360 [01:25<00:02,  4.33it/s, loss=1.26] 97%|█████████▋| 350/360 [01:25<00:02,  4.34it/s, loss=1.26] 97%|█████████▋| 350/360 [01:25<00:02,  4.34it/s, loss=1.3]  98%|█████████▊| 351/360 [01:25<00:02,  4.34it/s, loss=1.3] 98%|█████████▊| 352/360 [01:25<00:01,  4.34it/s, loss=1.3] 98%|█████████▊| 353/360 [01:26<00:01,  4.34it/s, loss=1.3] 98%|█████████▊| 354/360 [01:26<00:01,  4.35it/s, loss=1.3] 99%|█████████▊| 355/360 [01:26<00:01,  4.35it/s, loss=1.3] 99%|█████████▉| 356/360 [01:26<00:00,  4.35it/s, loss=1.3] 99%|█████████▉| 357/360 [01:27<00:00,  4.35it/s, loss=1.3] 99%|█████████▉| 358/360 [01:27<00:00,  4.35it/s, loss=1.3]100%|█████████▉| 359/360 [01:27<00:00,  4.35it/s, loss=1.3]100%|██████████| 360/360 [01:27<00:00,  4.35it/s, loss=1.3]100%|██████████| 360/360 [01:27<00:00,  4.10it/s, loss=1.3]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:27,  3.62it/s]  2%|▏         | 2/100 [00:00<00:17,  5.46it/s]  3%|▎         | 3/100 [00:00<00:14,  6.52it/s]  4%|▍         | 4/100 [00:00<00:13,  7.26it/s]  5%|▌         | 5/100 [00:00<00:12,  7.79it/s]  6%|▌         | 6/100 [00:00<00:11,  8.15it/s]  7%|▋         | 7/100 [00:00<00:11,  8.32it/s]  8%|▊         | 8/100 [00:01<00:10,  8.51it/s]  9%|▉         | 9/100 [00:01<00:10,  8.57it/s] 10%|█         | 10/100 [00:01<00:10,  8.39it/s] 11%|█         | 11/100 [00:01<00:10,  8.55it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.65it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.77it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.83it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.88it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.48it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.65it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.77it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.83it/s] 20%|██        | 20/100 [00:02<00:09,  8.87it/s] 21%|██        | 21/100 [00:02<00:08,  8.84it/s] 22%|██▏       | 22/100 [00:02<00:08,  8.84it/s] 23%|██▎       | 23/100 [00:02<00:13,  5.82it/s] 24%|██▍       | 24/100 [00:03<00:11,  6.54it/s] 25%|██▌       | 25/100 [00:03<00:10,  7.15it/s] 26%|██▌       | 26/100 [00:03<00:09,  7.66it/s] 27%|██▋       | 27/100 [00:03<00:09,  8.04it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.34it/s] 29%|██▉       | 29/100 [00:03<00:08,  8.44it/s] 30%|███       | 30/100 [00:03<00:08,  8.55it/s] 31%|███       | 31/100 [00:03<00:07,  8.69it/s] 32%|███▏      | 32/100 [00:03<00:07,  8.82it/s] 33%|███▎      | 33/100 [00:04<00:07,  8.82it/s] 34%|███▍      | 34/100 [00:04<00:07,  8.88it/s] 35%|███▌      | 35/100 [00:04<00:07,  8.93it/s] 36%|███▌      | 36/100 [00:04<00:07,  8.95it/s] 37%|███▋      | 37/100 [00:04<00:07,  8.96it/s] 38%|███▊      | 38/100 [00:04<00:06,  8.99it/s] 39%|███▉      | 39/100 [00:04<00:06,  8.98it/s] 40%|████      | 40/100 [00:04<00:06,  8.99it/s] 41%|████      | 41/100 [00:04<00:06,  8.84it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.85it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.80it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.81it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.71it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.82it/s] 47%|████▋     | 47/100 [00:05<00:05,  8.88it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.92it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.96it/s] 50%|█████     | 50/100 [00:05<00:05,  8.97it/s] 51%|█████     | 51/100 [00:06<00:05,  8.96it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.97it/s] 53%|█████▎    | 53/100 [00:06<00:05,  8.99it/s] 54%|█████▍    | 54/100 [00:06<00:05,  8.96it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.00it/s] 56%|█████▌    | 56/100 [00:06<00:04,  9.02it/s] 57%|█████▋    | 57/100 [00:06<00:04,  9.05it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.07it/s] 59%|█████▉    | 59/100 [00:06<00:04,  9.08it/s] 60%|██████    | 60/100 [00:07<00:04,  9.07it/s] 61%|██████    | 61/100 [00:07<00:04,  8.93it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.00it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.76it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.81it/s] 65%|██████▌   | 65/100 [00:07<00:03,  8.89it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.95it/s] 67%|██████▋   | 67/100 [00:07<00:03,  8.97it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.01it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.02it/s] 70%|███████   | 70/100 [00:08<00:03,  9.00it/s] 71%|███████   | 71/100 [00:08<00:03,  9.03it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.04it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.05it/s] 74%|███████▍  | 74/100 [00:08<00:02,  8.69it/s] 75%|███████▌  | 75/100 [00:08<00:02,  8.72it/s] 76%|███████▌  | 76/100 [00:08<00:02,  8.83it/s] 77%|███████▋  | 77/100 [00:09<00:02,  8.56it/s] 78%|███████▊  | 78/100 [00:09<00:02,  8.82it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.00it/s] 80%|████████  | 80/100 [00:09<00:02,  9.12it/s] 81%|████████  | 81/100 [00:09<00:02,  9.19it/s] 82%|████████▏ | 82/100 [00:09<00:02,  6.48it/s] 83%|████████▎ | 83/100 [00:09<00:02,  7.18it/s] 84%|████████▍ | 84/100 [00:09<00:02,  7.76it/s] 85%|████████▌ | 85/100 [00:10<00:01,  8.22it/s] 86%|████████▌ | 86/100 [00:10<00:01,  8.58it/s] 87%|████████▋ | 87/100 [00:10<00:01,  8.84it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.02it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.15it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.25it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.30it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.36it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.39it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.41it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.41it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.41it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.40it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.42it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.41it/s]100%|██████████| 100/100 [00:11<00:00,  9.39it/s]100%|██████████| 100/100 [00:11<00:00,  8.58it/s]
Model tested on 100 tasks. Accuracy: 67.20%
0.4000000059604645 0.4000000059604645
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:32,  3.03it/s]  2%|▏         | 2/100 [00:00<00:20,  4.78it/s]  3%|▎         | 3/100 [00:00<00:16,  5.92it/s]  4%|▍         | 4/100 [00:00<00:14,  6.69it/s]  5%|▌         | 5/100 [00:00<00:13,  7.18it/s]  6%|▌         | 6/100 [00:00<00:12,  7.52it/s]  7%|▋         | 7/100 [00:01<00:11,  7.79it/s]  8%|▊         | 8/100 [00:01<00:11,  7.97it/s]  9%|▉         | 9/100 [00:01<00:11,  8.21it/s] 10%|█         | 10/100 [00:01<00:10,  8.44it/s] 11%|█         | 11/100 [00:01<00:10,  8.60it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.72it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.79it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.73it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.82it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.89it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.93it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.96it/s] 19%|█▉        | 19/100 [00:02<00:08,  9.01it/s] 20%|██        | 20/100 [00:02<00:08,  9.02it/s] 21%|██        | 21/100 [00:02<00:08,  9.02it/s] 22%|██▏       | 22/100 [00:02<00:08,  9.03it/s] 23%|██▎       | 23/100 [00:02<00:08,  9.02it/s] 24%|██▍       | 24/100 [00:02<00:08,  9.04it/s] 25%|██▌       | 25/100 [00:03<00:08,  9.03it/s] 26%|██▌       | 26/100 [00:03<00:08,  9.02it/s] 27%|██▋       | 27/100 [00:03<00:08,  9.05it/s] 28%|██▊       | 28/100 [00:03<00:07,  9.06it/s] 29%|██▉       | 29/100 [00:03<00:07,  9.05it/s] 30%|███       | 30/100 [00:03<00:07,  9.04it/s] 31%|███       | 31/100 [00:03<00:07,  9.06it/s] 32%|███▏      | 32/100 [00:03<00:07,  9.06it/s] 33%|███▎      | 33/100 [00:03<00:07,  9.05it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.05it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.04it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.05it/s] 37%|███▋      | 37/100 [00:04<00:06,  9.06it/s] 38%|███▊      | 38/100 [00:04<00:06,  9.06it/s] 39%|███▉      | 39/100 [00:04<00:06,  9.07it/s] 40%|████      | 40/100 [00:04<00:09,  6.10it/s] 41%|████      | 41/100 [00:05<00:08,  6.78it/s] 42%|████▏     | 42/100 [00:05<00:07,  7.36it/s] 43%|████▎     | 43/100 [00:05<00:07,  7.83it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.18it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.43it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.62it/s] 47%|████▋     | 47/100 [00:05<00:06,  8.77it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.88it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.94it/s] 50%|█████     | 50/100 [00:05<00:05,  8.98it/s] 51%|█████     | 51/100 [00:06<00:05,  9.02it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.02it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.04it/s] 54%|█████▍    | 54/100 [00:06<00:05,  9.04it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.04it/s] 56%|█████▌    | 56/100 [00:06<00:04,  9.03it/s] 57%|█████▋    | 57/100 [00:06<00:04,  9.02it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.01it/s] 59%|█████▉    | 59/100 [00:06<00:04,  9.03it/s] 60%|██████    | 60/100 [00:07<00:04,  9.03it/s] 61%|██████    | 61/100 [00:07<00:04,  9.02it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.01it/s] 63%|██████▎   | 63/100 [00:07<00:04,  9.02it/s] 64%|██████▍   | 64/100 [00:07<00:03,  9.02it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.00it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.01it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.02it/s] 68%|██████▊   | 68/100 [00:07<00:03,  9.01it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.01it/s] 70%|███████   | 70/100 [00:08<00:03,  9.01it/s] 71%|███████   | 71/100 [00:08<00:03,  9.03it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.05it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.05it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.05it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.06it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.07it/s] 77%|███████▋  | 77/100 [00:08<00:02,  9.17it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.25it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.33it/s] 80%|████████  | 80/100 [00:09<00:02,  9.37it/s] 81%|████████  | 81/100 [00:09<00:02,  9.40it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.41it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.42it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.42it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.43it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.44it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.47it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.47it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.48it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.48it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.49it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.48it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.47it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.47it/s] 95%|█████████▌| 95/100 [00:10<00:00,  9.49it/s] 96%|█████████▌| 96/100 [00:10<00:00,  9.48it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.47it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.47it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.48it/s]100%|██████████| 100/100 [00:11<00:00,  6.44it/s]100%|██████████| 100/100 [00:11<00:00,  8.61it/s]
Model tested on 100 tasks. Accuracy: 38.91%
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/360 [00:00<?, ?it/s]  0%|          | 0/360 [00:00<?, ?it/s, loss=8.73]  0%|          | 1/360 [00:00<05:50,  1.02it/s, loss=8.73]  1%|          | 2/360 [00:01<03:35,  1.66it/s, loss=8.73]  1%|          | 3/360 [00:01<02:37,  2.27it/s, loss=8.73]  1%|          | 4/360 [00:01<02:10,  2.74it/s, loss=8.73]  1%|▏         | 5/360 [00:02<01:54,  3.09it/s, loss=8.73]  2%|▏         | 6/360 [00:02<01:43,  3.40it/s, loss=8.73]  2%|▏         | 7/360 [00:02<01:37,  3.63it/s, loss=8.73]  2%|▏         | 8/360 [00:02<01:32,  3.80it/s, loss=8.73]  2%|▎         | 9/360 [00:03<01:29,  3.92it/s, loss=8.73]  3%|▎         | 10/360 [00:03<01:27,  4.00it/s, loss=8.73]  3%|▎         | 10/360 [00:03<01:27,  4.00it/s, loss=5.16]  3%|▎         | 11/360 [00:03<01:27,  3.98it/s, loss=5.16]  3%|▎         | 12/360 [00:03<01:25,  4.05it/s, loss=5.16]  4%|▎         | 13/360 [00:03<01:24,  4.08it/s, loss=5.16]  4%|▍         | 14/360 [00:04<01:24,  4.11it/s, loss=5.16]  4%|▍         | 15/360 [00:04<01:23,  4.12it/s, loss=5.16]  4%|▍         | 16/360 [00:04<01:23,  4.13it/s, loss=5.16]  5%|▍         | 17/360 [00:04<01:22,  4.15it/s, loss=5.16]  5%|▌         | 18/360 [00:05<01:22,  4.16it/s, loss=5.16]  5%|▌         | 19/360 [00:05<01:21,  4.16it/s, loss=5.16]  6%|▌         | 20/360 [00:05<01:21,  4.17it/s, loss=5.16]  6%|▌         | 20/360 [00:05<01:21,  4.17it/s, loss=3.79]  6%|▌         | 21/360 [00:05<01:21,  4.16it/s, loss=3.79]  6%|▌         | 22/360 [00:06<01:20,  4.18it/s, loss=3.79]  6%|▋         | 23/360 [00:06<01:21,  4.13it/s, loss=3.79]  7%|▋         | 24/360 [00:06<01:21,  4.15it/s, loss=3.79]  7%|▋         | 25/360 [00:06<01:21,  4.13it/s, loss=3.79]  7%|▋         | 26/360 [00:07<01:20,  4.12it/s, loss=3.79]  8%|▊         | 27/360 [00:07<01:20,  4.13it/s, loss=3.79]  8%|▊         | 28/360 [00:07<01:19,  4.15it/s, loss=3.79]  8%|▊         | 29/360 [00:07<01:19,  4.17it/s, loss=3.79]  8%|▊         | 30/360 [00:08<01:18,  4.18it/s, loss=3.79]  8%|▊         | 30/360 [00:08<01:18,  4.18it/s, loss=3.46]  9%|▊         | 31/360 [00:08<01:18,  4.19it/s, loss=3.46]  9%|▉         | 32/360 [00:08<01:18,  4.20it/s, loss=3.46]  9%|▉         | 33/360 [00:08<01:17,  4.20it/s, loss=3.46]  9%|▉         | 34/360 [00:09<01:17,  4.21it/s, loss=3.46] 10%|▉         | 35/360 [00:09<01:17,  4.20it/s, loss=3.46] 10%|█         | 36/360 [00:09<01:17,  4.20it/s, loss=3.46] 10%|█         | 37/360 [00:09<01:17,  4.19it/s, loss=3.46] 11%|█         | 38/360 [00:09<01:17,  4.13it/s, loss=3.46] 11%|█         | 39/360 [00:10<01:18,  4.11it/s, loss=3.46] 11%|█         | 40/360 [00:10<01:17,  4.14it/s, loss=3.46] 11%|█         | 40/360 [00:10<01:17,  4.14it/s, loss=2.9]  11%|█▏        | 41/360 [00:10<01:16,  4.16it/s, loss=2.9] 12%|█▏        | 42/360 [00:10<01:16,  4.18it/s, loss=2.9] 12%|█▏        | 43/360 [00:11<01:34,  3.35it/s, loss=2.9] 12%|█▏        | 44/360 [00:11<01:28,  3.58it/s, loss=2.9] 12%|█▎        | 45/360 [00:11<01:23,  3.75it/s, loss=2.9] 13%|█▎        | 46/360 [00:12<01:21,  3.87it/s, loss=2.9] 13%|█▎        | 47/360 [00:12<01:18,  3.98it/s, loss=2.9] 13%|█▎        | 48/360 [00:12<01:17,  4.04it/s, loss=2.9] 14%|█▎        | 49/360 [00:12<01:15,  4.09it/s, loss=2.9] 14%|█▍        | 50/360 [00:13<01:15,  4.09it/s, loss=2.9] 14%|█▍        | 50/360 [00:13<01:15,  4.09it/s, loss=2.72] 14%|█▍        | 51/360 [00:13<01:14,  4.13it/s, loss=2.72] 14%|█▍        | 52/360 [00:13<01:14,  4.15it/s, loss=2.72] 15%|█▍        | 53/360 [00:13<01:13,  4.17it/s, loss=2.72] 15%|█▌        | 54/360 [00:13<01:13,  4.18it/s, loss=2.72] 15%|█▌        | 55/360 [00:14<01:13,  4.18it/s, loss=2.72] 16%|█▌        | 56/360 [00:14<01:14,  4.10it/s, loss=2.72] 16%|█▌        | 57/360 [00:14<01:13,  4.13it/s, loss=2.72] 16%|█▌        | 58/360 [00:14<01:12,  4.14it/s, loss=2.72] 16%|█▋        | 59/360 [00:15<01:12,  4.17it/s, loss=2.72] 17%|█▋        | 60/360 [00:15<01:11,  4.18it/s, loss=2.72] 17%|█▋        | 60/360 [00:15<01:11,  4.18it/s, loss=2.67] 17%|█▋        | 61/360 [00:15<01:11,  4.19it/s, loss=2.67] 17%|█▋        | 62/360 [00:15<01:10,  4.20it/s, loss=2.67] 18%|█▊        | 63/360 [00:16<01:10,  4.20it/s, loss=2.67] 18%|█▊        | 64/360 [00:16<01:10,  4.21it/s, loss=2.67] 18%|█▊        | 65/360 [00:16<01:09,  4.22it/s, loss=2.67] 18%|█▊        | 66/360 [00:16<01:09,  4.22it/s, loss=2.67] 19%|█▊        | 67/360 [00:17<01:09,  4.22it/s, loss=2.67] 19%|█▉        | 68/360 [00:17<01:09,  4.22it/s, loss=2.67] 19%|█▉        | 69/360 [00:17<01:09,  4.22it/s, loss=2.67] 19%|█▉        | 70/360 [00:17<01:09,  4.19it/s, loss=2.67] 19%|█▉        | 70/360 [00:18<01:09,  4.19it/s, loss=2.56] 20%|█▉        | 71/360 [00:18<01:08,  4.20it/s, loss=2.56] 20%|██        | 72/360 [00:18<01:08,  4.21it/s, loss=2.56] 20%|██        | 73/360 [00:18<01:08,  4.21it/s, loss=2.56] 21%|██        | 74/360 [00:18<01:07,  4.22it/s, loss=2.56] 21%|██        | 75/360 [00:18<01:07,  4.22it/s, loss=2.56] 21%|██        | 76/360 [00:19<01:07,  4.21it/s, loss=2.56] 21%|██▏       | 77/360 [00:19<01:07,  4.22it/s, loss=2.56] 22%|██▏       | 78/360 [00:19<01:06,  4.22it/s, loss=2.56] 22%|██▏       | 79/360 [00:19<01:06,  4.22it/s, loss=2.56] 22%|██▏       | 80/360 [00:20<01:06,  4.22it/s, loss=2.56] 22%|██▏       | 80/360 [00:20<01:06,  4.22it/s, loss=2.43] 22%|██▎       | 81/360 [00:20<01:06,  4.22it/s, loss=2.43] 23%|██▎       | 82/360 [00:20<01:05,  4.23it/s, loss=2.43] 23%|██▎       | 83/360 [00:20<01:05,  4.22it/s, loss=2.43] 23%|██▎       | 84/360 [00:21<01:05,  4.22it/s, loss=2.43] 24%|██▎       | 85/360 [00:21<01:05,  4.19it/s, loss=2.43] 24%|██▍       | 86/360 [00:21<01:05,  4.19it/s, loss=2.43] 24%|██▍       | 87/360 [00:22<01:20,  3.41it/s, loss=2.43] 24%|██▍       | 88/360 [00:22<01:15,  3.62it/s, loss=2.43] 25%|██▍       | 89/360 [00:22<01:11,  3.78it/s, loss=2.43] 25%|██▌       | 90/360 [00:22<01:09,  3.91it/s, loss=2.43] 25%|██▌       | 90/360 [00:22<01:09,  3.91it/s, loss=2.34] 25%|██▌       | 91/360 [00:22<01:07,  3.99it/s, loss=2.34] 26%|██▌       | 92/360 [00:23<01:06,  4.04it/s, loss=2.34] 26%|██▌       | 93/360 [00:23<01:05,  4.09it/s, loss=2.34] 26%|██▌       | 94/360 [00:23<01:04,  4.12it/s, loss=2.34] 26%|██▋       | 95/360 [00:23<01:05,  4.05it/s, loss=2.34] 27%|██▋       | 96/360 [00:24<01:04,  4.09it/s, loss=2.34] 27%|██▋       | 97/360 [00:24<01:03,  4.12it/s, loss=2.34] 27%|██▋       | 98/360 [00:24<01:03,  4.13it/s, loss=2.34] 28%|██▊       | 99/360 [00:24<01:02,  4.15it/s, loss=2.34] 28%|██▊       | 100/360 [00:25<01:03,  4.12it/s, loss=2.34] 28%|██▊       | 100/360 [00:25<01:03,  4.12it/s, loss=2.17] 28%|██▊       | 101/360 [00:25<01:02,  4.13it/s, loss=2.17] 28%|██▊       | 102/360 [00:25<01:02,  4.14it/s, loss=2.17] 29%|██▊       | 103/360 [00:25<01:01,  4.15it/s, loss=2.17] 29%|██▉       | 104/360 [00:26<01:01,  4.17it/s, loss=2.17] 29%|██▉       | 105/360 [00:26<01:01,  4.17it/s, loss=2.17] 29%|██▉       | 106/360 [00:26<01:00,  4.17it/s, loss=2.17] 30%|██▉       | 107/360 [00:26<01:00,  4.18it/s, loss=2.17] 30%|███       | 108/360 [00:27<01:00,  4.19it/s, loss=2.17] 30%|███       | 109/360 [00:27<01:00,  4.13it/s, loss=2.17] 31%|███       | 110/360 [00:27<01:00,  4.14it/s, loss=2.17] 31%|███       | 110/360 [00:27<01:00,  4.14it/s, loss=2.19] 31%|███       | 111/360 [00:27<00:59,  4.16it/s, loss=2.19] 31%|███       | 112/360 [00:28<00:59,  4.18it/s, loss=2.19] 31%|███▏      | 113/360 [00:28<00:59,  4.18it/s, loss=2.19] 32%|███▏      | 114/360 [00:28<00:58,  4.19it/s, loss=2.19] 32%|███▏      | 115/360 [00:28<00:58,  4.19it/s, loss=2.19] 32%|███▏      | 116/360 [00:28<00:58,  4.20it/s, loss=2.19] 32%|███▎      | 117/360 [00:29<00:57,  4.21it/s, loss=2.19] 33%|███▎      | 118/360 [00:29<00:57,  4.21it/s, loss=2.19] 33%|███▎      | 119/360 [00:29<00:57,  4.22it/s, loss=2.19] 33%|███▎      | 120/360 [00:29<00:56,  4.22it/s, loss=2.19] 33%|███▎      | 120/360 [00:30<00:56,  4.22it/s, loss=2.01] 34%|███▎      | 121/360 [00:30<00:56,  4.23it/s, loss=2.01] 34%|███▍      | 122/360 [00:30<00:56,  4.23it/s, loss=2.01] 34%|███▍      | 123/360 [00:30<00:56,  4.22it/s, loss=2.01] 34%|███▍      | 124/360 [00:30<00:55,  4.22it/s, loss=2.01] 35%|███▍      | 125/360 [00:31<00:55,  4.22it/s, loss=2.01] 35%|███▌      | 126/360 [00:31<00:55,  4.22it/s, loss=2.01] 35%|███▌      | 127/360 [00:31<00:55,  4.22it/s, loss=2.01] 36%|███▌      | 128/360 [00:31<00:54,  4.23it/s, loss=2.01] 36%|███▌      | 129/360 [00:32<00:54,  4.23it/s, loss=2.01] 36%|███▌      | 130/360 [00:32<00:54,  4.24it/s, loss=2.01] 36%|███▌      | 130/360 [00:32<00:54,  4.24it/s, loss=1.97] 36%|███▋      | 131/360 [00:32<01:06,  3.44it/s, loss=1.97] 37%|███▋      | 132/360 [00:32<01:02,  3.66it/s, loss=1.97] 37%|███▋      | 133/360 [00:33<00:59,  3.81it/s, loss=1.97] 37%|███▋      | 134/360 [00:33<00:57,  3.93it/s, loss=1.97] 38%|███▊      | 135/360 [00:33<00:56,  4.01it/s, loss=1.97] 38%|███▊      | 136/360 [00:33<00:55,  4.07it/s, loss=1.97] 38%|███▊      | 137/360 [00:34<00:55,  4.05it/s, loss=1.97] 38%|███▊      | 138/360 [00:34<00:54,  4.09it/s, loss=1.97] 39%|███▊      | 139/360 [00:34<00:53,  4.12it/s, loss=1.97] 39%|███▉      | 140/360 [00:34<00:53,  4.15it/s, loss=1.97] 39%|███▉      | 140/360 [00:35<00:53,  4.15it/s, loss=1.93] 39%|███▉      | 141/360 [00:35<00:52,  4.17it/s, loss=1.93] 39%|███▉      | 142/360 [00:35<00:52,  4.18it/s, loss=1.93] 40%|███▉      | 143/360 [00:35<00:51,  4.18it/s, loss=1.93] 40%|████      | 144/360 [00:35<00:51,  4.19it/s, loss=1.93] 40%|████      | 145/360 [00:36<00:51,  4.19it/s, loss=1.93] 41%|████      | 146/360 [00:36<00:50,  4.20it/s, loss=1.93] 41%|████      | 147/360 [00:36<00:50,  4.19it/s, loss=1.93] 41%|████      | 148/360 [00:36<00:50,  4.20it/s, loss=1.93] 41%|████▏     | 149/360 [00:37<00:50,  4.19it/s, loss=1.93] 42%|████▏     | 150/360 [00:37<00:50,  4.19it/s, loss=1.93] 42%|████▏     | 150/360 [00:37<00:50,  4.19it/s, loss=1.93] 42%|████▏     | 151/360 [00:37<00:49,  4.19it/s, loss=1.93] 42%|████▏     | 152/360 [00:37<00:50,  4.13it/s, loss=1.93] 42%|████▎     | 153/360 [00:37<00:49,  4.16it/s, loss=1.93] 43%|████▎     | 154/360 [00:38<00:49,  4.16it/s, loss=1.93] 43%|████▎     | 155/360 [00:38<00:49,  4.18it/s, loss=1.93] 43%|████▎     | 156/360 [00:38<00:48,  4.19it/s, loss=1.93] 44%|████▎     | 157/360 [00:38<00:48,  4.19it/s, loss=1.93] 44%|████▍     | 158/360 [00:39<00:48,  4.15it/s, loss=1.93] 44%|████▍     | 159/360 [00:39<00:48,  4.13it/s, loss=1.93] 44%|████▍     | 160/360 [00:39<00:48,  4.16it/s, loss=1.93] 44%|████▍     | 160/360 [00:39<00:48,  4.16it/s, loss=1.81] 45%|████▍     | 161/360 [00:39<00:47,  4.18it/s, loss=1.81] 45%|████▌     | 162/360 [00:40<00:47,  4.20it/s, loss=1.81] 45%|████▌     | 163/360 [00:40<00:46,  4.21it/s, loss=1.81] 46%|████▌     | 164/360 [00:40<00:46,  4.22it/s, loss=1.81] 46%|████▌     | 165/360 [00:40<00:46,  4.23it/s, loss=1.81] 46%|████▌     | 166/360 [00:41<00:45,  4.23it/s, loss=1.81] 46%|████▋     | 167/360 [00:41<00:45,  4.24it/s, loss=1.81] 47%|████▋     | 168/360 [00:41<00:45,  4.24it/s, loss=1.81] 47%|████▋     | 169/360 [00:41<00:45,  4.24it/s, loss=1.81] 47%|████▋     | 170/360 [00:42<00:44,  4.24it/s, loss=1.81] 47%|████▋     | 170/360 [00:42<00:44,  4.24it/s, loss=1.81] 48%|████▊     | 171/360 [00:42<00:44,  4.24it/s, loss=1.81] 48%|████▊     | 172/360 [00:42<00:44,  4.24it/s, loss=1.81] 48%|████▊     | 173/360 [00:42<00:44,  4.24it/s, loss=1.81] 48%|████▊     | 174/360 [00:42<00:43,  4.24it/s, loss=1.81] 49%|████▊     | 175/360 [00:43<00:53,  3.44it/s, loss=1.81] 49%|████▉     | 176/360 [00:43<00:50,  3.66it/s, loss=1.81] 49%|████▉     | 177/360 [00:43<00:47,  3.82it/s, loss=1.81] 49%|████▉     | 178/360 [00:44<00:46,  3.94it/s, loss=1.81] 50%|████▉     | 179/360 [00:44<00:45,  4.01it/s, loss=1.81] 50%|█████     | 180/360 [00:44<00:44,  4.08it/s, loss=1.81] 50%|█████     | 180/360 [00:44<00:44,  4.08it/s, loss=1.85] 50%|█████     | 181/360 [00:44<00:43,  4.12it/s, loss=1.85] 51%|█████     | 182/360 [00:45<00:42,  4.15it/s, loss=1.85] 51%|█████     | 183/360 [00:45<00:42,  4.17it/s, loss=1.85] 51%|█████     | 184/360 [00:45<00:42,  4.18it/s, loss=1.85] 51%|█████▏    | 185/360 [00:45<00:41,  4.20it/s, loss=1.85] 52%|█████▏    | 186/360 [00:45<00:41,  4.20it/s, loss=1.85] 52%|█████▏    | 187/360 [00:46<00:41,  4.20it/s, loss=1.85] 52%|█████▏    | 188/360 [00:46<00:40,  4.20it/s, loss=1.85] 52%|█████▎    | 189/360 [00:46<00:40,  4.20it/s, loss=1.85] 53%|█████▎    | 190/360 [00:46<00:40,  4.21it/s, loss=1.85] 53%|█████▎    | 190/360 [00:47<00:40,  4.21it/s, loss=1.71] 53%|█████▎    | 191/360 [00:47<00:40,  4.20it/s, loss=1.71] 53%|█████▎    | 192/360 [00:47<00:39,  4.21it/s, loss=1.71] 54%|█████▎    | 193/360 [00:47<00:39,  4.20it/s, loss=1.71] 54%|█████▍    | 194/360 [00:47<00:39,  4.21it/s, loss=1.71] 54%|█████▍    | 195/360 [00:48<00:39,  4.21it/s, loss=1.71] 54%|█████▍    | 196/360 [00:48<00:39,  4.14it/s, loss=1.71] 55%|█████▍    | 197/360 [00:48<00:39,  4.16it/s, loss=1.71] 55%|█████▌    | 198/360 [00:48<00:38,  4.18it/s, loss=1.71] 55%|█████▌    | 199/360 [00:49<00:38,  4.19it/s, loss=1.71] 56%|█████▌    | 200/360 [00:49<00:38,  4.20it/s, loss=1.71] 56%|█████▌    | 200/360 [00:49<00:38,  4.20it/s, loss=1.6]  56%|█████▌    | 201/360 [00:49<00:37,  4.21it/s, loss=1.6] 56%|█████▌    | 202/360 [00:49<00:37,  4.22it/s, loss=1.6] 56%|█████▋    | 203/360 [00:50<00:37,  4.21it/s, loss=1.6] 57%|█████▋    | 204/360 [00:50<00:36,  4.22it/s, loss=1.6] 57%|█████▋    | 205/360 [00:50<00:36,  4.23it/s, loss=1.6] 57%|█████▋    | 206/360 [00:50<00:36,  4.23it/s, loss=1.6] 57%|█████▊    | 207/360 [00:50<00:36,  4.23it/s, loss=1.6] 58%|█████▊    | 208/360 [00:51<00:35,  4.23it/s, loss=1.6] 58%|█████▊    | 209/360 [00:51<00:35,  4.20it/s, loss=1.6] 58%|█████▊    | 210/360 [00:51<00:35,  4.21it/s, loss=1.6] 58%|█████▊    | 210/360 [00:51<00:35,  4.21it/s, loss=1.59] 59%|█████▊    | 211/360 [00:51<00:35,  4.16it/s, loss=1.59] 59%|█████▉    | 212/360 [00:52<00:35,  4.19it/s, loss=1.59] 59%|█████▉    | 213/360 [00:52<00:34,  4.20it/s, loss=1.59] 59%|█████▉    | 214/360 [00:52<00:34,  4.21it/s, loss=1.59] 60%|█████▉    | 215/360 [00:52<00:34,  4.22it/s, loss=1.59] 60%|██████    | 216/360 [00:53<00:34,  4.22it/s, loss=1.59] 60%|██████    | 217/360 [00:53<00:33,  4.23it/s, loss=1.59] 61%|██████    | 218/360 [00:53<00:33,  4.24it/s, loss=1.59] 61%|██████    | 219/360 [00:53<00:40,  3.45it/s, loss=1.59] 61%|██████    | 220/360 [00:54<00:38,  3.67it/s, loss=1.59] 61%|██████    | 220/360 [00:54<00:38,  3.67it/s, loss=1.71] 61%|██████▏   | 221/360 [00:54<00:36,  3.82it/s, loss=1.71] 62%|██████▏   | 222/360 [00:54<00:35,  3.94it/s, loss=1.71] 62%|██████▏   | 223/360 [00:54<00:34,  4.03it/s, loss=1.71] 62%|██████▏   | 224/360 [00:55<00:33,  4.08it/s, loss=1.71] 62%|██████▎   | 225/360 [00:55<00:32,  4.12it/s, loss=1.71] 63%|██████▎   | 226/360 [00:55<00:32,  4.16it/s, loss=1.71] 63%|██████▎   | 227/360 [00:55<00:31,  4.18it/s, loss=1.71] 63%|██████▎   | 228/360 [00:56<00:31,  4.19it/s, loss=1.71] 64%|██████▎   | 229/360 [00:56<00:31,  4.19it/s, loss=1.71] 64%|██████▍   | 230/360 [00:56<00:30,  4.20it/s, loss=1.71] 64%|██████▍   | 230/360 [00:56<00:30,  4.20it/s, loss=1.62] 64%|██████▍   | 231/360 [00:56<00:30,  4.20it/s, loss=1.62] 64%|██████▍   | 232/360 [00:57<00:30,  4.21it/s, loss=1.62] 65%|██████▍   | 233/360 [00:57<00:30,  4.20it/s, loss=1.62] 65%|██████▌   | 234/360 [00:57<00:29,  4.21it/s, loss=1.62] 65%|██████▌   | 235/360 [00:57<00:29,  4.21it/s, loss=1.62] 66%|██████▌   | 236/360 [00:58<00:29,  4.21it/s, loss=1.62] 66%|██████▌   | 237/360 [00:58<00:29,  4.21it/s, loss=1.62] 66%|██████▌   | 238/360 [00:58<00:28,  4.22it/s, loss=1.62] 66%|██████▋   | 239/360 [00:58<00:28,  4.22it/s, loss=1.62] 67%|██████▋   | 240/360 [00:58<00:28,  4.16it/s, loss=1.62] 67%|██████▋   | 240/360 [00:59<00:28,  4.16it/s, loss=1.57] 67%|██████▋   | 241/360 [00:59<00:28,  4.16it/s, loss=1.57] 67%|██████▋   | 242/360 [00:59<00:28,  4.18it/s, loss=1.57] 68%|██████▊   | 243/360 [00:59<00:28,  4.17it/s, loss=1.57] 68%|██████▊   | 244/360 [00:59<00:28,  4.13it/s, loss=1.57] 68%|██████▊   | 245/360 [01:00<00:27,  4.15it/s, loss=1.57] 68%|██████▊   | 246/360 [01:00<00:27,  4.17it/s, loss=1.57] 69%|██████▊   | 247/360 [01:00<00:26,  4.19it/s, loss=1.57] 69%|██████▉   | 248/360 [01:00<00:26,  4.20it/s, loss=1.57] 69%|██████▉   | 249/360 [01:01<00:26,  4.21it/s, loss=1.57] 69%|██████▉   | 250/360 [01:01<00:26,  4.22it/s, loss=1.57] 69%|██████▉   | 250/360 [01:01<00:26,  4.22it/s, loss=1.57] 70%|██████▉   | 251/360 [01:01<00:25,  4.22it/s, loss=1.57] 70%|███████   | 252/360 [01:01<00:25,  4.23it/s, loss=1.57] 70%|███████   | 253/360 [01:02<00:25,  4.23it/s, loss=1.57] 71%|███████   | 254/360 [01:02<00:25,  4.23it/s, loss=1.57] 71%|███████   | 255/360 [01:02<00:24,  4.23it/s, loss=1.57] 71%|███████   | 256/360 [01:02<00:24,  4.23it/s, loss=1.57] 71%|███████▏  | 257/360 [01:03<00:24,  4.24it/s, loss=1.57] 72%|███████▏  | 258/360 [01:03<00:24,  4.24it/s, loss=1.57] 72%|███████▏  | 259/360 [01:03<00:23,  4.24it/s, loss=1.57] 72%|███████▏  | 260/360 [01:03<00:23,  4.24it/s, loss=1.57] 72%|███████▏  | 260/360 [01:03<00:23,  4.24it/s, loss=1.56] 72%|███████▎  | 261/360 [01:03<00:23,  4.24it/s, loss=1.56] 73%|███████▎  | 262/360 [01:04<00:23,  4.24it/s, loss=1.56] 73%|███████▎  | 263/360 [01:04<00:22,  4.24it/s, loss=1.56] 73%|███████▎  | 264/360 [01:04<00:28,  3.43it/s, loss=1.56] 74%|███████▎  | 265/360 [01:05<00:26,  3.65it/s, loss=1.56] 74%|███████▍  | 266/360 [01:05<00:24,  3.81it/s, loss=1.56] 74%|███████▍  | 267/360 [01:05<00:23,  3.92it/s, loss=1.56] 74%|███████▍  | 268/360 [01:05<00:22,  4.01it/s, loss=1.56] 75%|███████▍  | 269/360 [01:06<00:22,  4.07it/s, loss=1.56] 75%|███████▌  | 270/360 [01:06<00:21,  4.12it/s, loss=1.56] 75%|███████▌  | 270/360 [01:06<00:21,  4.12it/s, loss=1.44] 75%|███████▌  | 271/360 [01:06<00:21,  4.15it/s, loss=1.44] 76%|███████▌  | 272/360 [01:06<00:21,  4.17it/s, loss=1.44] 76%|███████▌  | 273/360 [01:06<00:20,  4.18it/s, loss=1.44] 76%|███████▌  | 274/360 [01:07<00:20,  4.19it/s, loss=1.44] 76%|███████▋  | 275/360 [01:07<00:20,  4.19it/s, loss=1.44] 77%|███████▋  | 276/360 [01:07<00:20,  4.20it/s, loss=1.44] 77%|███████▋  | 277/360 [01:07<00:19,  4.20it/s, loss=1.44] 77%|███████▋  | 278/360 [01:08<00:19,  4.20it/s, loss=1.44] 78%|███████▊  | 279/360 [01:08<00:19,  4.20it/s, loss=1.44] 78%|███████▊  | 280/360 [01:08<00:19,  4.21it/s, loss=1.44] 78%|███████▊  | 280/360 [01:08<00:19,  4.21it/s, loss=1.49] 78%|███████▊  | 281/360 [01:08<00:18,  4.21it/s, loss=1.49] 78%|███████▊  | 282/360 [01:09<00:18,  4.21it/s, loss=1.49] 79%|███████▊  | 283/360 [01:09<00:18,  4.22it/s, loss=1.49] 79%|███████▉  | 284/360 [01:09<00:18,  4.22it/s, loss=1.49] 79%|███████▉  | 285/360 [01:09<00:17,  4.22it/s, loss=1.49] 79%|███████▉  | 286/360 [01:10<00:17,  4.22it/s, loss=1.49] 80%|███████▉  | 287/360 [01:10<00:17,  4.22it/s, loss=1.49] 80%|████████  | 288/360 [01:10<00:17,  4.23it/s, loss=1.49] 80%|████████  | 289/360 [01:10<00:16,  4.23it/s, loss=1.49] 81%|████████  | 290/360 [01:11<00:16,  4.23it/s, loss=1.49] 81%|████████  | 290/360 [01:11<00:16,  4.23it/s, loss=1.39] 81%|████████  | 291/360 [01:11<00:16,  4.23it/s, loss=1.39] 81%|████████  | 292/360 [01:11<00:16,  4.22it/s, loss=1.39] 81%|████████▏ | 293/360 [01:11<00:15,  4.20it/s, loss=1.39] 82%|████████▏ | 294/360 [01:11<00:15,  4.21it/s, loss=1.39] 82%|████████▏ | 295/360 [01:12<00:15,  4.22it/s, loss=1.39] 82%|████████▏ | 296/360 [01:12<00:15,  4.23it/s, loss=1.39] 82%|████████▎ | 297/360 [01:12<00:14,  4.23it/s, loss=1.39] 83%|████████▎ | 298/360 [01:12<00:14,  4.23it/s, loss=1.39] 83%|████████▎ | 299/360 [01:13<00:14,  4.23it/s, loss=1.39] 83%|████████▎ | 300/360 [01:13<00:14,  4.23it/s, loss=1.39] 83%|████████▎ | 300/360 [01:13<00:14,  4.23it/s, loss=1.42] 84%|████████▎ | 301/360 [01:13<00:13,  4.24it/s, loss=1.42] 84%|████████▍ | 302/360 [01:13<00:13,  4.24it/s, loss=1.42] 84%|████████▍ | 303/360 [01:14<00:13,  4.24it/s, loss=1.42] 84%|████████▍ | 304/360 [01:14<00:13,  4.24it/s, loss=1.42] 85%|████████▍ | 305/360 [01:14<00:13,  4.23it/s, loss=1.42] 85%|████████▌ | 306/360 [01:14<00:12,  4.23it/s, loss=1.42] 85%|████████▌ | 307/360 [01:15<00:12,  4.23it/s, loss=1.42] 86%|████████▌ | 308/360 [01:15<00:12,  4.23it/s, loss=1.42] 86%|████████▌ | 309/360 [01:15<00:14,  3.42it/s, loss=1.42] 86%|████████▌ | 310/360 [01:15<00:13,  3.64it/s, loss=1.42] 86%|████████▌ | 310/360 [01:16<00:13,  3.64it/s, loss=1.45] 86%|████████▋ | 311/360 [01:16<00:12,  3.80it/s, loss=1.45] 87%|████████▋ | 312/360 [01:16<00:12,  3.92it/s, loss=1.45] 87%|████████▋ | 313/360 [01:16<00:11,  4.01it/s, loss=1.45] 87%|████████▋ | 314/360 [01:16<00:11,  4.01it/s, loss=1.45] 88%|████████▊ | 315/360 [01:17<00:11,  4.06it/s, loss=1.45] 88%|████████▊ | 316/360 [01:17<00:10,  4.10it/s, loss=1.45] 88%|████████▊ | 317/360 [01:17<00:10,  4.12it/s, loss=1.45] 88%|████████▊ | 318/360 [01:17<00:10,  4.14it/s, loss=1.45] 89%|████████▊ | 319/360 [01:18<00:09,  4.15it/s, loss=1.45] 89%|████████▉ | 320/360 [01:18<00:09,  4.15it/s, loss=1.45] 89%|████████▉ | 320/360 [01:18<00:09,  4.15it/s, loss=1.34] 89%|████████▉ | 321/360 [01:18<00:09,  4.18it/s, loss=1.34] 89%|████████▉ | 322/360 [01:18<00:09,  4.19it/s, loss=1.34] 90%|████████▉ | 323/360 [01:19<00:08,  4.20it/s, loss=1.34] 90%|█████████ | 324/360 [01:19<00:08,  4.19it/s, loss=1.34] 90%|█████████ | 325/360 [01:19<00:08,  4.20it/s, loss=1.34] 91%|█████████ | 326/360 [01:19<00:08,  4.20it/s, loss=1.34] 91%|█████████ | 327/360 [01:19<00:07,  4.20it/s, loss=1.34] 91%|█████████ | 328/360 [01:20<00:07,  4.21it/s, loss=1.34] 91%|█████████▏| 329/360 [01:20<00:07,  4.21it/s, loss=1.34] 92%|█████████▏| 330/360 [01:20<00:07,  4.21it/s, loss=1.34] 92%|█████████▏| 330/360 [01:20<00:07,  4.21it/s, loss=1.29] 92%|█████████▏| 331/360 [01:20<00:06,  4.21it/s, loss=1.29] 92%|█████████▏| 332/360 [01:21<00:06,  4.22it/s, loss=1.29] 92%|█████████▎| 333/360 [01:21<00:06,  4.22it/s, loss=1.29] 93%|█████████▎| 334/360 [01:21<00:06,  4.22it/s, loss=1.29] 93%|█████████▎| 335/360 [01:21<00:06,  4.15it/s, loss=1.29] 93%|█████████▎| 336/360 [01:22<00:05,  4.18it/s, loss=1.29] 94%|█████████▎| 337/360 [01:22<00:05,  4.22it/s, loss=1.29] 94%|█████████▍| 338/360 [01:22<00:05,  4.27it/s, loss=1.29] 94%|█████████▍| 339/360 [01:22<00:04,  4.21it/s, loss=1.29] 94%|█████████▍| 340/360 [01:23<00:04,  4.26it/s, loss=1.29] 94%|█████████▍| 340/360 [01:23<00:04,  4.26it/s, loss=1.37] 95%|█████████▍| 341/360 [01:23<00:04,  4.30it/s, loss=1.37] 95%|█████████▌| 342/360 [01:23<00:04,  4.32it/s, loss=1.37] 95%|█████████▌| 343/360 [01:23<00:03,  4.33it/s, loss=1.37] 96%|█████████▌| 344/360 [01:23<00:03,  4.34it/s, loss=1.37] 96%|█████████▌| 345/360 [01:24<00:03,  4.35it/s, loss=1.37] 96%|█████████▌| 346/360 [01:24<00:03,  4.36it/s, loss=1.37] 96%|█████████▋| 347/360 [01:24<00:02,  4.36it/s, loss=1.37] 97%|█████████▋| 348/360 [01:24<00:02,  4.37it/s, loss=1.37] 97%|█████████▋| 349/360 [01:25<00:02,  4.37it/s, loss=1.37] 97%|█████████▋| 350/360 [01:25<00:02,  4.37it/s, loss=1.37] 97%|█████████▋| 350/360 [01:25<00:02,  4.37it/s, loss=1.33] 98%|█████████▊| 351/360 [01:25<00:02,  4.37it/s, loss=1.33] 98%|█████████▊| 352/360 [01:25<00:01,  4.38it/s, loss=1.33] 98%|█████████▊| 353/360 [01:26<00:01,  4.37it/s, loss=1.33] 98%|█████████▊| 354/360 [01:26<00:01,  3.59it/s, loss=1.33] 99%|█████████▊| 355/360 [01:26<00:01,  3.80it/s, loss=1.33] 99%|█████████▉| 356/360 [01:26<00:01,  3.96it/s, loss=1.33] 99%|█████████▉| 357/360 [01:27<00:00,  4.07it/s, loss=1.33] 99%|█████████▉| 358/360 [01:27<00:00,  4.16it/s, loss=1.33]100%|█████████▉| 359/360 [01:27<00:00,  4.22it/s, loss=1.33]100%|██████████| 360/360 [01:27<00:00,  4.26it/s, loss=1.33]100%|██████████| 360/360 [01:27<00:00,  4.10it/s, loss=1.33]
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  1%|          | 1/100 [00:00<00:28,  3.52it/s]  2%|▏         | 2/100 [00:00<00:18,  5.33it/s]  3%|▎         | 3/100 [00:00<00:15,  6.32it/s]  4%|▍         | 4/100 [00:00<00:13,  7.12it/s]  5%|▌         | 5/100 [00:00<00:12,  7.65it/s]  6%|▌         | 6/100 [00:00<00:11,  8.01it/s]  7%|▋         | 7/100 [00:00<00:11,  8.24it/s]  8%|▊         | 8/100 [00:01<00:11,  8.33it/s]  9%|▉         | 9/100 [00:01<00:11,  8.11it/s] 10%|█         | 10/100 [00:01<00:10,  8.36it/s] 11%|█         | 11/100 [00:01<00:10,  8.51it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.63it/s] 13%|█▎        | 13/100 [00:01<00:09,  8.71it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.76it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.81it/s] 16%|█▌        | 16/100 [00:02<00:09,  8.80it/s] 17%|█▋        | 17/100 [00:02<00:09,  8.85it/s] 18%|█▊        | 18/100 [00:02<00:09,  8.76it/s] 19%|█▉        | 19/100 [00:02<00:09,  8.82it/s] 20%|██        | 20/100 [00:02<00:09,  8.85it/s] 21%|██        | 21/100 [00:02<00:08,  8.88it/s] 22%|██▏       | 22/100 [00:02<00:08,  8.92it/s] 23%|██▎       | 23/100 [00:02<00:08,  8.93it/s] 24%|██▍       | 24/100 [00:02<00:08,  8.94it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.94it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.97it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.97it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.98it/s] 29%|██▉       | 29/100 [00:03<00:07,  8.96it/s] 30%|███       | 30/100 [00:03<00:07,  8.99it/s] 31%|███       | 31/100 [00:03<00:07,  9.01it/s] 32%|███▏      | 32/100 [00:03<00:07,  9.01it/s] 33%|███▎      | 33/100 [00:03<00:07,  9.02it/s] 34%|███▍      | 34/100 [00:04<00:07,  9.04it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.03it/s] 36%|███▌      | 36/100 [00:04<00:07,  8.92it/s] 37%|███▋      | 37/100 [00:04<00:07,  8.95it/s] 38%|███▊      | 38/100 [00:04<00:06,  8.98it/s] 39%|███▉      | 39/100 [00:04<00:06,  8.99it/s] 40%|████      | 40/100 [00:04<00:06,  9.00it/s] 41%|████      | 41/100 [00:04<00:06,  9.01it/s] 42%|████▏     | 42/100 [00:04<00:06,  9.04it/s] 43%|████▎     | 43/100 [00:05<00:06,  9.05it/s] 44%|████▍     | 44/100 [00:05<00:06,  9.05it/s] 45%|████▌     | 45/100 [00:05<00:06,  9.04it/s] 46%|████▌     | 46/100 [00:05<00:05,  9.06it/s] 47%|████▋     | 47/100 [00:05<00:05,  9.04it/s] 48%|████▊     | 48/100 [00:05<00:05,  9.05it/s] 49%|████▉     | 49/100 [00:05<00:05,  9.05it/s] 50%|█████     | 50/100 [00:05<00:05,  9.06it/s] 51%|█████     | 51/100 [00:05<00:05,  9.05it/s] 52%|█████▏    | 52/100 [00:06<00:05,  9.06it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.06it/s] 54%|█████▍    | 54/100 [00:06<00:07,  5.95it/s] 55%|█████▌    | 55/100 [00:06<00:06,  6.67it/s] 56%|█████▌    | 56/100 [00:06<00:06,  7.28it/s] 57%|█████▋    | 57/100 [00:06<00:05,  7.76it/s] 58%|█████▊    | 58/100 [00:06<00:05,  8.16it/s] 59%|█████▉    | 59/100 [00:06<00:04,  8.44it/s] 60%|██████    | 60/100 [00:07<00:04,  8.53it/s] 61%|██████    | 61/100 [00:07<00:04,  8.71it/s] 62%|██████▏   | 62/100 [00:07<00:04,  8.84it/s] 63%|██████▎   | 63/100 [00:07<00:04,  8.90it/s] 64%|██████▍   | 64/100 [00:07<00:04,  8.96it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.00it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.03it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.04it/s] 68%|██████▊   | 68/100 [00:07<00:03,  9.05it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.05it/s] 70%|███████   | 70/100 [00:08<00:03,  9.07it/s] 71%|███████   | 71/100 [00:08<00:03,  9.03it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.02it/s] 73%|███████▎  | 73/100 [00:08<00:03,  9.00it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.00it/s] 75%|███████▌  | 75/100 [00:08<00:02,  8.99it/s] 76%|███████▌  | 76/100 [00:08<00:02,  8.99it/s] 77%|███████▋  | 77/100 [00:08<00:02,  9.08it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.16it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.21it/s] 80%|████████  | 80/100 [00:09<00:02,  9.22it/s] 81%|████████  | 81/100 [00:09<00:02,  9.23it/s] 82%|████████▏ | 82/100 [00:09<00:01,  9.24it/s] 83%|████████▎ | 83/100 [00:09<00:01,  9.25it/s] 84%|████████▍ | 84/100 [00:09<00:01,  9.26it/s] 85%|████████▌ | 85/100 [00:09<00:01,  9.26it/s] 86%|████████▌ | 86/100 [00:09<00:01,  9.29it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.30it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.31it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.31it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.33it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.35it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.36it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.37it/s] 94%|█████████▍| 94/100 [00:10<00:00,  9.35it/s] 95%|█████████▌| 95/100 [00:10<00:00,  9.36it/s] 96%|█████████▌| 96/100 [00:10<00:00,  9.36it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.37it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.39it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.39it/s]100%|██████████| 100/100 [00:11<00:00,  9.40it/s]100%|██████████| 100/100 [00:11<00:00,  8.72it/s]
Model tested on 100 tasks. Accuracy: 64.53%
0.5 0.5
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/amirreza/anaconda3/envs/tf/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:30,  3.30it/s]  2%|▏         | 2/100 [00:00<00:19,  5.01it/s]  3%|▎         | 3/100 [00:00<00:16,  6.04it/s]  4%|▍         | 4/100 [00:00<00:14,  6.79it/s]  5%|▌         | 5/100 [00:00<00:13,  7.30it/s]  6%|▌         | 6/100 [00:00<00:12,  7.27it/s]  7%|▋         | 7/100 [00:01<00:12,  7.63it/s]  8%|▊         | 8/100 [00:01<00:11,  7.73it/s]  9%|▉         | 9/100 [00:01<00:11,  7.75it/s] 10%|█         | 10/100 [00:01<00:11,  8.07it/s] 11%|█         | 11/100 [00:01<00:10,  8.37it/s] 12%|█▏        | 12/100 [00:01<00:10,  8.59it/s] 13%|█▎        | 13/100 [00:01<00:10,  8.61it/s] 14%|█▍        | 14/100 [00:01<00:09,  8.72it/s] 15%|█▌        | 15/100 [00:01<00:09,  8.83it/s] 16%|█▌        | 16/100 [00:02<00:14,  5.67it/s] 17%|█▋        | 17/100 [00:02<00:13,  6.16it/s] 18%|█▊        | 18/100 [00:02<00:12,  6.83it/s] 19%|█▉        | 19/100 [00:02<00:10,  7.40it/s] 20%|██        | 20/100 [00:02<00:10,  7.85it/s] 21%|██        | 21/100 [00:02<00:09,  8.18it/s] 22%|██▏       | 22/100 [00:02<00:09,  8.40it/s] 23%|██▎       | 23/100 [00:03<00:08,  8.60it/s] 24%|██▍       | 24/100 [00:03<00:08,  8.75it/s] 25%|██▌       | 25/100 [00:03<00:08,  8.84it/s] 26%|██▌       | 26/100 [00:03<00:08,  8.89it/s] 27%|██▋       | 27/100 [00:03<00:08,  8.95it/s] 28%|██▊       | 28/100 [00:03<00:08,  8.97it/s] 29%|██▉       | 29/100 [00:03<00:07,  8.90it/s] 30%|███       | 30/100 [00:03<00:07,  8.93it/s] 31%|███       | 31/100 [00:03<00:07,  8.98it/s] 32%|███▏      | 32/100 [00:04<00:07,  8.99it/s] 33%|███▎      | 33/100 [00:04<00:07,  9.00it/s] 34%|███▍      | 34/100 [00:04<00:07,  8.99it/s] 35%|███▌      | 35/100 [00:04<00:07,  9.01it/s] 36%|███▌      | 36/100 [00:04<00:07,  9.01it/s] 37%|███▋      | 37/100 [00:04<00:06,  9.00it/s] 38%|███▊      | 38/100 [00:04<00:06,  8.98it/s] 39%|███▉      | 39/100 [00:04<00:06,  8.98it/s] 40%|████      | 40/100 [00:04<00:06,  8.93it/s] 41%|████      | 41/100 [00:05<00:06,  8.53it/s] 42%|████▏     | 42/100 [00:05<00:06,  8.67it/s] 43%|████▎     | 43/100 [00:05<00:06,  8.79it/s] 44%|████▍     | 44/100 [00:05<00:06,  8.85it/s] 45%|████▌     | 45/100 [00:05<00:06,  8.89it/s] 46%|████▌     | 46/100 [00:05<00:06,  8.91it/s] 47%|████▋     | 47/100 [00:05<00:05,  8.96it/s] 48%|████▊     | 48/100 [00:05<00:05,  8.97it/s] 49%|████▉     | 49/100 [00:05<00:05,  8.98it/s] 50%|█████     | 50/100 [00:06<00:05,  8.96it/s] 51%|█████     | 51/100 [00:06<00:05,  8.99it/s] 52%|█████▏    | 52/100 [00:06<00:05,  8.99it/s] 53%|█████▎    | 53/100 [00:06<00:05,  9.00it/s] 54%|█████▍    | 54/100 [00:06<00:05,  9.02it/s] 55%|█████▌    | 55/100 [00:06<00:04,  9.06it/s] 56%|█████▌    | 56/100 [00:06<00:04,  9.08it/s] 57%|█████▋    | 57/100 [00:06<00:04,  9.06it/s] 58%|█████▊    | 58/100 [00:06<00:04,  9.05it/s] 59%|█████▉    | 59/100 [00:07<00:04,  9.08it/s] 60%|██████    | 60/100 [00:07<00:04,  9.10it/s] 61%|██████    | 61/100 [00:07<00:04,  9.09it/s] 62%|██████▏   | 62/100 [00:07<00:04,  9.08it/s] 63%|██████▎   | 63/100 [00:07<00:04,  9.08it/s] 64%|██████▍   | 64/100 [00:07<00:03,  9.04it/s] 65%|██████▌   | 65/100 [00:07<00:03,  9.05it/s] 66%|██████▌   | 66/100 [00:07<00:03,  9.06it/s] 67%|██████▋   | 67/100 [00:07<00:03,  9.09it/s] 68%|██████▊   | 68/100 [00:08<00:03,  9.09it/s] 69%|██████▉   | 69/100 [00:08<00:03,  9.09it/s] 70%|███████   | 70/100 [00:08<00:03,  9.09it/s] 71%|███████   | 71/100 [00:08<00:03,  9.11it/s] 72%|███████▏  | 72/100 [00:08<00:03,  9.10it/s] 73%|███████▎  | 73/100 [00:08<00:02,  9.09it/s] 74%|███████▍  | 74/100 [00:08<00:02,  9.08it/s] 75%|███████▌  | 75/100 [00:08<00:02,  9.09it/s] 76%|███████▌  | 76/100 [00:08<00:02,  9.07it/s] 77%|███████▋  | 77/100 [00:09<00:02,  9.17it/s] 78%|███████▊  | 78/100 [00:09<00:02,  9.25it/s] 79%|███████▉  | 79/100 [00:09<00:02,  9.31it/s] 80%|████████  | 80/100 [00:09<00:03,  6.21it/s] 81%|████████  | 81/100 [00:09<00:02,  6.95it/s] 82%|████████▏ | 82/100 [00:09<00:02,  7.58it/s] 83%|████████▎ | 83/100 [00:09<00:02,  8.09it/s] 84%|████████▍ | 84/100 [00:09<00:01,  8.47it/s] 85%|████████▌ | 85/100 [00:10<00:01,  8.76it/s] 86%|████████▌ | 86/100 [00:10<00:01,  8.98it/s] 87%|████████▋ | 87/100 [00:10<00:01,  9.16it/s] 88%|████████▊ | 88/100 [00:10<00:01,  9.27it/s] 89%|████████▉ | 89/100 [00:10<00:01,  9.34it/s] 90%|█████████ | 90/100 [00:10<00:01,  9.39it/s] 91%|█████████ | 91/100 [00:10<00:00,  9.42it/s] 92%|█████████▏| 92/100 [00:10<00:00,  9.44it/s] 93%|█████████▎| 93/100 [00:10<00:00,  9.43it/s] 94%|█████████▍| 94/100 [00:11<00:00,  9.44it/s] 95%|█████████▌| 95/100 [00:11<00:00,  9.45it/s] 96%|█████████▌| 96/100 [00:11<00:00,  9.42it/s] 97%|█████████▋| 97/100 [00:11<00:00,  9.40it/s] 98%|█████████▊| 98/100 [00:11<00:00,  9.38it/s] 99%|█████████▉| 99/100 [00:11<00:00,  9.33it/s]100%|██████████| 100/100 [00:11<00:00,  9.24it/s]100%|██████████| 100/100 [00:11<00:00,  8.52it/s]
Model tested on 100 tasks. Accuracy: 37.92%
